
<!DOCTYPE HTML>

<html>

<head>
	<meta charset="utf-8">
	<title>Theoretical view of Regularization - Htedsv Backyard</title>
	<meta name="author" content="htedsv">

	
	<meta name="description" content="Theoretical View of Regularization Around two weeks ago, I attended a job interview, which is ML-related. During the interview, I was asked to &hellip;">
	

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="/atom.xml" rel="alternate" title="Htedsv Backyard" type="application/atom+xml">
	
	<link rel="canonical" href="http://xuanhuangyiqi.github.io/blog/2016/03/27/theoretical-view-of-regularization/">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'>
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
        }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
    }
    });
</script>

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

  
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-47274484-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>


</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
			<header id="header" class="inner"><div class="profilepic">
	
	<script src="/javascripts/md5.js"></script>
	<script type="text/javascript">
		$(function(){
			$('.profilepic').append("<img src='http://www.gravatar.com/avatar/" + MD5("xuanhuangyiqi@126.com") + "?s=160' alt='Profile Picture' style='width: 160px;' />");
		});
	</script>
	
</div>

<nav id="main-nav"><ul class="main">
    <li><a href="/">Blog</a></li>
    <li><a href="http://about.me/htedsv">About</a></li>
    <li><a href="/you-qu-de-shi-yan">Projects</a></li>
    <li><a href="/dailystatistics">Statistics</a></li>
    <li><a href="/wordcloud">Interests</a></li>
    <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
<nav id="sub-nav">
	<div class="social">
		
			<a class="email" href="mailto:xuanhuangyiqi@126.com" title="Email">Email</a>
		
		
		
			<a class="google" href="https://plus.google.com/110950069380933156197" rel="author" title="Google+">Google+</a>
		
		
			<a class="twitter" href="http://twitter.com/htedsv" title="Twitter">Twitter</a>
		
		
			<a class="github" href="https://github.com/xuanhuangyiqi" title="GitHub">GitHub</a>
		
		
		
		
		
		
		
		
		
		
    	
    	
			<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
	</div>
</nav>
</header>				
			</div>
		</div>	
		<div class="mid-col">
			
				
			
			<div class="mid-col-container">
				<div id="content" class="inner"><article class="post" itemscope itemtype="http://schema.org/BlogPosting">
	<h1 class="title" itemprop="name">Theoretical View of Regularization</h1>
	<div class="entry-content" itemprop="articleBody"><p>Around two weeks ago, I attended a job interview, which is ML-related. During the interview, I was asked to explain SVM and regulizers. When I said the classical SVM didn’t involve empirical risk term in its objective functions, the interviewer was shocked. This experience inspired me to write this blog to summarize the some of the aspects of regularization techniques.</p>

<h1 id="introduction-to-regularizaiton">Introduction to regularizaiton</h1>
<p>In many introduction courses of machine learning, it’s always referred that regularization terms are considered in order to avoid overfitting problem. The most common way is to write objective function as follows:</p>

<script type="math/tex; mode=display"> f(\theta) = R_{emp}(X, \theta) + R_{structural}(\theta) </script>

<p>Intuitively, the risk (or called “error”) of a model come from two parts, the empirical risk is a measure of how different is the behaviours of the parameters and real data set, the structural risk is caused by the model itself. The structural risk is relatively difficult to understand. It’s mainly due to the high capability of the model. The capability or so called complexity is the main topic of <em>statistical learning theory</em>. Roughly speaking, <strong>we don’t want our model to be too capable</strong>, or not fit the current data-set very well. The reason can be explained from two aspects: </p>

<ul>
  <li><em>Deviation of the training-set and real data distribution</em>. Because of the limited size of training set, we can not find any possible data point from hypothesis space in the training set. More specificly, if the estimated parameters are too complex, the unlabeled data points (maybe in the test-set) may differ much from its real output. </li>
  <li><em>Existing noise in data-set</em>. Data are not always clean, the existances noise and outliers requires the model not to fit noisy data very well.</li>
</ul>

<p>While the measures of empirical risk and structural risk are independent, we often use a trade-off parameter to combine them together as:</p>

<script type="math/tex; mode=display"> \hat{\theta} = argmin_{\theta} R_{emp}(X, \theta) + C \cdot R_{structural}(\theta) </script>

<h1 id="svm-as-regularizer">SVM as Regularizer</h1>

<p>In most practical machine learning books, SVM is described as a very common classifier. Even everyone can write down the objective function as a combination of the two risks. But only a few people know how it comes to the current form.</p>

<p>As we talked in the last paragrah, regulizer aims to reduce high complexity of the model (we will discuss the <em>complexity</em> later on). Usually the regularizer term is auxiliary in the objective function. But for SVM, regularizer is the only term that matters. The empirical risk term is dispensable to deal with noise.</p>

<p>The origion SVM didn’t take noise into account, the only goal was to maximize the distance of  two parallel hyper-planes. As we know, if they are separable, the hyperplane definitely exist, so the empirical loss is 0. The only goal is to minimize the structural risk under the restriction that each training sample is correctly classified. In terms of <strong>Structural Risk Minimization</strong>, you can refer to <a href="http://www.svms.org/srm/">SRM</a>. The original goal to maximize the distance between $y=w^Tx + b + 1$ and $y=w^Tx + b - 1$ under the restriction that every point is correctly classified is formulated as follows:</p>

<script type="math/tex; mode=display"> \min_w w^T w </script>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{align*}
\text{subject to:} & (w^Tx_i+b+1)y_i \ge 0 \quad & \forall i: positive\\
& (w^Tx_i+b-1)y_i \ge 0 \quad & \forall i: negative
\end{align*} %]]></script>

<p>In order to tolerate noises, we add empirical terms to allow some points to appear between hyperplanes.</p>

<h1 id="choice-of-regularizer">Choice of Regularizer</h1>

<p>In many examples explaining <em>underfit</em>, <em>fit</em>, <em>overfit</em> we have seen how regularizer helps solve a optimal regression problem. But for classification problems, there are no general principle in terms of definition of regularization terms. For the existing regularization terms, we seldom see how each theoretically minimize the structural risk. For example, the objective function of decision tree problem involves the regularizer as the number of nodes in the tree, the more are the nodes $n$, the large is the penality. Obviously there are more alternatives. We can also use $e^n$. For both cases, we can find out a suitable trade-off parameter $C$ to balance the two risks. But ideally, which means all error come from noise, the best $C$ is independent on the traning set. However, the best $C$ must be different if we change the penality from $n$ to $e^n$. </p>

<p>The next question is whether there is alternative to combine the two risks instead of the summarization. Maybe product?</p>

<p>But the most annoying question is $L_1$ or $ L_2 $. In most cases we use $L_2$ to penalize outliers, but why we use $L_1$ in SVM to tolerate noise?</p>

<p>The answer to the previous questions can be concluded that <strong>it’s not necessary to be theoretically accurate</strong>. Machine learning is an engineering anyway.</p>

<h1 id="overfitting">Overfitting?</h1>

<p>A question that I have been ever asked is like “If you see overfitting with the current parameters, how to change your trade-off parameter”. I was therefore confused: is overfitting observable? Ideally it is definitely right theoretically if the objective function with respect to complexity of the model is convex. The convexity implies, there is a optimal complexity that minimizes the global error. Practically it’s sometimes wrong. To take SVM as an example, if we see higher test error rate than training error rate, there may be due to the overfitting. Now if we reduce the value of $C$ (reduce the weight of the tolerance), the distance between the two hyperplanes would absolutely be increased. But how about the discriminate boundary? It all depends on the distribution of the noises around the hyperplanes. Therefore, whether the generality of the model increases is still unknown. </p>

<h1 id="modern-regularizer">Modern Regularizer</h1>

<h1 id="dropout">Dropout</h1>
<p>In the most of traditional statistical models, regulizer normally acts as an additional term in the objective function. But in order to tacle the side effect of the most popular and effective learning machine – Deep Neural Networks, new kinds of regularizer should be proposed. As mentioned in the paper [1], the most popular regulizer <em>Dropout</em>, is a variation of model averaging. Comparing to averaging serveral independent neural networks, Dropout is much easier to implement. Due the the popularity of Dropout, we don’t need to explain it in details.</p>

<h1 id="l2-normalization">L2 normalization</h1>
<p>One of the simplest way to apply L2 constraint is adding penalty as the L2 norm of the weight vectors at each layer, but instead, the second strategy is limiting the upper bound of the L2 norm of the weight vector of each layer. Each time when the constraint is violated, the vector is re-normalized.</p>

<p>[1] Hinton, Geoffrey E., et al. “Improving neural networks by preventing co-adaptation of feature detectors.” arXiv preprint arXiv:1207.0580 (2012).</p>
</div>

</article>

	<div class="share">
	<div class="addthis_toolbox addthis_default_style ">
	
	
	<a class="addthis_button_tweet"></a>
	
	
	<a class="addthis_counter addthis_pill_style"></a>
	</div>
  <script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#pubid="></script>
</div>



<section id="comment">
    <h1 class="title">Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>
</div>
			</div>
			<footer id="footer" class="inner">Copyright &copy; 2016

    htedsv


Design credit: <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a></footer>
		</div>
	</div>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->


<script type="text/javascript">
      var disqus_shortname = 'htedsvbackyard';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://xuanhuangyiqi.github.io/blog/2016/03/27/theoretical-view-of-regularization/';
        var disqus_url = 'http://xuanhuangyiqi.github.io/blog/2016/03/27/theoretical-view-of-regularization/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-47274484-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>



</body>
</html>
