<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Logic | Htedsv Backyard]]></title>
  <link href="http://xuanhuangyiqi.github.io/blog/categories/logic/atom.xml" rel="self"/>
  <link href="http://xuanhuangyiqi.github.io/"/>
  <updated>2015-02-08T18:25:35+01:00</updated>
  <id>http://xuanhuangyiqi.github.io/</id>
  <author>
    <name><![CDATA[htedsv]]></name>
    <email><![CDATA[xuanhuangyiqi@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[更通用的角度理解程序与数据]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program/"/>
    <updated>2014-12-01T00:22:00+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program</id>
    <content type="html"><![CDATA[<p>本学期有一门叫做“Fondation of Data Science”的课，打着Data Science的幌子来讲数据库，让我完全对7所的理论课兴趣大减。究其原因，大概是它的某些先修课程我之前没学过，所以学起来吃力。</p>

<p>尽管如此，其中的很多观点还是值得借鉴和思考的。现在回想本科比较重要的课，大概有这么几门：编译原理、数据库、计算机网络、操作系统。我认为包括美国在内的计算机专业比较强的学校都在教育过程中弱化各个领域的关系。这当然从某些角度来说是有好处的。但是从更抽象、更理论的角度来看，不利于学生对于本质问题的理解。
<!--more--></p>

<p>比如数据库，这个比较晚才出现的概念，在正式用来解决并发管理、高效操作之前，它也被看做解决问题的通用模型的一部分。具体来说，就是<strong>我们认为世界上所有的数据都存在关系数据库里面，所有的程序都是SQL写的</strong>，因为我们假设这就是最基础的通用模型。我们为什么不假设别的语言呢，比如C或者Java？因为现在我们是要站在数据的角度来思考，而关系数据库是相对好理解的模型中最基本的，其他的NoSQL数据库模型算不上理论模型，因为它们更自由，缺少约束。既然已经确定了关系数据库，那么SQL显然是最合适的程序语言。但是基本的SQL并不能表达其他语言的所有逻辑，最基本的，就是递归。因为SQL和代数表达式，一阶逻辑等价（证明方法很多，这里不赘述），所以我们也可以换个角度理解一个概念，一阶逻辑（First-Order Logik）像是一维空间，没有层次。我们需要“函数”来完善SQL来实现递归逻辑。<a href="http://en.wikipedia.org/wiki/Datalog">Datalog</a>就是这么一个东西，既包含了一般的以细节的规则，每条规则是一些关系表达式的Intersection，这是不是和编译原理中的文法从逻辑上很类似？因为我们写的文法是针对Context-Free Language的，所以是不是现在又打通了语言和程序的关系？回到Datalog上，因为左边的变量可以出现在右边，所以我们可以实现递归了。但这并不完全是我们要的，因为要保证其单调性，所以Datalog不能包含“非”运算，所以要使用它的升级版Inflationary Datalog。相比之下，传统的SQL从理论层面上来看简直弱爆了。</p>

<p>现在再回到程序语言上来看，编程语言最本质的性质不是循环Loop、选择if，而是一阶和高阶，也就是是否包含递归（而不仅仅是函数，函数可以不实现递归）。至于“循环”和“选择”只是为了更适应人类和业务思维而产生的，即使没有他们，也可以用其他的方式用递归来代替。这就是函数式编程所谓“最高级”的地方，他们采用的都是更“元”的操作。</p>

<p>说到这，就不得不提Prolog——我打算写这篇文章的初衷。在基本了解这门语言的语法之后不会意识到这门语言的强大，但当我们具备了一些数理逻辑的知识之后，发现这门语言就是在用一阶逻辑编程。在以形式理论发展如日中天的时候，几乎所有科学家都认为逻辑推导就是构造世界的基本方法。对于形式化推导求值方法，可以了解<a href="http://en.wikipedia.org/wiki/Ehrenfeucht%E2%80%93Fra%C3%AFss%C3%A9_game">Ehrenfeucht–Fraïssé game</a>。同样的，对于任意一个问题，只要我们构造出这个问题的基本事实和推到规则，用术语来说就是公理系统(Axiom System)和赋值(Interpretation)，我们就可以判断这个子世界的任何命题的真假，前提是你需要了解理论(Theory)是由公理系统分形出来的无限谓词句子的集合。回到Prolog，我们只要向它输入所有世界的规则，就可以知道任何可以推断出的问题。然而这种最直观的构造世界的方式并没有被工业界认可。其原因在我看来有几点：</p>

<ol>
  <li>尽管它更加直观，但是从表达能力上来看，并不比其他语言更强大。我们完全可以用其他语言代替，即任何常见的编程语言都能实现与之等价的功能。</li>
  <li>理论上更直观并不代表应用上更直观，很多简单的问题并不需要抽象出它的实施和推理规则就能解决，或者说，大部分我们能接触到的问题并没有太多的逻辑规则。</li>
  <li>每种问题会有特殊的解法，我们的目标是解决问题，而不是把所有问题标准化成一种问题，标准化的过程会把问题复杂化。</li>
</ol>

<p>真正理解这几点之后，才不会让一些略懂人工智能的人们对人工智能盲目崇拜，对形式逻辑盲目崇拜。而我们之前接受的教育似乎完完全全的把这些东西避开了，以至于让一些求知欲更强的学生在课外学习中以为自己了解了更高深的东西而发出更幼稚的言论。</p>

<p>说到通用，前面已经提到数据库是表述数据相对直观的最通用的模型。然而在更早的时候就出现了一个更加通用的模型用来表述数据的输入、输出、中间过程——那就是用01纸带做记录的图灵机。图灵机最大，或者说唯一的意义在于分析程序的复杂度，或者叫实现难度。这是一项相当底层的任务，所以最好也要用相当底层的数据表示来完成更为恰当。具体原因可以这么解释：所有较难的复杂度分析几乎都是基于reduction去和已知复杂度的模型建立联系，也就是说，越是底层的表示，reduction的自由度也就越大，具体方法可以参照<a href="http://en.wikipedia.org/wiki/Reduction_%28complexity%29">Reduction</a>，而P != NP并不是一个已经被证实的问题，尽管我们感性上能理解，但不存在这样的方法来证明，我们找不到NP到P的reduction并不能代表不存在。</p>

<p>提到复杂度，可能也要顺便提一下，时间复杂度NP和P并不是衡量复杂度的唯一标准，在其之上还有PSPACE和NPSPACE，只不过是把占用的空间作为评价标准，刚了解时有点毁三观。之后有时间我会单独写一篇关于复杂度的东西。</p>

<p>关于通用，还有一个我一直记错的概念，就是“通用图灵机”，他只是一个可以模拟图灵机的图灵机，即把需要模拟的图灵机的状态转移表用二进制表示作为字符串输入给通用图灵机，让其模拟。它就像一个编译器，读一段代码，然后执行这段代码，高明的地方就是两种代码用同一种语言写的，也就是说，我们可以再用一另一个通用图灵机来模拟这个通用图灵机——即用编译器来编译执行编译器。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[确定世界与概率世界]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie/"/>
    <updated>2014-10-24T21:45:20+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie</id>
    <content type="html"><![CDATA[<p>这篇没有涉及具体的理论知识，主要是建立在部分相关经历之上的一些臆想，所以多少会有些民科思维。的确，我在这方面的经验少的可怜。但是相比于能看到我博客的人来说，信息不对称还是非常严重的。从刚入学我就多次表达了我的震惊。我发现计算机科学还有这么大一片领域我几乎一无所知，最可怕的是，我所谓的一些“基础和见识”可能也就是当地大二学生的水平。经过了一个暑假的沉淀和反思，我对于形式理论的极端态度渐渐冷却，所以，我也希望我能把它放到和其他领域平等的地位上考虑一些它们之间的关系。</p>

<!--more-->
<p>如果我们想表达两件事情A和B的必然因果关系，形式方法是A-&gt;B，而概率方法是p(B\|A)=1。这就是两个世界的建立基础。所谓“世界”并不是从学科发展的角度，即，不是按照先学p(A)，然后学p(A,B)最后学条件概率。而是按照世界运转的“元操作”。说到这，可能还是要提一下“世界”的动与静。世界可以是一个生态环境，一个程序中的类，也可以是一个人的个体。而“动”和“静”（或者叫行为与状态），可以类比成“算法”和“数据结构”，“类方法”和“类参数”，“代码”与“数据”。不过要认识、分析和解决计算机科学根本的问题还是要放在最根本的模型上。我说的当然不是图灵机，而是比图灵机更早，而且标定计算机能力疆界的——形式逻辑。</p>

<p>从计算机出现甚至更早，整个计算机科学的发展都是建立在形式理论的基础上，比如程序语言的设计、数据库的设计。到了后来，计算机的计算能力已经达到可以真正为人所用。于是就有了大量人开始用计算机来搞数值分析，并因此诞生了一些图灵奖得主。这些就扯远了。总之，到了90年代，单纯的形式理论已经没再出现真正的突破。</p>

<p>形式理论正如前面的例子中提到的，是一套完备的体系，一切都是确定的。当然，“真正的智能无法实现”也是被确定的，因为二阶逻辑的不完备性在图灵机诞生之前已经把计算能力的极限定在了那里。
而无法实现的原因可以直接参照停机问题的矛盾句式。</p>

<p>于是，当我们发现一个世界的运行规则被基本挖掘干净，并且没有找到我们真正需要的东西时，就尝试在确定性世界的框架下加入不确定性，也就是概率来完善形式理论的一些不足：</p>

<ol>
  <li>
    <p>一个世界的规则是确定的，不能随着外部输入而变化，一个世界需要更多的可能性。尽管随着概率的加入，我们可以通过修改参数的方式修改世界的一些规则（一个具体的例子就是通过样本来训练机器学习模型的参数），然而制定规则的世界依然是确定的。概率的加入只是帮助我们从应用的角度解决了更多具体的问题，对于真正的理论研究上的“智能”并没有什么帮助。</p>
  </li>
  <li>
    <p>真实世界是概率的，至少我们对于一个值得研究的子世界（所谓子世界，就是真实世界中我们需要研究的对象集合的子集），其中的个体都是概率的；或者说全世界是确定的，其原子行为构成一个闭包。但对于真实世界的任意一个有限子集，都不是封闭的，所以我们只能把其中的一部分当成随机变量来考虑和认识。通过从“元”上达到和真实世界的一致可以帮助我们更好的转移现实世界的内容到构造世界。尽管我们依然构造不出绝对的智能，但是已经可以用简单的智能结合真实生活，完善生活中的问题，这样可能更有社会意义。</p>
  </li>
  <li>
    <p>我们可以在假设存在一个真实世界的子集，并且在这个我们要研究的子集是一个闭包，即，一切都是确定的。但问题就变成，为了尽量真实的模拟现实世界，需要足够大的子集，以至于超过了计算机的运算能力，难以继续，一个实例就是神经网络算法的发展瓶颈。</p>
  </li>
  <li>
    <p>确定性世界并非一无是处，因为其关系的建立都是稳固而确定的，所以整个世界体系也是可靠的（往往是无限大），而概率世界更多的是表达隐含关系，不确定关系，所以单纯由概率搭建的世界（比如贝叶斯网络）不会很庞大，仅仅是为了解决某类特定小问题。</p>
  </li>
</ol>

<p>当然概率帮助我们简化了一些“确定性”解决起来很棘手的问题，但是根本性问题永远无法解决。也正因为这一点，形式化理论研究很早就进入了瓶颈，而且创造不了价值，所以时至今日，只有为数不多的欧洲大学把它作为重要计算机专业最重要的基础知识传授给学生。然而学习它却对于理解计算机科学从宏观上能有最重要的价值。</p>

<p>如今，“单纯的形式理论”已死，也不会有人太多人研究“真正的智能”。更多的是将其和统计学方法结合，解决生活中具体的问题。相信随着更多复杂的形式理论的引入，我们能在未来构建更加完善的“世界”。</p>
]]></content>
  </entry>
  
</feed>
