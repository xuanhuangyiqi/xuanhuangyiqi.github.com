<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Htedsv Backyard]]></title>
  <link href="http://xuanhuangyiqi.github.io/feed" rel="self"/>
  <link href="http://xuanhuangyiqi.github.io/"/>
  <updated>2015-12-05T16:25:55+01:00</updated>
  <id>http://xuanhuangyiqi.github.io/</id>
  <author>
    <name><![CDATA[htedsv]]></name>
    <email><![CDATA[xuanhuangyiqi@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[表达能力 (Expressiveness)]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/12/05/biao-da-li-expressiveness/"/>
    <updated>2015-12-05T11:16:24+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/12/05/biao-da-li-expressiveness</id>
    <content type="html"><![CDATA[<p>第一次正式接触“表达力(Expressiveness)”这个概念是在Fondation of Data Science这门课上，当时我更喜欢用“计算能力”这个词来翻译它，因为当时接触到的更多的是计算模型，比如自动机、图灵机、一阶逻辑之类的，而它又是从能计算的函数的角度来衡量这些模型。不过后来发现“计算能力”这个词多多少少有些歧义，所以就放弃了这个说法。而由于当时接触到的模型多是用于判定的模型，即，接受一个输入，并且唯一的返回“是”或者“否”的模型，所以对于“表达能力”这个说法觉得更加贴切。用自然语言举个例子：“我是个聪明而且友善的人”，这句话是可以用真伪判断正确性的，如果把“而且”及其类似的表达方式从自然语言中删掉，那么我们就没办法表达这句话，或者说后者的表达能力要比原本的表达能力更弱，这样来解释理论中用到的“表达能力”这一概念就更加合理了。</p>

<p>前面提到“计算能力”这一翻译会有歧义就是因为它和“可计算性(Comptability)”一词概念和说法类似，但是定义上又有很大区别。“可计算性”说的是某个集合、类能否用图灵机及其等价模型计算出来，这个问题的回答只有“是”或“否”。所以可计算性只是在以图灵机为分界点的问题上的表达能力的判定。</p>

<!--more-->

<p>另外值得一提的就是判定问题。所有的可计算函数都可以转化为其对应的指示函数(indicator function)，比如$f(x)=y$可以转化为$F(x,y)=1$。而我们说“表达能力”也往往说的是指示函数。这也就是为什么，前面会把自动机、一阶逻辑这种只能返回真假的模型和图灵机相提并论。</p>

<p>从这个角度切入，可以解释一个问题，为什么自动机应用中往往只是作为编译原理中语法分析的最简化模型，或者用于正则表达式，却可以独立作为一个独立的理论计算机的领域。</p>

<h1 id="section">自动机与图灵机</h1>

<p>之前在知乎上回答过两个问题：“自动机的状态能否无限？”以及“图灵机最重要的是状态转移表，纸带无限并不是必要的？”。</p>

<p>首先，从历史发展上来讲，自动机的提出要比图灵机早，然而现在很少有人关注到自动机这个工作方式更简单，结构类似于图灵机的模型，反而把图灵机的提出作为计算机科学的开端。之所以把图灵机作为开端，是因为它是第一个为一阶逻辑明确定义了“机械运动”的模型，而自动机虽然更简单，而且更加容易用机械模拟，但是却只能表示图灵机的有限操作，或者说，自动机的表达能力弱于图灵机。</p>

<p>前面提到的两个问题其实本质是一样的，那么就拿后者为例从表达能力上解释一下：</p>

<p>首先图灵机和自动机其实从操作本质上是一样的：根据当前状态和确定的转移规则，唯一的转化到另一个状态上。区别在于自动机的状态就是定义里明确的状态，而图灵机的状态是“配置(configuration)”:（纸带内容，指针位置，状态）。假设图灵机的纸带长度有限，那么明显纸带内容的可能性和指针位置又是有限的，再加上根据定义，图灵机的状态也是有限的，所以对于每个图灵机的配置，都可以构造一个自动机的状态，那么得到的自动机的状态数是有限的，即对于每个纸带有限的图灵机都有与之等价的自动机，所以图灵机也就不比自动机表达能力更强。进而，图灵机表达能力强大也就在于其纸带的无限长度。</p>

<p>而第一个问题相对不那么好回答就是因为无限这个范围内，又分为可数，不可数，不可数又可以不断继续划分。每种情况几乎都要分别考虑，并不是那么简单。</p>

<h1 id="section-1">表达能力与可分析性</h1>

<p>无论是数理逻辑，还是平时讨论的计算机模型，一个重要的结论是“表达能力越强，可分析性就越差”。比如一阶逻辑里所有问题都是可以判定真伪的，而二阶逻辑就做不到，但从表达能力上来说，二阶逻辑强于一阶。这个结论可以拓展到任何逻辑上以及计算机里面的计算模型。拿形式语言举例子，比如star-free语言的表达能力弱于正则语言，弱于上下文五官语言，然而star-free语言存在于群的等价对应关系，正则语言存在与幺半群的等价关系，即不存在唯一元，唯一元的性质也就不能用了。</p>

<p>当然这个结论方到图灵机上，我们即希望我们发明的模型能尽量丰富的表达我们想要的想法，同时又不得不失去对这个模型的掌控能力，比如我们连一套通用的判断一段程序死机的方法都没有，更别说从理论上判断一段程序有没有bug了。</p>

<p>也正是因为这种无法完美妥协的trade-off，才会让很多表达力更弱的模型在一些领域没有被完全取代。</p>

<h1 id="section-2">参数化机器学习与表达能力</h1>

<p>这里说的机器学习模型都是可以训练出参数的，而非参数方法，比如KNN之类的非参数方法不在讨论之列。从参数学习的角度来看计算模型，我们可以新增一个性质——可学习性。比如自动机和图灵机的状态转移表，都可以看做其不同于其他自动机或者图灵机的参数，SVM的参数就是那个分割面的$w$和$b$。决策树的参数就是一套按顺序执行的判断规则等等。那么这些这些参数都是可学习的吗？显然如果是的话，我们似乎看到了机器学习的一个终极问题——学习一台图灵机。作为最强表达能力的理论模型，如果图灵机的参数可以通过某种算法学得，并且智能的思维可以用任意一个可计算函数表示的话，那么就似乎就抵达了人工智能的终点。</p>

<p>然而，即便是我们把问题简化到一个一定停机的判别图灵机上。问题也是不可解的。当然，首先要合理地定义一下“可学习”，我们就按PAC-learnable的标准来定义，即对于任意误差$\delta$，我们都能找到一个无关于样本分布的训练集大小$N$使得对于任意大于$N$的样本集，我们都能训练出误差小于$\delta$的参数。然而对于一个最经典的PAC不可学习问题：一个可数集合上有限个点，是可以表示成一个图灵机的判别函数的。所以似乎这一构想从最基础的层面就是不可实现的。</p>

<h1 id="section-3">深度学习机器的表达能力</h1>

<p>之前的文章已经提过一些关于深度学习大火的理论层面的原因，但是没有说的很具体。这里再从表达能力的角度稍微说一点。</p>

<p>在上一学期的深度学习的seminar上，我选的题目是“Recurrent Neural Networks and Convolutional Neural Networks”，估计算是最大的一个题目了，因为两个话题随便哪个都能扯出一连串的子问题，最后的结果就是我最终没有切入教授和助教的兴趣点。在slide的结尾，我被要求从某个维度把这两种关系并不大的神经网络联系起来。最后选的就是表达能力上。即，对于结构相同（神经元层数和数量相同，连接数不一定相同）的RNN，普通DNN和CNN，表达能力RNN &gt; DNN &gt; CNN。这个结论非常显然，因为明显它们的参数数量是递减的，而且后一个的参数是前一个的子集。这一结论显然没有什么意义，但是借此引入更多的机器学习模型作为对比，我们可能会有新的认识。比如就拿SVM在图像识别上作对比。在06年Deep Learning大火之前，SVM+kernel是很长时间以来在很多领域都是高效而精巧的方法。所有高维度的分类问题对于任何形状的分割面理论上都能model的出来。然而，这一结论并不正确，在Y. Bengio的On the challenge of learning complex functions中就提到了kernel作为机器学习领域几乎最常用的方法在模拟复杂结构化函数上的局限性。从这个角度来说，非线性kernel作为把任意分割面映射到超平面的方法从表达能力上来说并不完美。同时也是侧面解释了深度学习大火的深层原因——传统模型无法企及的表达能力。</p>

<p>再回到提到的两种最常见的深度模型RNN和CNN，可以说它们都是从不同的角度增强其表达能力。但各自的原因还是要取决于具体的问题，比如CNN是因为local feature，RNN是因为模型或者数据的时序性。</p>

<p>虽然表达力的增强是大势所趋，但并不是在任何情况下都是好的。在机器学习中和它相对的就是泛化能力，具体来说，对于训练100个样本，我们用一个10层的神经网络显然不合适，训练出的参数很容易过拟合，原因无非就是没有做好样本数量和参数的平衡。这也解释了早年间样本数量有限时，神经网络并没有表现的好过其他模型。</p>

<p>说到底，表达能力只是发展的大趋势，平时真正困扰学术界和工业界的往往是表达能力之外的问题，比如泛化问题，比如数据不足充分利用数据的问题。</p>

<p>作为神经网络中表达能力最有趣的问题——神经图灵机，以后有机会再单独说。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[算法问题的标准型]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/10/16/wen-ti-luo-ji-suan-fa/"/>
    <updated>2015-10-16T20:02:30+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/10/16/wen-ti-luo-ji-suan-fa</id>
    <content type="html"><![CDATA[<p>几乎每个学计算机的学生都会在很早的时候就接触到“算法”这个概念，然而大家对于“算法”的理解往往是一套解决某个特定问题的“做法”。而“做法”一词又可以描述成几个连续、嵌套的步骤，最后又可以转化成指令式代码，包括顺序、循环、选择结构。</p>

<p>在之后接触到函数式编程，我们对于“算法”的理解又变得更宽泛一些。函数式编程中的算法更像是直接描述问题本身，而不是一个一个的步骤。比如用Haskell实现快排算法，思路和细节问题都要比指令式编程简单得多：每一次用filter过滤出比标准元素大的集合和小的集合，然后分别递归。正因为函数式编程这样的特点，很多人介绍函数式编程的时候，都会提到“学函数式编程不用学算法，因为函数式编程本身就是在描述问题”。当我以这句话作为学习的目标学完 Funcitonal Programming 这门课之后（以及后面学的Logic Programming）之后，发现这么句话有很强的误导性，虽说函数式编程只是简单的描述问题，但是必须按照函数式编程（以及逻辑式编程）自己的方式来实现，这这就有涉及到“转化成本”的问题，对于很多10行就能搞定的指令式程序换成函数式程序有时会长很多倍，另一方面算法的优势在于可优化性，也就是说，只要算法学的足够好，任何程序都能在足够短的时间内运行完。</p>

<!-- more -->

<p>有点扯远了，好吧，下面切入正题。也就是如何看待计算机科学中几乎最重要的概念——算法。尽管按照前面关于算法的说法也没有任何问题，但作为一门科学的重要组成部分，算法显得太过零散，没有什么结构，框架。比如同样叫算法，机器学习中的学习算法（比如EM）和图论中的最短路径算法就没有办法找到一个一般形式同时概括他们。另一个更具体的例子就是ACM竞赛，从初中开始为了比赛，就多多少少接触了一些算法竞赛书或者在线教程。最常见的一类就是把教程分为几个章节：一章讲朴素、贪心方法；一章讲图论算法；一章讲动态规划等等。然而章节之间其实没有任何联系，唯一能把它们联系起来的线索就是——它们都是方便考察，变形灵活的计算机算法。或者说，它们是计算机科学中所有的算法在“容易考察”这个维度上的一个投影，并且再根据主题进行归类。如果学生只通过这个没有太大价值的维度了解计算机科学显然是片面的。</p>

<p>回到问题本身，也就是在我自学优化课的时候想到的，对于一般意义上的算法，是否有一种方法把他们统一起来？当然有。在机器学习中这个概念叫做「目标函数」，在连续优化中叫做标准形式。</p>

<p>更一般一点，对于所有确定性算法，或者叫做有明确目标的算法，都可以分成最优化某函数的，和构造可行解的。再把他们统一起来，结论就是，所有确定性算法都可以用优化中的 standard form 来表示，即：</p>

<script type="math/tex; mode=display"> \min_x f(x) </script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
    \text{subject to:} &g_i(x) \le 0 \\
    &h_j(x) = 0
\end{align*}
 %]]&gt;</script>

<p>更一般一点，我们可以把$h_j(x)$省去，构造可行解的问题就可以表示成没有$f(x)$而只有$g_i(x)$。
其中$x$是算法要求的输入，$f$是要优化的函数，拿背包问题为例：</p>

<script type="math/tex; mode=display"> \min_x \sum x_i w_i </script>

<script type="math/tex; mode=display"> \text{subject to:} \sum x_i v_i - V \le 0 </script>

<p>其中x_i是0或1，分别表示每个物品是否被选。仔细一想，发现还是有问题，$x_i$的值域(只能取0或1)并没有反映在限制条件中，这就要引出优化问题中比较麻烦的一类问题——整数规划，也就是限制解集必须是整数，只不过这里不展开解释。</p>

<p>另外想到的一个典型的例子是<a href="http://research.microsoft.com/pubs/69644/tr-98-14.pdf">SMO(Sequential Minimal Optimization)</a>算法，学过SVM(Support Vector Machine)的人基本都知道，SVM做二分类时从定义类间距最大化为目标函数，再到借助 Lagrange Duality 把问题转换到求解支持向量上面，SMO算法就是在每次迭代时，通过限制条件来找到最好的两个$a_i$ $a_j$来使目标函数最小，直到整个$a$向量收敛至最优。</p>

<script type="math/tex; mode=display"> \min_a \Phi(a) = \min_a {1 \over 2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j (x_i \cdot x_j) a_i a_j - \sum_{i=1}^N a_i </script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
  \text{subject to: } & a_i \ge 0 \\
  & \sum_{i=1}^N y_i a_i = 0
\end{align*}
 %]]&gt;</script>

<p>前面从Optimization的角度来看待算法，任何问题都可以转化成一个优化函数的标准型，而事实上，在一些学校数学系的学生不会和计算机系的学生一起上算法课，他们会上一门“Discreate Optimization”来同时涵盖算法和可计算、复杂度理论。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[无限计算(Infinite Computation)]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/05/26/wu-xian-ji-suan/"/>
    <updated>2015-05-26T12:22:03+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/05/26/wu-xian-ji-suan</id>
    <content type="html"><![CDATA[<p>这是一门上学期的课，趁着假期总结一下。虽然没有可计算理论和数理逻辑那样颠覆三观，但是作为一门相对基础的理论课，还是能给我不少启示的。</p>

<p>刚开始选这门课一方面是因为它是逻辑所开的，一方面是因为这个名字比较唬人。因为任何离散的问题一旦放在“无限”这个限制条件下，就会变得困难加倍，许多针对“有限”问题的解法就变得不那么适用。比如命题逻辑的完备性定理，就是把无限集合的判定性问题转化到任意有限集合上判定，从而使不可解问题变得可解。</p>

<!--more-->
<p>然而这门课的内容虽然可以推广到任意通用的计算问题，但是所有讨论的对象都是形式语言，或者说大部分都是正则语言，不过这也很符合7所的风格：任何理论计算机讨论的问题都是“语言”的问题。“语言”是计算机理论问题的基本对象，代码、数据从一定程度上说，都是“语言”。然而从“语言”的角度来看待所有计算机问题并不合适，比如把问题看做“语言”并不能助于解决任何算法问题。如果把逻辑、语言、图灵机看成解决问题的一般方法，那么算法就是解决问题的特殊、具体方法。逻辑、图灵机只讨论可能性，而千奇百怪的算法（或者说复杂度理论）才是真正解决问题的具体工具。也正是因为世界上没有一套简单的工具、方法能最优的解决所有问题，才会出现整个科学的繁荣发展。“不用一个工具解决所有问题”这一观念不止在计算机领域，在任何学科都适用。</p>

<p>回到形式方法上，有限正则语言中存在DFA和NFA的等价转换，然而对于无限语言DBA(Definite Buechi Automaton)和NBA(Nondefinite Buechi Automaton)就不是等价，而是层级关系了；而且在DBA之内也有另外的一些层级。相比于有限语言，无限语言的层级要丑陋一些，许多具体问题的处理也要更棘手，比如把NBA可接受的语言转化为NBA。</p>

<p>7所几门课给我带来的另外一个认识就是几种模型的等价性，比如每种语言都会存在某种对应的自动机，复杂一点的会对应Pushdown Automaton, 甚至只能对应图灵机，一阶逻辑，二阶逻辑；甚至还有一些另一个体系下的逻辑：Linear Temporal Logic与之对应。更通用一点，与“有限”或者“无限”语言对应的是判定性函数，甚至是机器学习模型：神经网络、决策树、线性分类器、SVM。</p>

<p>最后一节课，提到了一点不会考，但是很有意思的东西——无限语言在有理数上的对应，再结合前面的二阶逻辑表达式，就是用二阶逻辑表示有理数集合，算是一个打通连续空间和离散空间的实例。在之后就是康托集合，它的存在证明了有理数和实数集合不存在一一对应。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从可计算理论到学习理论]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/03/07/cong-ke-ji-suan-li-lun-dao-xue-xi-li-lun/"/>
    <updated>2015-03-07T11:03:37+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/03/07/cong-ke-ji-suan-li-lun-dao-xue-xi-li-lun</id>
    <content type="html"><![CDATA[<p>我曾经多次表示过，计算理论是计算机科学的根，也是计算机科学有别于数学的原因。在计算机科学与数学的边界上，计算机科学这边是可计算理论，数学那边是数理逻辑。</p>

<p>可计算理论讨论最多的具体问题的共同特点是“绝对性”，即没有任何条件限制的情况下，某些问题是否可计算。比如最常见的是停机问题、等价问题：是否存在机械方法能够绝对准确的计算“任何”函数是否停机，判断任意两个图灵机是否等价（“等价”即是否计算相同的函数）。第一个问题的不可判定性随处可以搜到。第二个问题是一个比第一个更难判定的问题，严格来说要构造Reduction：$H \leq Eq$；通俗来理解，就是要说两个图灵机功能能完全相同，必须枚举所有可能输入，并且判断输出，然而输入集合是无限的，所以这一过程不会停止。当然，这一问题用人脑来模拟，在图灵机结构简单的时候是可以完成的，不过我们人脑模拟的过程是带有一定启发性的，即不能严格的归为机械过程。</p>

<!--more-->
<p>下面就拿这个Eq问题（判断两个图灵机是否计算同一函数）作为引子，讨论学习理论。</p>

<p>计算理论研究的一般是通用问题的可能性，即在最严格的情况下，某种问题是否可计算。然而更多的时候，我们并不需要那么严格。只要能近似相等就可以了，或者随着学习样本的增加，模型能逐渐逼近真实值。回到$Eq$问题，即“如果我们尝试了“足够多”的输入，两个图灵机都能输出相同的内容，那么就可以说两个图灵机等价了”。以上判定性问题的表述，从可计算性上，等价于“对于一个图灵机$M_1$和一个给定的足够大的输入集，我们可以构造出$M_2$，使得$M_1$和$M_2$对于给定输入能够得到相同输出，也就是说能够构造出一个近似和$M_1$等价的图灵机$M_2$”。</p>

<p>这一问题相关的理论就是“可计算学习理论”，其中一个重要而基础的部分就是研究一个模型是否可以学习。一个模型，或者叫结构可以被学习，当且仅当随着样本的增加，误差能收敛到无限小。形式化的表示即：</p>

<script type="math/tex; mode=display">P(error \ge \epsilon) \le \delta</script>

<p>其中需要注意的是，$\epsilon$是单个样本的误差概率。那么显然式子左边是关于样本集大小N相关的，即符合形式</p>

<script type="math/tex; mode=display">P(error \ge \epsilon) = p^N \le \delta</script>

<p>所以关于它还有另一种表述：对于任意小，且不为0的$\epsilon$和$\delta$。我们都能找到一个N。</p>

<p>那么有些模型就无法被学习，即随着样本增大，误差不会收敛于无穷小。虽然这样的模型很多，不过一个共同点是，可以映射到“从一个无限集合中学习任意多个元素”。当然，对这一问题的严谨而通俗表述可以参见PAC理论。</p>

<p>当然，学习并不都是基于概率，有些结构因为其符合一定的性质，可以直接得到目标解。比如前面写过的<a href="http://blog.htedsv.com/blog/2014/08/15/anluin-learning-algorithm/">Angluin’s Learning Algorithm</a>，就是根据正则语言的一致性学习的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对Spotlight的简单扩展]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/01/17/spotlight-extension/"/>
    <updated>2015-01-17T19:34:08+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/01/17/spotlight-extension</id>
    <content type="html"><![CDATA[<p>Mac下现有的Spotlight虽然比起前几个版本要好用很多，但是扩展性上还是没办法和Alfred相比。今天忽然想到一个做法对Spotlight做小小的扩展。</p>

<ul>
  <li>首先写一个shell文件，随便放到哪里。比如如下，打开某项作业文件夹中最新建立的文件夹（表示最新的一次作业）。</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#! /bin/sh
</span><span class="line"># Filename: isc
</span><span class="line">find ~/Desktop/Dropbox/ISC/Exercises/ -name "SC*" -type d | tail -n 1 | xargs open</span></code></pre></td></tr></table></div></figure></notextile></div>
<ul>
  <li>增加执行权限</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">&gt; chmod +x isc</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>
    <p>右击文件修改 -&gt; “Get Info” -&gt; “Open With” -&gt; “iTerm” -&gt; “Change All”</p>
  </li>
  <li>
    <p>在Spotlight中输入文件并执行</p>
  </li>
</ul>

<p>不过这种方法只是利用了Shell的扩展性，还是不能做到Alfred那样任意接收参数并且实时返回结果。但是解决一般问题应该是足够了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于德国教育的私吐槽]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/12/14/guan-yu-de-guo-jiao-yu-de-si-tu-cao/"/>
    <updated>2014-12-14T13:07:06+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/12/14/guan-yu-de-guo-jiao-yu-de-si-tu-cao</id>
    <content type="html"><![CDATA[<p>列举几个关于德国教育的私吐槽。之前在知乎上评论或者发表过一些对于德国大学教育的看法，从始至终，无论对方如何恶言相对，我都保持和善和尽量谦虚的态度和他讨论，但最后发现，真正得到对方的认可，不是通过你的讨论态度、不是你的论点、不是你说话的逻辑，重要的是你有多了不起的经历。只要有这些经历，对方就会认为自己没资格继续否定你，否则，你的所有观点都是逻辑混乱。大概经历了几次，也就不再会去知乎参与德国或者德国大学相关的问题了。</p>

<p>不过最近的一些学习体验还是让我想对德国大学或者教育发表一些看法。当然我不是最有资格发表评论的，经历也肯定比不上很多人。但是因为从未在公开场合看到完全符合我的认识的想法或观点，所以我很希望能把我的认识在这个相对隐私的地方表达出来，以免招来不必要的非议。</p>

<!--more-->

<h1 id="section">教学与科研</h1>

<p>在很多人看来，科研重于教学，一个发过顶级会议论文的人一定比每天花大量时间上课学习的人要厉害，因为科研活动能培养一个人全面独立的思维，而上课只能学来一些死知识。然而有这一观点的人除了人云亦云的情况之外，基本都是受了国内学术氛围的影响。国内教授能在基础理论上有深入理解并且转化到课堂上的少之又少，基础理论在计算机学生心中的印象是“无聊”“没用”，即使学得好，领悟的透，也并不一定会用。</p>

<p>然而，更多情况下，无论是工作、科研，一定程度上都是在消耗课上学习的知识，尽管科研、工作过程中会继续补充知识，但是没因为没有系统的知识体系，学习速度、深度肯定远比不上课堂。那些没有把课上知识深入理解的人一定不会在科研道路上走得太远。科研并不比工作高尚太多，都是在有目的有针对性的解决不同层面具体问题。</p>

<p>鄙视“上课”的人不仅表现在对科研盲目崇拜上，还表现在对于自学成才的大牛的盲目崇拜以及效仿上。同样的知识，自学而成的大牛往往比上课或者某种循规蹈矩学来的人更容易得到崇拜。从而有时会形成一种风气，放弃好的学习资源去选择自学，仿佛这么做了就能让自己更牛。</p>

<p>这是搞错了因果关系，厉害的人被人关注，往往是因为他处在相对弱的环境里，比如老师水平很差，没有办法轻易获得好的学习资源等等，从而迫不得已选择自学。自学成才的人尽管自我提高了自学能力，但是往往会遇到两个问题：</p>

<ol>
  <li>
    <p>因为自己自学的小成就而对正式、规范的知识体系不屑一顾。比如我在不懂编译原理的时候写出的编译器大作业没有符号表，词法分析和语法分析被写在了一起，并且当时自认为这是化繁为简的行为，因为经过我的改进，减少了代码量、简化了步骤、提高了效率。并且对规范的流程不屑一顾。直到后来深入学习，才意识到规范化的重要性，野路子因为对问题的简化假设，往往做不大，问题稍微复杂一些就要重写很多东西。</p>
  </li>
  <li>
    <p>对问题的理解认识流于表面。无论在哪个学科，凡是相对理论一点的东西都会有不同层次的理解，比如在数值分析课上学到了几种基本的矩阵分解和特点。因为当时做了不少作业，所以并不像当年自己从网上找资料时理解的那么肤浅，但是依然没有理解到各种矩阵分解内在的关系。直到这学期学SVD时遇到一些证明题，从课上的仅有几个理论完全难以下手，学长看完就可以瞬间联想到从Schur分解的结论推过来，从而解决了Singular-Value的一些问题。我才意识到我之前自己为是的学习成果是多么不堪一击。</p>
  </li>
  <li>
    <p>浪费更多的时间。一些公认的优秀学习资源（比如某某教授上的课）不一定比自己查阅资料的方式学习效率更高，但是一定是在兼顾理解深度和广度的最高效做法。随着知识的深入，自学的人往往会遇到很多理解瓶颈，可能需要比上课更多的时间重新理解，才能突破瓶颈。</p>
  </li>
</ol>

<p>正如前面所说，科研是在消耗课堂学过的基础知识，科研虽然听起来高端，但深度一定比不上那些基础知识，新的idea是否可行，从理论模型的层面上一定能找到答案的，基础知识薄弱的人，往往会为一个idea的可行性被迫重新学习一些资料，因为被迫，这些临时学来的东西又难以转化到另一个idea的判断上。</p>

<p>说了这么多，我无非是要表达，在学术上，那些看似捷径的成功做法都很难成就长久的成功。那些片面看重科研重视程度而忽略教学的做法对于学校和希望从事科研的学生个人的长久发展都是不利的。</p>

<h1 id="section-1">大学排名</h1>

<p>除了批评德国过于重视“教学”之外，其他的批评声音很多都指向大学排名。毕竟大学排名是最客观、可量化的标准。但是既然标准化，很多特殊因素就可能很难被完全考虑进去。RWTH的Informatik专业会有10个左右的研究所，负责教学和科研活动。然而在亚琛的强项方向：形式化、逻辑的1所和7所，都只有1个教授，1-3个讲师。其余全是博士生。但是这一个教授的水平往往是领域内奠基人。相比之下，QS排名更高的KIT，每个研究所会有5-10个教授。</p>

<p>RWTH的自然语言识别处理的研究所也是只有一个教授，1个讲师，然而教授是机器翻译领域几个奠基人之一，Google的数据中，他的引用数有30000+，在一个相对不算太水的领域，这个数字已经能说明一些问题了。如果在亚琛想做自然语言相关的学习或者科研，你一定会频繁地接触到Ney教授，比如读博士，你的导师只能是Ney，当然从任何角度来说，这都比去排名更高，跟水平一般的教授要划算得多。</p>

<p>这些东西是大学排名反映不出来的。</p>

<h1 id="section-2">本科生水平与课余生活</h1>

<p>最近因为感觉自己在理论课的思维方式和助教相差很大，可能是知识结构导致的，所以最近开始狂补本科生课程。最大的感受就是德国的本科教育非常的填鸭。必修课的内容2年学完，剩下一年上一上选修课，而且大部分人还学有余力，可以提前修Master的课程。最可怕的是，他们的高中毕业水平和中国学生相差太多，基本中国一般的学生过来大一都能拿到相当好的成绩。然而2年之后，德国学生的理论水平就比我这个学了4年本科的学生高出不少了。以至于自己如果不找时间补他们的本科课，Master的选课空间就小了很多。</p>

<p>具体来说，一门叫做 Diskrete Struktur 的第二学期本科课程（直译就是“离散结构”）。可能和我们的离散数学比较像。但是书中分的三个大章节分别是组合数学、图论、代数，只不过都是比较基础的概念。回顾一下，虽然自己好像也学了一个学期的离散数学，不过也不比他们图论的部分多什么了。</p>

<p>因为他们本科生的课程紧，作业多，所以一方面他们本科期间没有时间参加类似于ACM竞赛之类的课外活动；另一方面因为长期接受这样的填鸭教育，导致在Master和PhD阶段尽管基础扎实，但是缺乏自主性，成果并不多。</p>

<h1 id="section-3">形式化</h1>

<p>形式化向来是法国的特点，然而不知道为什么，外国学生中比较喜欢理论方向的来到亚琛都会有“过度形式化”的感受。不仅仅是理论方向，比如授课时，每个新知识都是先给出定义，然后就是理论和证明，中间很少会有具体的例子，以至于对定义的理解要通过理论的证明来猜。经过一番猜测之后，对这一理论有了一些理解，发现它并不难，完全可以从一个感性的角度更轻松的理解。</p>

<p>另外一个表现就是很多偏工程课程也会大量把自己的概念去靠向形式化。一方面这种形式化本身就是不严谨的，另一方面，把问题形式化的意义是使用形式化方法解决问题，然而这些问题明明直接去理解会更简单。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[更通用的角度理解程序与数据]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program/"/>
    <updated>2014-12-01T00:22:00+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program</id>
    <content type="html"><![CDATA[<p>本学期有一门叫做“Fondation of Data Science”的课，打着Data Science的幌子来讲数据库，让我完全对7所的理论课兴趣大减。究其原因，大概是它的某些先修课程我之前没学过，所以学起来吃力。</p>

<p>尽管如此，其中的很多观点还是值得借鉴和思考的。现在回想本科比较重要的课，大概有这么几门：编译原理、数据库、计算机网络、操作系统。我认为包括美国在内的计算机专业比较强的学校都在教育过程中弱化各个领域的关系。这当然从某些角度来说是有好处的。但是从更抽象、更理论的角度来看，不利于学生对于本质问题的理解。
<!--more--></p>

<p>比如数据库，这个比较晚才出现的概念，在正式用来解决并发管理、高效操作之前，它也被看做解决问题的通用模型的一部分。具体来说，就是<strong>我们认为世界上所有的数据都存在关系数据库里面，所有的程序都是SQL写的</strong>，因为我们假设这就是最基础的通用模型。我们为什么不假设别的语言呢，比如C或者Java？因为现在我们是要站在数据的角度来思考，而关系数据库是相对好理解的模型中最基本的，其他的NoSQL数据库模型算不上理论模型，因为它们更自由，缺少约束。既然已经确定了关系数据库，那么SQL显然是最合适的程序语言。但是基本的SQL并不能表达其他语言的所有逻辑，最基本的，就是递归。因为SQL和代数表达式，一阶逻辑等价（证明方法很多，这里不赘述），所以我们也可以换个角度理解一个概念，一阶逻辑（First-Order Logik）像是一维空间，没有层次。我们需要“函数”来完善SQL来实现递归逻辑。<a href="http://en.wikipedia.org/wiki/Datalog">Datalog</a>就是这么一个东西，既包含了一般的以细节的规则，每条规则是一些关系表达式的Intersection，这是不是和编译原理中的文法从逻辑上很类似？因为我们写的文法是针对Context-Free Language的，所以是不是现在又打通了语言和程序的关系？回到Datalog上，因为左边的变量可以出现在右边，所以我们可以实现递归了。但这并不完全是我们要的，因为要保证其单调性，所以Datalog不能包含“非”运算，所以要使用它的升级版Inflationary Datalog。相比之下，传统的SQL从理论层面上来看简直弱爆了。</p>

<p>现在再回到程序语言上来看，编程语言最本质的性质不是循环Loop、选择if，而是一阶和高阶，也就是是否包含递归（而不仅仅是函数，函数可以不实现递归）。至于“循环”和“选择”只是为了更适应人类和业务思维而产生的，即使没有他们，也可以用其他的方式用递归来代替。这就是函数式编程所谓“最高级”的地方，他们采用的都是更“元”的操作。</p>

<p>说到这，就不得不提Prolog——我打算写这篇文章的初衷。在基本了解这门语言的语法之后不会意识到这门语言的强大，但当我们具备了一些数理逻辑的知识之后，发现这门语言就是在用一阶逻辑编程。在以形式理论发展如日中天的时候，几乎所有科学家都认为逻辑推导就是构造世界的基本方法。对于形式化推导求值方法，可以了解<a href="http://en.wikipedia.org/wiki/Ehrenfeucht%E2%80%93Fra%C3%AFss%C3%A9_game">Ehrenfeucht–Fraïssé game</a>。同样的，对于任意一个问题，只要我们构造出这个问题的基本事实和推到规则，用术语来说就是公理系统(Axiom System)和赋值(Interpretation)，我们就可以判断这个子世界的任何命题的真假，前提是你需要了解理论(Theory)是由公理系统分形出来的无限谓词句子的集合。回到Prolog，我们只要向它输入所有世界的规则，就可以知道任何可以推断出的问题。然而这种最直观的构造世界的方式并没有被工业界认可。其原因在我看来有几点：</p>

<ol>
  <li>尽管它更加直观，但是从表达能力上来看，并不比其他语言更强大。我们完全可以用其他语言代替，即任何常见的编程语言都能实现与之等价的功能。</li>
  <li>理论上更直观并不代表应用上更直观，很多简单的问题并不需要抽象出它的实施和推理规则就能解决，或者说，大部分我们能接触到的问题并没有太多的逻辑规则。</li>
  <li>每种问题会有特殊的解法，我们的目标是解决问题，而不是把所有问题标准化成一种问题，标准化的过程会把问题复杂化。</li>
</ol>

<p>真正理解这几点之后，才不会让一些略懂人工智能的人们对人工智能盲目崇拜，对形式逻辑盲目崇拜。而我们之前接受的教育似乎完完全全的把这些东西避开了，以至于让一些求知欲更强的学生在课外学习中以为自己了解了更高深的东西而发出更幼稚的言论。</p>

<p>说到通用，前面已经提到数据库是表述数据相对直观的最通用的模型。然而在更早的时候就出现了一个更加通用的模型用来表述数据的输入、输出、中间过程——那就是用01纸带做记录的图灵机。图灵机最大，或者说唯一的意义在于分析程序的复杂度，或者叫实现难度。这是一项相当底层的任务，所以最好也要用相当底层的数据表示来完成更为恰当。具体原因可以这么解释：所有较难的复杂度分析几乎都是基于reduction去和已知复杂度的模型建立联系，也就是说，越是底层的表示，reduction的自由度也就越大，具体方法可以参照<a href="http://en.wikipedia.org/wiki/Reduction_%28complexity%29">Reduction</a>，而P != NP并不是一个已经被证实的问题，尽管我们感性上能理解，但不存在这样的方法来证明，我们找不到NP到P的reduction并不能代表不存在。</p>

<p>提到复杂度，可能也要顺便提一下，时间复杂度NP和P并不是衡量复杂度的唯一标准，在其之上还有PSPACE和NPSPACE，只不过是把占用的空间作为评价标准，刚了解时有点毁三观。之后有时间我会单独写一篇关于复杂度的东西。</p>

<p>关于通用，还有一个我一直记错的概念，就是“通用图灵机”，他只是一个可以模拟图灵机的图灵机，即把需要模拟的图灵机的状态转移表用二进制表示作为字符串输入给通用图灵机，让其模拟。它就像一个编译器，读一段代码，然后执行这段代码，高明的地方就是两种代码用同一种语言写的，也就是说，我们可以再用一另一个通用图灵机来模拟这个通用图灵机——即用编译器来编译执行编译器。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[确定世界与概率世界]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie/"/>
    <updated>2014-10-24T21:45:20+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie</id>
    <content type="html"><![CDATA[<p>这篇没有涉及具体的理论知识，主要是建立在部分相关经历之上的一些臆想，所以多少会有些民科思维。的确，我在这方面的经验少的可怜。但是相比于能看到我博客的人来说，信息不对称还是非常严重的。从刚入学我就多次表达了我的震惊。我发现计算机科学还有这么大一片领域我几乎一无所知，最可怕的是，我所谓的一些“基础和见识”可能也就是当地大二学生的水平。经过了一个暑假的沉淀和反思，我对于形式理论的极端态度渐渐冷却，所以，我也希望我能把它放到和其他领域平等的地位上考虑一些它们之间的关系。</p>

<!--more-->
<p>如果我们想表达两件事情A和B的必然因果关系，形式方法是A-&gt;B，而概率方法是p(B\|A)=1。这就是两个世界的建立基础。所谓“世界”并不是从学科发展的角度，即，不是按照先学p(A)，然后学p(A,B)最后学条件概率。而是按照世界运转的“元操作”。说到这，可能还是要提一下“世界”的动与静。世界可以是一个生态环境，一个程序中的类，也可以是一个人的个体。而“动”和“静”（或者叫行为与状态），可以类比成“算法”和“数据结构”，“类方法”和“类参数”，“代码”与“数据”。不过要认识、分析和解决计算机科学根本的问题还是要放在最根本的模型上。我说的当然不是图灵机，而是比图灵机更早，而且标定计算机能力疆界的——形式逻辑。</p>

<p>从计算机出现甚至更早，整个计算机科学的发展都是建立在形式理论的基础上，比如程序语言的设计、数据库的设计。到了后来，计算机的计算能力已经达到可以真正为人所用。于是就有了大量人开始用计算机来搞数值分析，并因此诞生了一些图灵奖得主。这些就扯远了。总之，到了90年代，单纯的形式理论已经没再出现真正的突破。</p>

<p>形式理论正如前面的例子中提到的，是一套完备的体系，一切都是确定的。当然，“真正的智能无法实现”也是被确定的，因为二阶逻辑的不完备性在图灵机诞生之前已经把计算能力的极限定在了那里。
而无法实现的原因可以直接参照停机问题的矛盾句式。</p>

<p>于是，当我们发现一个世界的运行规则被基本挖掘干净，并且没有找到我们真正需要的东西时，就尝试在确定性世界的框架下加入不确定性，也就是概率来完善形式理论的一些不足：</p>

<ol>
  <li>
    <p>一个世界的规则是确定的，不能随着外部输入而变化，一个世界需要更多的可能性。尽管随着概率的加入，我们可以通过修改参数的方式修改世界的一些规则（一个具体的例子就是通过样本来训练机器学习模型的参数），然而制定规则的世界依然是确定的。概率的加入只是帮助我们从应用的角度解决了更多具体的问题，对于真正的理论研究上的“智能”并没有什么帮助。</p>
  </li>
  <li>
    <p>真实世界是概率的，至少我们对于一个值得研究的子世界（所谓子世界，就是真实世界中我们需要研究的对象集合的子集），其中的个体都是概率的；或者说全世界是确定的，其原子行为构成一个闭包。但对于真实世界的任意一个有限子集，都不是封闭的，所以我们只能把其中的一部分当成随机变量来考虑和认识。通过从“元”上达到和真实世界的一致可以帮助我们更好的转移现实世界的内容到构造世界。尽管我们依然构造不出绝对的智能，但是已经可以用简单的智能结合真实生活，完善生活中的问题，这样可能更有社会意义。</p>
  </li>
  <li>
    <p>我们可以在假设存在一个真实世界的子集，并且在这个我们要研究的子集是一个闭包，即，一切都是确定的。但问题就变成，为了尽量真实的模拟现实世界，需要足够大的子集，以至于超过了计算机的运算能力，难以继续，一个实例就是神经网络算法的发展瓶颈。</p>
  </li>
  <li>
    <p>确定性世界并非一无是处，因为其关系的建立都是稳固而确定的，所以整个世界体系也是可靠的（往往是无限大），而概率世界更多的是表达隐含关系，不确定关系，所以单纯由概率搭建的世界（比如贝叶斯网络）不会很庞大，仅仅是为了解决某类特定小问题。</p>
  </li>
</ol>

<p>当然概率帮助我们简化了一些“确定性”解决起来很棘手的问题，但是根本性问题永远无法解决。也正因为这一点，形式化理论研究很早就进入了瓶颈，而且创造不了价值，所以时至今日，只有为数不多的欧洲大学把它作为重要计算机专业最重要的基础知识传授给学生。然而学习它却对于理解计算机科学从宏观上能有最重要的价值。</p>

<p>如今，“单纯的形式理论”已死，也不会有人太多人研究“真正的智能”。更多的是将其和统计学方法结合，解决生活中具体的问题。相信随着更多复杂的形式理论的引入，我们能在未来构建更加完善的“世界”。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[一年咖啡龄感受总结]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/23/yi-nian-ka-pei-ling-gan-shou-zong-jie/"/>
    <updated>2014-09-23T04:40:00+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/23/yi-nian-ka-pei-ling-gan-shou-zong-jie</id>
    <content type="html"><![CDATA[<p>尽管从大三开始就已经开始喝咖啡，不过在国内，更多的还是笼罩在速溶咖啡的阴云之下。速溶咖啡能比较受欢迎，一方面是因为比较好买到的速溶咖啡都比较甜，大众能接受；另一方面也是因为它确实有一定的提神功效。不过后来到了德国，比较甜的速溶不是那么普遍，所以也就趁超市打折买了一个滴漏咖啡壶，从此开始喝起了咖啡粉。</p>

<p>德国的咖啡粉比较好买到，而且一般一包也就4欧左右，即使每天喝也差不多能喝两个月，比较实惠。</p>

<!--more-->

<h1 id="section">适应</h1>

<p>开始的时候因为太苦，所以每次都要往咖啡里加一些白砂糖或者牛奶。大概适应了一两周，开始习惯直接喝，不添加其他食物。直接喝的原因也很简单，就是为了能够更充分的享受咖啡粉本身的香气。后来也很多次尝试添加糖，打些奶泡，不过咖啡的香气却被削弱了不少。</p>

<h1 id="section-1">健康</h1>

<p>相比于茶，咖啡可能更适合火气旺盛的西方人。在过去的一年里，我每天平均能喝一杯多的咖啡，有时候是因为兴趣，有时候是提神需要。喝完之后，一个最常见的反应就是利尿，所以经常会感觉身体缺水。如果完全空腹喝，也会身体过于亢奋而不能专注。最严重的可能也是对它的依赖——很长一段时间都会很享受刚起床喝一杯咖啡的时刻。在这一年中，我也莫名其妙的发烧了3次，最严重的一次嗓子疼到不能喝粥。用中医的说法应该就是火气太重，上火了。在最后一次发烧之后，我又尝试加入些牛奶，感觉对胃部刺激就轻了很多，所以加奶咖啡的另一个好处就是能缓解上火。</p>

<h1 id="section-2">效率</h1>

<p>尤其是到了后半年，因为学习压力太大，每天都是被作业牵着鼻子走，所以必须要用咖啡提高效率，否则在思考一些数学题时完全没有任何进展。然而，咖啡虽然短期内效果明显，可以把大脑疲惫期拖延到睡觉的时间，然而时间一长，推迟效果越来越不明显，甚至喝完之后，本来还算清醒的大脑直接就疲惫了，然而身体还处于亢奋，所以没办法通过睡觉消除疲惫，只能等咖啡的效果消失了再睡觉，反而更耽误时间。</p>

<p>所以，当这些负面效果逐渐明显时，我会稍微克制几天，通过多喝水和跑步来缓解。</p>

<h1 id="section-3">咖啡胶囊</h1>

<p>有一次超市胶囊咖啡促销，借机买了一台胶囊咖啡机，从此喝咖啡变得更简单。通过网上了解的一些信息，咖啡胶囊的品质整体会比普通的粉品质高一些。不过刚开始喝得几天反倒觉得胶囊更苦，而且也没有更香。适应了一周，慢慢觉得两种方式冲出的咖啡应该说是口味不同，各有千秋。</p>

<h1 id="section-4">口味</h1>

<p>无论是滴漏式咖啡机还是胶囊咖啡机，从口味上都难以和手冲相比，所以时间充裕的时候我也会和手冲咖啡。个人感觉手冲咖啡一方面因为水和咖啡粉能够更充分的接触，另一方面漏下的咖啡没有被持续加热，所以味道更香，酸涩感更淡。</p>

<h1 id="section-5">免疫</h1>

<p>有些人咖啡喝多了会有免疫，我觉得有些时候可能真是因为咖啡喝的不够多。我对自己冲的咖啡就从来没有免疫过，但是昨天下午在一家咖啡厅不听服务生劝阻的喝了两杯Espresso却没影响到我晚上睡觉。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automaton to Regex]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/12/automaton-to-regex/"/>
    <updated>2014-09-12T04:12:15+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/12/automaton-to-regex</id>
    <content type="html"><![CDATA[<p>前面提过了Angluin学习算法的基本原理和作用，通过不断回答程序提问的句子是否属于该指定语言来生成这门语言对应的最简DFA。然而DFA的表示并不是很直观，也不实用。所以我今天介绍一个方法，把生成的DFA转化成对应的正则表达式。
<!--more--></p>

<h1 id="section">问题</h1>

<p>通过之前在网上搜索，没有找到比较完美的解决方案，但是从正则表达式到DFA的方法却有很多。我想“DFA -&gt; Regex”的主要难点有两点：
- 没有一个好的方式化简(合并)生成的正则表达式，或者说不能把任意正则表达式形式化的转化成一个特定的范式。然而对于任意NFA都是有范式的，即“最简DFA”。所以我经常觉得正则表达式是一个不太好的设计。
- 存在多个互相可达的终止状态的情况会丢掉一些句子。</p>

<h1 id="section-1">相关工作</h1>

<p><a href="http://cs.stackexchange.com/questions/2016/how-to-convert-finite-automata-to-regular-expressions">这里</a>有一个方法实现自动机到NFA的转化，是针对更广泛的NFA，但是限制也很大，就是不能完美解决多终止状态的情况。其实现方法可以归结为：<strong>每一步删除一个中间状态k，并且对于每对状态i，j，如果存在i-&gt;k，k-&gt;j，那么删除的时候用L[i][k].star(L[k][k]).L[k][j]来完善L[i][j]，其中L[i][j]是从状态i到j的所有语言，初始情况两个状态间的语言最多只包括了一个字母。最后计算初始状态到终止状态的语言就是结论</strong> 这么做的问题就在于如果有多个终止状态，那么他们都不会被删除，但是最后结果会漏掉状态start -&gt; f1 -&gt; f2的语言，其中f1，f2是终止状态。由于在剩余的状态集合中可能存在多种路径，而且不能继续删除，比如f1 -&gt; start -&gt; f2，因此情况会变得非常复杂。</p>

<h1 id="floyd">Floyd算法计算状态间语言</h1>

<p>我的想法是不删除中间状态，而是通过Floyd算法直接计算每两个状态之间的语言，听起来似乎和删除中间状态效果一样，不过用Floyd的好处是能把终止状态作为中间状态的情况也考虑全面。这样，最终的答案就是对于每一个终结状态f，<script type="math/tex"> result = \Sigma_{f \in F} L_{start,f} </script></p>

<h1 id="section-2">计算过程使用结构元组而不是直接用字符串</h1>

<p>在更新状态间语言的过程中，<strong>L[i][j]</strong>不是直接用字符串格式的正则表达式来存储i，j之间的语言，因为用字符串尽管不会影响结果的正确性，但是会给字符串化简制造麻烦。比如消除冗余括号。所以我使用了tuple来代替字符串来描述正则表达式的”树结构”，描述只使用’+’, ‘.’, ‘*‘三种结构，分别代表“语言集合的并”，“语言的连接”，“出现任意次”。当然，这个符号定义和平时使用的正则表达式有区别，只要修改程序的输出函数即可。比如(‘+’, [‘a’, ‘b’, ‘c’])代表一个有4个节点的树，生成的语言L只接受aa,b,c三个句子。</p>

<h1 id="section-3">冗余消除策略</h1>

<p>用Floyd算法遇到的最大问题就是会制造很多语言的冗余。下面说几个主要策略，不过并不能保证完全消除，其中一些策略的正确性是建立在“输入DFA是最简DFA”的基础上的。</p>

<ul>
  <li>消除空结构，比如(‘+’, []), (‘+’, [’’])，这种结果本身不会造成错误，但是会在计算过程中造成冗余</li>
  <li>消除重复括号，当一个结构例如s=(‘+’, [‘1’,’2’])作为子结构出现在另外一个结构中时，并且主结构包含多于一个元素，这时要给子结构的字符串形式加括号，比如t=(‘.’, [s, (‘*’, ‘b’, ‘c’)])</li>
  <li>消除不必要的循环变量运算，在Floyd算法中需要枚举变量k, i, j。然而根据k和i，j的等价关系可以省略一些运算，例如如果k和i相等，那么当<script type="math/tex">L_{i,j} \neq EMPTY</script>时，可以化简运算为<script type="math/tex">L_{i,j}= star(i,i).L_{i,j}</script></li>
  <li>最简DFA，这个优化并不需要我用代码实现什么，而是作为一项限制，用来证明我很多看似不严谨的代码的正确性。例如不会有两个等价的终止状态。</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[考试总结]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/08/kao-shi-zong-jie/"/>
    <updated>2014-09-08T16:33:34+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/08/kao-shi-zong-jie</id>
    <content type="html"><![CDATA[<p>中秋佳节之际做一些小学生做的事情：考试总结。</p>

<p>这学期总体来说收获很大，但是和成果并不匹配，也就是说成绩很不理想，或者说和我之前的经验恰恰相反，唯一一门考试资格都没有获得的课是我本科算上加分之后拿到满分的“编译原理”，而且两次课程内容基本一致。唯一一门挂掉的MA课是我自认为本科期间做过很多相关工作（包括两次实验室经历和一次实习经历）的“数据挖掘”。通过的3门都是之前接触不多，反而最后能低分飘过。下面总结一下原因：
<!--more--></p>

<h1 id="section">基础知识薄弱</h1>

<p>现在回头看看，确实觉得这一学期学的很辛苦，但是实际上那些被奉为神课的课并没有想象的那么难，即使缺少一些必要的基础知识，也是可以通过以前学习的考试技巧来Hack一下。尤其是理论方向的课，现在想想，每门课研究的“实体”其实都有章可循。大概都是归为相关性质与相关实例，性质引出实例，不同的实例又有不同的新性质。另外，通过考试内容可以发现，很多性质的证明并不重要，不需要深刻的理解。</p>

<h1 id="section-1">题型不适应</h1>

<p>基本每门课都不会给太多复习资料，对于第一次来德国，完全不知道如何复习，很多国内通用的复习策略在这里行不通。比如没有办法通过作业中的题目本身来判断是否可能在考试中出现，尤其是第一门的数据挖掘，看了一遍卷子就直接傻了。</p>

<h1 id="section-2">时间安排不合理</h1>

<p>考前一方面有些轻敌，一方面回家心切，所以5门必须的考试被安排在15天里，以至于第一门发现策略失败后，后面完全没有时间调整。另外一个很主观的原因就是这学期记忆力大不如前，连写带画的看了5遍的课件，考试时还是记不住。</p>

<p>时间安排的另一个方面就是平时上课写作业，每天被作业牵着鼻子走，最后只是为了草草完成作业，难点没有时间搞懂，重点也没有时间回顾。</p>

<h1 id="section-3">考试与学习的协调</h1>

<p>这也是最让我最难以权衡的。正如和在国内一样，应对考试必须要有很多额外的时间去刷题，背一些没有启发性的知识，并不是你把课上的东西领会了或者有所推广就一定能考试。所谓启发性，也就是我最感兴趣的。而这些启发性又难以用标准化的题目去考核。所以到了最后，就不得已放弃很多思考启发性问题的时间去背一些标准化的概念。</p>

<p>上个学期总体说来时间安排还算合理，或者说幸运，每天一门作业。在接下来的学期上课撞车更严重，恐怕要更加努力了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数理逻辑如何改变我的世界]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/08/20/how-mathematische-logik-changes-my-world/"/>
    <updated>2014-08-20T18:25:31+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/08/20/how-mathematische-logik-changes-my-world</id>
    <content type="html"><![CDATA[<p>在大二上学期，我自认为对程序员的工作有了足够的了解——无非就是学语言，学库，即使是涉及到其他专业领域，也只要掌握最基本的概念，然后调用对应的库就可以了。于是“程序员”这个概念更多的是对经验的要求——对各种库的用法掌握、编码风格的统一等等。与之相对应的就是我自认为之后的所有课程就是背概念，因为计算机科学的基本轮廓都是围绕着编程的。</p>

<p>不过当我真正学习编译、操作系统以及自己后来看一些工业界用机器学习解决实际问题的文章的时候，忽然发现，我才刚刚了解计算机科学是在做什么。</p>

<p>Master的第一学期学习了数理逻辑之后，我的世界观进一步被摧毁。我在之前从没认真想过“为什么计算机科学可以看做数学的分支？”“计算机科学在过去几十年都有哪些成果？”“为什么说图灵是计算机之父？他的成就都有什么价值？”这类的问题。这些问题似乎成为了计算机科学系统的公理，不需要证明，也无法解释。虽然并不是每个问题都能直接从“数理逻辑”上找到答案，但是很多它却给了我们一个入口，让我们看到计算机科学目前很少被关注的一面。</p>

<!--more-->

<h1 id="section">计算机科学与数学</h1>

<p>如果去问刚上本科的我，为什么说计算机科学是数学的分支，我多半会说“因为算法、数据结构都是数学问题，计算机科学经常要处理一些数学问题。”这个回答内容不算错，不过答非所问，因为既然说“分支”，必然是有“根”有“叶”。计算机处理数学问题最多只能说“计算机和数学关系密切，计算机因为其性能特点，能帮助解决一些数学问题”。然而体现不出计算机理论如何建立在数学之上，或者说计算机有什么核心理论是从数学得出的。</p>

<p>如果说有的话，我觉得“数理逻辑”应该是其中最重要的一门。很多基础学科都会有一些理论能普世万物，“数理逻辑”因为其主要其应用领域算到了计算机科学下面，但很多核心概念都是能普世万物的。比如公理系统，公理系统并不只作用于数域，任何<strong>理论(Theory)</strong>都有可能会有有一个公理系统。公理系统具体来说就是描述一个理论最小的句子集合，以至于建立在该理论上面的所有句子都能通过这个集合的句子组合判断真伪。而一个理论，正是一个无限的句子集合。建立在这个基础上，我们就可以轻易判断一个理论有没有可能被证明。这仿佛给给各学科开疆拓土，只要有公理系统，那么任何问题都可以用形式化方法判断真伪，当然也不会有什么“无法证明的问题”了。然而并不是每个理论都能被公理化（具体细节专业性较强，在此不做讨论），而且对于自然学科，总会有新的发现，因此公理系统也就不断变化，即使是已经被证明的理论，也会在之后被推翻。</p>

<p>回到计算机与数学上面，正是因为数学与计算机对计算问题的研究都建立在一个较为封闭的系统，因此才会有价值。具体到编程语言上，可以把命令式编程语言中看成顺序结构、循环结构与判断结构看成它的公理系统，它与函数式编程的基础“Lambda表达式”是原子等价的，因此可以把他们具有同样的表达能力。</p>

<p>数理逻辑中另外一个非常有用的定理就是一阶逻辑的完备性定理。很多问题一旦涉及到“无限”，那么许多有限问题上的做法都不能适用了，比如我们不能对比两个无限字符串中某个字母的个数的长度奇偶性。而完备性定理正是帮我们把无限问题放在有限问题上解决。</p>

<h1 id="acm">数理逻辑与ACM</h1>

<p>在来德国之前，我从来没有怀疑过计算机领域还有什么问题是因为其模型抽象而难以理解，主要还是因为德国大学在ACM方面基本没有涉足。然而从数理逻辑课上遇到的很多问题，即使直接看答案也要思考几个小时才能彻底看懂。所以说，其实抽象的问题很多，只是通过编程方法能够表达出来的数量很有限。</p>

<p>另一方面，ACM涉及的知识散碎，各种算法间相互关联性并不大，所以每掌握一门新算法，收获是常数增长，从中得到的一些小方法或许能够很好地解决一些常见小问题，但是没有体系，无法深入。</p>

<h1 id="section-1">关于图灵机</h1>

<p>很多人都喜欢表达对图灵这一计算机科学鼻祖的憧憬，不过大多数人可能对其认识仅仅停留在知道图灵机的程度上，至于图灵机的意义和相关理论，感兴趣的人似乎不多。了解的多一些的可能还会知道它和自动机关系密切，受限图灵机是和自动机、以及Pushdown System等价的，然而根据市面上的为数不多的自动机相关资料，很少会提及它与数理逻辑之间的关系。事实上受限图灵机或者说与其对应的自动机课可以看成一个<strong>理论</strong>，也就是说，图灵机也可以被公理化的。在自动机领域有很多图灵机的变种，有些互相等价，有些不等价，甚至随便一次自动机考试的试卷都能看到一个新变种。要讨论清楚任意两种之间的关系可能方法各异，但是如果从最本源的角度——公理系统来考虑，很多问题一下子就简单多了。</p>

<h1 id="section-2">函数式编程</h1>

<p>与其要讨论“函数式编程”这种被讨论烂的话题，不如简单说一下相对不那么烂的“Lambda演算”。所谓的看似高端的函数是语言Lisp、Scheme、Haskell从抽象程度的角度来看都是Lambda演算的一个具体实现方法。那些希望通过掌握Lisp来标榜自己的人，与其掌握一个实现，不如首先想透lambda表达式相关的一些抽象问题。比如Lambda不允许自递归，而是通过不动点的方式。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Angluin-Learning-Algorithm]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/08/15/anluin-learning-algorithm/"/>
    <updated>2014-08-15T20:24:10+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/08/15/anluin-learning-algorithm</id>
    <content type="html"><![CDATA[<p>前两天在知乎回答了一个关于<a href="http://www.zhihu.com/question/24824487/answer/29196253">识别某种特定语言的方法</a>，根据这两天0赞同的情况，不禁让我对知户上计算机从业者的基础理论水平深表怀疑，不过当我看到第一名的答案时确实非常激动，因为一下子想起了2年前在知乎实习时为了识别某个语言，发现用正则表达式可能会非常麻烦，于是写了个自动机，当时刚刚从编译课上学了些自动机的原理，并且第一次正式尝试用它来解决实际遇到的问题，自然是非常激动，而第一名的方法也正是通过构造自动机来生成正则表达式。</p>

<p>不过在那之后，“自动机”与“编译”在我脑中基本成为两个非常相似的概念。我因为自动机的神奇而对编译中各种文法以及编译器的实现产生了兴趣。自认为目前做过最NB的一件事也于此相关——在看过龙书第一章，并且学习了推到规则之后，自己动手写了个简单的编译器作为大作业，不过大作业并没有因此得到满分，因为理论的欠缺，我甚至分不清lexical和syntax之间的区别，自认为高明的把词法分析、语法分析写在一起，并且跳过符号表；这些都被老师当成了扣分点。</p>

<!--more-->

<p>知道来到RWTH，一学期同时学了自动机与编译原理，才渐渐认识到两种理论其实各自自成体系，交集并没我想象的那么多。重学一边编译的过程从理论方面并没有给我更多的收获，这也让我第一次感觉本科上的课也不是都那么水，但是每周的作业中都包含编程题，也就是实现一个语言的一部分代码，当然这部分都是每周课上新学的，从词法分析，到LL(1)、LR(0)、SLR(1)、LR(1)一直到最后的语义分析和生成代码部分。相比于自己实现，这种方法显然更加“德国”，减少了像我一样“误入歧途”的风险。因为从这个过程中确实学到了很多编译器更规范的做法。</p>

<p>不过相比于编译原理，我从应用自动机课上收获的东西更多，以至于很多东西对我的世界观产生了很大影响，具体的很多想法以后在慢慢谈。今天就来说个具体的，也就是我认为这门课最正统，也是最NB的算法——Angluin’s Learning Algorithm。在回答问题之前我也把<a href="https://github.com/xuanhuangyiqi/AnluinLearning">项目</a>地址贴了上去。除了一些简单的介绍外，我今天想着重介绍下Angluin’s Learning Algorithm的核心：封闭性与一致性。不过在此之前，我还是要说，这个算法不是机器学习算法，不涉及概率，或者说每一步都是为了达到一个严谨的稳定状态。在混沌的世界，机器学习可以解决“一切”问题。这种说法是要表达：对于现实世界，机器学习因为对问题本身要求更低，所以解决问题的范围更广（因此看似神奇），然而也正因为其规律并不能适用所有，所以解决的效果无法达到完美。然而在理想化的形式世界，所有的问题都被抽象，形式化，虽然和现实脱离，但是却能在形式化的世界得到完美解决。下面就要正式介绍一下两个性质：</p>

<p>根据我在回答中的一些简单描述，大概可以把该算法解释为：<strong>通过反复询问新的词是否属于该语言来维护一个观测表的封闭性与一致性。</strong></p>

<h2 id="section">封闭性</h2>

<p>对于有两个词w,v 属于语言L，如果它们后面无论接什么词，都是要么同时属于L，要么同时不属于L，那么可以说这两个词的等价的，如果把他们放到自动机上面从初始状态开始跑一下，它们会停在同一个状态，那么这两个词就是等价的，比如对于正则语言(12)*，12和1212还有121212是互相等价的，1和121也是等价的。然而为了说明两个词的等价，我们并不一定需要找到所有词接在他们后面来验证他们是否属于这一语言，只需要几个有代表性的就够了，而要接的词也就是S集合（具体原因稍后再说），<strong>而R集合中的每一个词都可以根据后接S中的每一个词是否属于语言L来代表一个DFA上的状态</strong>，如果R中两个词w, v它们后接S中的每一个词都是要么同时属于L，要么同时不属于L，那么显然他们是等价的。如果我观测到有一个词R中的词w后接某个字母a之后不在R中，并且和R集合中的任意一个词都不等价，那么目前的DFA是不封闭的，说明wa代表了一个自动机上的新状态，那么通过R中加入wa来维护这个自动机的封闭性。</p>

<h2 id="section-1">一致性</h2>

<p>前面提到了两个词w, v等价的条件，并且我们认为S集合可以代表全集，来验证任意两个词的等价性，然而S集合虽然表面上R中的每个词是否等价区分开了，但其实可能是因为S集合不够大，并没有真正把两个不等价的词区分开，比如从现有S集合来判断，w~v（表示两者等价）。然而如果发现对于某个字母a，wa与va通过S来区分时并不等价，即对于S中的某个词s，was与vas并不是同时属于L，那么显然w和v也并不是真的等价，而区分它们的词正是as，所以通过给S添加元素as来维持自动机的一致性。</p>

<p>好了，虽然算法还有很多小问题值得思考与证明，比如知乎回答下vczh提的问题，但是最核心的东西应该是说清楚了，细节可以以后慢慢讨论。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[反面看极简生活]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/07/30/fan-mian-kan-ji-jian-sheng-huo/"/>
    <updated>2014-07-30T08:54:47+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/07/30/fan-mian-kan-ji-jian-sheng-huo</id>
    <content type="html"><![CDATA[<p>这篇来说一下自己对于极简生活的2年多的实践和成果。最重要的不是倡导人去追求极简，而是如何让它改善我们现有的生活。当然我的大部分观点和主流极简主义倡导者是有出入的。</p>

<p>我从两年前的豆瓣上看到“极简”这个词，当时还不太清楚极简代表着什么，指示看到一些优秀的极简风格设计，或者是品牌，比如MUJI，甚是喜欢。于是我决定尝试改善我的生活，让我能在学习工作中更专注于任务本身。最开始的几个主要方面就是删除手机上的多余app，减少生活用品，卖掉不必要的书，减少自己的信息源（比如当时的Google Reader），处理掉不重要不紧急的任务，改掉信息焦虑症。</p>

<!--more-->
<p>但在之后的一段时间，我发现这样的实践和我很多固有的观念产生了冲突，让我找不到好的妥协方案。比如对待无用或者劣质的生活用品，我从很早就一直在贯彻父母教导的节俭思想，只要还凑活能用的东西就继续用，不要轻易扔东西。然而按照极简生活的要求，我应该果断把它们舍弃，换成一些品质更高、数量更精简的替代品。由于那段时间我要花很多的钱学习德语，准备考试，同时也没有挣钱的途径，自然也不愿意再找父母要钱花在这种小事上，当时觉得极简就是有钱没处花的大人为了挣更多的钱，更舒服的生活才要考虑的。另外一方面极简要求摒弃无用的东西，但事实上，每个物品对于你都有两个维度：使用频率与重要性，另外还有一个获取难度。有太多东西是生活中使用频率并不高，但是每次需要都非常迫切，而且难以及时获得，比如指甲刀、体重秤、插线板。道理同样适用于一些手机app。另外一个问题是处在公共生活空间很难把自己的生活观念让周围的人认同支持，这里“公共生活空间”指的是宿舍或者二人世界，因为不是每个人都愿意按着你的标准来生活，同样每个人对不同物品的几个维度值也各不相同。</p>

<p>总结问题所在：极简并不能做到“极”，因为一个维度上的“极”会引发其他维度造成的问题。</p>

<p>不过幸运的是有一项实践是成功的，因为它实践成本很低。那就是极简我的硬盘。把文档、作品放到云端，常用的文件放到网盘同步，脚本与项目放到Github托管。至于图片没有找到完美的解决办法，目前采用的是放到iPhoto里面，并且把文件夹放在网盘同步目录来实现备份。</p>

<p>不过来到德国之后，发现极简生活的实践变得更加简单，因为德国人对于多样性并不那么在乎，或者说好的品牌很有限，也不会有淘宝用价格诱惑你买便宜而劣质的物品。经常的搬家也不会让你有机会考虑什么是更重要的。</p>

<p>最后一个成功的实践还是手机app。在德国除了查车意外，基本没有什么app是必须的。这也让我的app数量相比在国内大大减少。另外也开始把大量信息源转移到苹果原生的应用上，比如任务、日历、笔记。相比之下，苹果原生的应用做的更简洁，方便，省去了大部分传统优秀GTD应用的无用功能。最后，应用数量成功控制在一个屏幕之内。</p>

<p>当然以上只是一个细碎事务少的穷学生的个人极简生活实践。相信对于不同的人，都会有不同的最佳“极简”状态。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[lambda]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/07/16/lambda/"/>
    <updated>2014-07-16T13:39:35+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/07/16/lambda</id>
    <content type="html"><![CDATA[<p>复习无聊，写点东西，想起来昨天“数理逻辑”课上最后一道题有些意思，拿出来分享一下：</p>

<p>我们定义一个运算“◦”，它是一个二元函数。下面我们定3个结构（包括操作域与运算符）：
<script type="math/tex"> \mathfrak{A}_1 = (\{0, 1\}, ◦), a ◦ b = min(a, b) </script></p>

<p>\[  \mathfrak{A}_2 = (\{0, 1\}, ◦), a ◦ b = max(a, b) \]</p>

<p>\[  \mathfrak{A}_3 = (\{0, 1\}, ◦), a ◦ b = (a+b) mod 2 \]</p>

<p>对于$ i, j \in {1, 2, 3}, i&lt;j $ 给出一个FO范式$\phi$来使得对于任意$i, j$都有$ \mathfrak{A}_i \models \phi $ 但是 $\mathfrak{A}_j \models \urcorner\phi$</p>

<!--more-->

<p><a href="https://logic.rwth-aachen.de/files/MaLo-SS14/home12.pdf">原题目</a>中要求对于每个<script type="math/tex">i, j</script>各写一个<script type="math/tex">\phi_{i,j}</script>，本题提高了难度，用一个范式写出来，如果这个题用编程的方法实现，大概是这样的，我们假设了：</p>

<pre><code>for all a in {0, 1}:

if i == 1, j == 2 then
	return (a ◦ !a == 0)
elif i == 1, j == 3 then
	return (a ◦ !a == 0)
elif i == 2, j == 3 then
	return ((a ◦ a) != (!a ◦ !a))
</code></pre>

<p>显然，我们对于特定的<script type="math/tex">i, j</script>我们选择了尽量简单的区分两种运算符的范式，如果你能看懂上面的思路，那么下面就是要把这些东西依次转化成FO范式：</p>

<script type="math/tex; mode=display"> \forall a(((i=1,j=2)\land(a◦!a == 0))\vee((i=1,j=3)\land(a ◦!a == 0))\vee((i=2,j=3)\land((a ◦ a) != (!a ◦ !a)))) </script>

<p>不过在FO范式中，我们只定义了◦操作，所以要把取反“!”和常数“0”替换掉，例如我们定义取反<script type="math/tex">$\exists y(y \neq a \land \varphi)</script>$，替换常数0的过程省略。</p>

<p>下一步要替换掉每个if条件，我们只要判断这个范式不符合第(6-i-j)个结构的性质就可以了，比如对于i=1, j=2,我们可以判断$ ((a ◦ a) != (!a ◦ !a)) $。</p>

<p>这道题的改进版思路有两个重要的解题关键：替换常数和if。是不是感觉很熟悉？对！纯lambda算子，理解它的关键就是在于所有现代编程的3大控制结构：循环、选择、顺序是如何替换成lambda表达式的，以及在纯lambda中不出现常数，或者说是常量，有的只有函数。下面给一个简单的关于纯lambda思考题：</p>

<p>我们用$\lambda xy.x$代表常量True，$\lambda xy.y$代表常量False。写出下列函数的lambda形式：</p>

<ul>
  <li>and</li>
  <li>or</li>
  <li>not</li>
  <li>xor</li>
  <li>implies</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从一个梦中得到的关于生活中的加密方法]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/06/24/encrypt-from-dream/"/>
    <updated>2014-06-24T10:30:05+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/06/24/encrypt-from-dream</id>
    <content type="html"><![CDATA[<p>前天做了一个梦：在火车上，列车员来查票，给她看了我的车票之后，他让我在上面写一个密码，来证明这张票是我的。</p>

<p>在德国，买火车票的时候，有些车票并不是实名，所以要在车票上签名才能使用。因为德国上火车不需要检票，只需要上车之后列车员查票，并且一般不要求同时出示对应证件。而签名，似乎只是为了表示确认车票的使用者，而不能随便更改。这种措施显然没有什么意义，任何倒票、冒名用票的手段都不能被防止。</p>

<p>显然倒票只能从购票的时候控制，所以不做讨论。我们下面来说下__如何防止偷票坐车__。</p>

<ul>
  <li>按照现有签名的办法，那么在检票员查票时应该同时查车票和有效证件，然后保证车票签名和证件姓名一致，证件照片和持票人本人一致，从而证明票人一致。但是因为增加了“证件”从而可能会引发一些问题，比如恰好偷票人和购票人姓名一样（当然可以通过增加生日来区分），比如伪造有效证件，或者忘记携带身份证，身份证被偷……</li>
</ul>

<p>下面提一下我在梦中的做法，也就是计算机加密算法的基础————大数分解：</p>

<ul>
  <li>
    <p>首先想两个大质数，然后它们的乘积写到车票上，显然这个大数与两个质数唯一对应。查票人只要检查持票人能否写出两个质数，乘积为车票上的数即可。对于非法持有人，理论上是不能通过车票上的大数直接得到两个质因数的。</p>
  </li>
  <li>
    <p>显然上面的方法要求持票人记住两个大质数，难以实现。所以完全可以换成两个普通的对自己有意义的大数，检票时，只要向检票员证明分解方法和两个数分别对自己的意义。</p>
  </li>
</ul>

<p>嗯，这就是我前天做梦的内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[最近这三年]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/04/17/zui-jin-zhe-san-nian/"/>
    <updated>2014-04-17T21:00:34+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/04/17/zui-jin-zhe-san-nian</id>
    <content type="html"><![CDATA[<h1 id="section">前言</h1>
<p>两年前想着，考过德福之后写一篇最近这一年，来总结一下德语学习对于一个IT男的影响。当时万万没想到，真正拿到语言证明竟然是在那两年之后。今天收到了第二份足以让我入学的语言证书，才想起当初给自己的约定，于是写下此文缅怀本应该最珍贵那三年。</p>

<!--more-->

<h1 id="section-1">起因</h1>
<p>对于一心想毕业直接工作的我，从来没把学习与成绩看得很重要。直到大三即将开始的暑假，受到女友的怂恿，决定一起尝试一个最不靠谱的选择————去德国留学。我只记得当时虽然意识到这个选择一定是错的，但是我决定和她一起沿着错误的路走到正确。</p>

<h1 id="section-2">经过</h1>
<p>在做出这个决定之后，我们争取一切时间一起报班学德语。同时也决心拒绝一切可能会影响留德之路的一切机会。在学了一年德语之后，开始尝试参加德语考试，一年3次德福考试被我们考了个遍，最后还是成绩很低。最后拿着语言条件限制的录取通知书来德国读语言班，并和大多数来德国读语言班的同学一样，半年后通过，入学。</p>

<h1 id="section-3">最重要的两年</h1>
<p>对于我而言，最重要的两年就是大三、大四。这两年内的决定会直接决定我未来的人生轨迹。这两年，可以潜下心认真继续搞ACM，或许能拿到更好成绩，从而有更多机会去美国或者国内顶级IT公司；我也可以认真找个实习，或许可以去一家比知乎或者雅虎更好的公司；或者早一点去唐老师的实验室，做一些有意思的研究，也可能已经去了美国一个不错的学校。再不济，放弃一些德福备考的时间，多做些外包，也能让当时的生活有很大的改善。但是当时决定了一条路，就要一直走下去，如果中途放弃，拐了弯，不管新的路是否更加宽阔，“德语学习的经历”对我来说才是真正的走了“弯路”。所以，我能做的就是放弃一切可能影响我的诱惑。</p>

<h1 id="section-4">收获</h1>
<p>大一大二我过的顺风顺水，究其原因是和很多大牛们学到要学会“走捷径”，这样能节省不少时间做更多的事。对于不管是计算机、软件的学生，还是IT从业人员，这一思想根深蒂固。我大一可以用OpenCV实现看似很牛逼的识别功能，大二用脚本语言框架来做各种Web项目，但其实做到这些都不需要你懂得太多。这一观念，是直到我德语数次失败之后才意识到的。大三大四我尝试学习思考一些复杂的过程，比如理解函数式编程，比如用机器学习方法获得一些有趣的实验，还有理解，甚至背写ACM的一些复杂算法。但是浮躁的内心已经让我无法专注于这些真正值得在那个年纪做的东西。</p>

<p>学了德语，在德国上了课，才让我认识到，真正厉害的人都是能耐心的做枯燥的事的。那些看似很好掌握，有威力强大的东西，对别人同样很简单。这些想法，恐怕我如果不学德语，一辈子都无法意识到的。</p>

<h1 id="section-5">目标与规划</h1>
<p>刚刚开学，身边的人都在讨论着毕业之后的路，大多是选择读博，确实毕业也不过是2年之后的事了。而毕业对于我似乎还算遥远，我并没有明确的想做什么。只是觉得，来到德国或者说亚琛最难得的就是能与世隔绝的学些大部头的东西，这些东西是我在国内恐怕难以做到的。所以就趁这段时间，学一些以后恐怕很难再直接用到，却又非常重要的理论课。至于我以后要做什么，确实和我现在在学什么关系不大，在一些理论课上，我已经清晰地感受到自己的基本功有多麽不扎实了。通过一些感兴趣的课拿到足够学分之后，我再开始考虑以后的问题，相信到时候选择会更多。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[德语助手Workflow]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/02/03/godic/"/>
    <updated>2014-02-03T22:04:57+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/02/03/godic</id>
    <content type="html"><![CDATA[<p>前两天mac下的德语助手不知怎么变聪明了，发现我没有购买，于是彻底不能用了，断网重装都不能解决，于是果断放弃，当晚写了一个alfred的workflow来代替。缺点就是在线查询确实慢。。。</p>

<p><img src="http://xuanhuangyiqi.github.io/images/ScreenShot2014-02-03at9.58.55PM.png" /></p>

<p>废话不多说，上项目链接：</p>

<p><a href="https://github.com/xuanhuangyiqi/godic-alfred">项目地址</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[翻译：如何读论文]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2013/09/28/ru-he-du-lun-wen/"/>
    <updated>2013-09-28T14:02:01+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2013/09/28/ru-he-du-lun-wen</id>
    <content type="html"><![CDATA[<p>本文只翻译了论文第二三部分</p>

<h1 id="section">读论文的三个阶段</h1>

<h2 id="section-1">第一个阶段</h2>

<p>第一个阶段就是快速浏览文章，对文章有概括的认识。你可以决定是否进行下面的阶段，这个阶段大约需要5-10分钟，并且包含一下几个步骤：</p>

<ol>
  <li>仔细阅读标题、摘要和介绍</li>
  <li>阅读段落和子段落标题，但是忽略其他的一切。</li>
  <li>粗看数学的内容（如果有的话）来确定基本的数学基础</li>
  <li>读结论</li>
  <li>粗看索引，剔出一些你已经看过的。</li>
</ol>

<!--more-->
<p>在第一阶段结束的时候，你应该能够回答5个问题：</p>

<ol>
  <li>类别：这个论文属于什么类？测量论文？一个已有系统的分析？还是一个研究标准的描述？</li>
  <li>上下文：和哪些其他论文相关？有哪些理论基础被用于分析这个问题？</li>
  <li>正确性：这个假设正确吗？</li>
  <li>贡献：这个论文的主要贡献</li>
  <li>清晰：这个论文值得写吗？</li>
</ol>

<p>通过这些信息，你可能选择不继续读（并且不用打印出来，节省树木）。这可能因为你对这篇论文没兴趣，或者你对相关的领域了解的不充分，或者这些作者设定了错误的假设。第一个阶段对于不是你领域的论文已经足够了，但是某天会被证明很重要。</p>

<p>顺便的，当你写一篇论文时，你可以期望大多数审阅人可以只进行第一个阶段。小心选择清晰的段落和子段落标题，并且写简明综合的概述，如果一个审阅人在一个阶段之后不能理解主旨，这个论文就可能被拒绝；如果一个读者不能在5分钟内不能理解论文的关键点，这个文章就可能不再被读。因为这些理由，一个能通过选好的图总结一篇论文的“绘声绘色的概述”是一个接触的想法，并且可以越来越多的在科学周刊中看到。</p>

<p><a href="http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf">原文地址</a></p>

<h2 id="section-2">第二个阶段</h2>

<p>在第二个阶段，越多要有更多的关注，但是忽略证明的细节。这可以帮助你记住关键点，或者按照你读到的在旁边记下注释。 Augsburg大学的Dominik Grusemann表示，你“记下不懂得部分或者想问作者的问题”。如果你是一个论文评审人，这些注释会帮助你当你在写回顾的时候，和在一个项目会议中回忆你写的的回顾的时候。</p>

<ol>
  <li>仔细看论文中图、图表和其他图解。特别关注图形。它的坐标轴有标注吗？结果展示有错误条吗，他们有很重要的统计学意义。这些常见的错误会把匆忙、错误的从真正优秀的论文中分辨出来。</li>
  <li>记得为了以后的阅读，标记重要的未读会议。</li>
</ol>

<p>第二个部分对于有经验的读者应该占据一个小时。度过这个阶段，你应该有能力抓住文章的内容。你应该有能力根据支持的证据给别人总结论文的主要事实。这个详细的级别对于一篇你感兴趣的论文是合适的，但是在你的研究特长上面还不够。</p>

<p>有时候你即使在第二阶段结束的时候也不能理解一篇文章。这可能是因为这个主题的术语或者所写对你是新的。或者作者想用你不理解的证明方法或者实验技巧，所以这个论文的含量是不能预测的。这个论文可能是有没有证实的断言和大量的会议挤出来的。或者有可能多的时候太晚了，你很累了。你现在可以选择（a）把这个论文放一边，希望你的生涯中不需要明白这些材料（b）读完背景材料之后，等会再回到这篇论文中（c）保留现在的状态，进入第三阶段。</p>

<h2 id="section-3">第三个阶段</h2>

<p>为了完整理解这篇论文，尤其你是一个审稿人，需要第三个阶段。第三阶段最关键的是尝试“实际重现”这篇论文：就是说，和作者做相同的假设，重新做一边下面的工作。通过对比这个重新构造的过程和论文，你就可以不仅仅看出论文体现的创造力，而且还有没有写出来的失败和假设。</p>

<p>这个过程需要把大量精力放在细节。你可以了解并且挑战陈述中的每个假设。另外，你可以考虑你如何表述一个特定的想法。这个和虚拟与实际情况的对比能引导你对证明过程和展示技巧更加敏锐的目光，而且你非常可能把它加到你的工具指令中。在这个过程中，你可以略记下一些想法为了未来的工作。
这个过程对于初学者可能持续很多小时，对于有经验的读者也要多于一个小时，甚至更多。在这个阶段的最后，你应该有能力从记忆中把把想法重新构造出论文全部的结构，而且有能力识别重要和不重要的观点，尤其是，你应该有能力指出重要工作的隐含的假设，和缺失的引用，以及试验和分析技巧潜在问题。</p>

<h1 id="section-4">做文献调研</h1>

<p>论文阅读技巧在文献调研的过程中得到测试。这将需要你读10篇论文，可能在一个不熟悉的领域。你需要读什么样的论文？这里有你应该怎么用三阶段法实施。
首先，使用学术搜索引擎，比如Google Scholar或者CiteSeer，还有选择好的关键词去搜索3-5篇这个领域中最近高引用的论文。每篇文章进行一个阶段，然后读他们的相关工作的段落。你会发现一个近几年相关工作的简略总结。如果你能发现这样一个调研，你就完成了。读这个调研，祝你好运。</p>

<p>否则，在第二步，在参考书目中发现共同的引用和重复的作者姓名。这些是这个领域的关键论文和研究员。下载这些关键论文并且把它们放一边。然后去这些关键研究员的网站去看看他们最近在哪里发表。这会帮你识别这个领域的顶级会议，因为最好的研究员经常在顶级会议上发表。</p>

<p>第三步就是去这些顶级会议的网站，看看最近的进程。一个快速的浏览将帮助你经常了解最近高质量的相关工作。这些论文，还有你刚才放一边的东西，组成了你调研的第一个版本，完成这些论文的第二个阶段，如果他们都引用了一篇你之前没发现的论文，把它添加进来并且读完它，增量是必须的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS Crash Logs 基本调研]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2013/09/02/ios-crack/"/>
    <updated>2013-09-02T13:30:03+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2013/09/02/ios-crack</id>
    <content type="html"><![CDATA[<p>1个月的实习马上就要结束，本文主要介绍我在实习期间主要调研内容——iOS app崩溃时的Crash Log分析与总结。</p>

<h1 id="crash-log-">Crash Log 是什么？</h1>

<p>首先需要介绍下Crash Log的基本信息，在大家在使用iOS app时有时候会因为各种原因而自动退出，比如系统内存不足，比如程序本身的bug，比如越狱造成的系统不稳定。总之，凡是app的自动退出都会对应的生成一个log文件，记录系统崩溃时的系统信息和崩溃栈。如下是一个crash log的样子: <a href="http://xuanhuangyiqi.github.io/assets/Bildschirmfoto2013-09-02um2.25.41PM.png">Crash Log example</a>。</p>

<p>从图中可一看到，每一次崩溃发生时，会有一个Crash Log存储在iOS设备中，并且在连接Xcode时转移到mac上。</p>

<p>另外，看到下面各个Thread的函数堆栈中，能看到每行最右面有错误发生的<em>函数名</em>和对应的<em>行数</em>，如果你自己尝试做这个试验就会发现开始<em>函数</em>的位置是一个16位内存地址，过了一会才会自动被Xcode转换成函数名。</p>

<!--more-->

<h1 id="symbolicate">Symbolicate</h1>

<p>Symbolicate过程就是把对应的函数栈转化成函数名的过程，我们为了方便，在后面把这个过程叫做<em>解析</em>。这个过程是怎么实现的呢？</p>

<p>首先，无论是在模拟器，还是真机debug一个app的过程中，除了XXX.app之外，还会生成一个XXX.app.dSYM文件（准确地说，这两个都是文件夹），如果你想知道生成的这两个文件路径，可以在terminal下使用mac的内嵌find工具mdfind：</p>
<div>
  <pre><code class="bash">$ mdfind -name &quot;XXX.app&quot;</code></pre>
</div>

<p>它是Spotlight功能的核心，在你的系统有文件更新时能及时更新索引，因此速度还是很快的，当然它还可以根据其他所引进行搜索。</p>

<p>dSYM文件保存了这个app的符号表，即指令内存地址和源代码的函数名和行数关系。这个文件显然保存了app源码的大量信息，因此是不能随意公布的。当然，只有你本机上包含有app的dSYM文件，其中的指令地址才能被解析成函数名（这句话是不严谨的，我们后面在解释）。</p>

<h1 id="xcode-">Xcode 解析工具</h1>

<p>解析的工作分为很多步，我们来介绍几个Xcode自带的相关小工具：</p>

<ul>
  <li>atos，用以获取某内存地址所对应的函数名。用法：</li>
</ul>
<div>
  <pre><code class="bash">$ atos -o /path/to/XXX.app/XXX.app.dSYM/Contents/Resources/DWARF/XXX -arch i386 0x973000</code></pre>
</div>

<p>这个函数是指定了arch的，即i386，这是模拟器（或者说mac）的处理器arch，对于iOS设备就是armv6／7。如果你指定的arch和app以及dsym文件的arch不对，是会报错的。那么如何知道一个app的arch呢？前面我已经说了最常见的两个，如果你不确定是哪个，就可以用</p>

<ul>
  <li>dwarfdump</li>
</ul>
<div>
  <pre><code class="bash">$ dwarfdump -u ~/Library/Developer/Xcode/iOS\ DeviceSupport/5.0.1\ \(9A405\)/Symbols/System/Library/Frameworks/UIKit.framework/UIKit</code></pre>
</div>

<ul>
  <li>symbolicatecrash 这是symbolicate过程的核心脚本</li>
</ul>
<div>
  <pre><code class="bash">$ /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/Library/PrivateFrameworks/DTDeviceKit.framework/Versions/A/Resources/symbolicatecrash</code></pre>
</div>

<h1 id="section">动态库与静态库</h1>

<p>在学习C语言的时候，我们就应该接触过这两种库，静态库是在编译时把其中涉及到的指令一起放入最终生成的app，动态库则是在运行时动态查找库的。在iOS开发中，动态库的常见形式是dylib或者Framework。</p>

<p>无论是app还是Framework，在生成的时候都是有特定的arch的，这是可以用dwarfdump命令可以查到。但是app会把符号表放到单独的dSYM文件中，库文件的符号表会在自己内部。这就是我现在不能理解的问题，只要有足够多种类的语句，是可以通过外部app的调用把库的每条语句解析出来的。这是不是一种不安全的设计？</p>

<h1 id="crash-log">现有Crash Log服务</h1>
<ul>
  <li>QuincyApp 主要用于app发行前的crash管理</li>
  <li>Crashlytics 界面友好</li>
  <li>Crittercism 统计内容详细</li>
</ul>

<h1 id="section-1">解析过程</h1>
<p>在执行解析的过程中，首先从dSYM文件中找到app编译时所调用的动态库的名称、UUID、arch。然后利用mac自带的mdfind功能在全局搜索是否存在对应的库，只有完全匹配才可以解析。目前遇到的问题是如果使用模拟器制造crash，那么crash文件底层库的内容不会被解析出来，已经判断出是因为Xcode.app文件下的arch为i386的Framework的UUID与app的dSYM的不符。具体原因需要进一步了解。而真机测试时调用的库文件在另外的文件夹，似乎不会出现类似的问题。</p>

]]></content>
  </entry>
  
</feed>
