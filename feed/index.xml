<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Htedsv Backyard]]></title>
  <link href="http://xuanhuangyiqi.github.io/feed" rel="self"/>
  <link href="http://xuanhuangyiqi.github.io/"/>
  <updated>2016-05-21T19:35:15+02:00</updated>
  <id>http://xuanhuangyiqi.github.io/</id>
  <author>
    <name><![CDATA[htedsv]]></name>
    <email><![CDATA[xuanhuangyiqi@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Booking Hackthon]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2016/05/21/booking-hackthon/"/>
    <updated>2016-05-21T19:26:56+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2016/05/21/booking-hackthon</id>
    <content type="html"><![CDATA[<p>收到Booking的offer已经有一段时间了，觉得应该有必要记录一下这次奇特的Hackthon经历。</p>

<h1 id="section">起因：</h1>
<p>在找工作的半年里，我逐渐意识到每天写工程的代码对于我的算法快速实现的能力还是多多少少有些负面影响，于是为了保持coding的手速和注意力，我偶尔会在网上刷一些算法比赛。尽管在找工作之初就对Booking的Data Scientist职位很非常有好感，但是立刻就因为没有正式工作经历被拒了数次，于是也没有抱太大希望。今年2月在Hackerrank上看到Booking的算法比赛，并且有机会拿到一件Tshirt，就决定试一试。
<!--more--></p>

<h1 id="section-1">初赛：</h1>
<p>比赛只有5-7道题，24小时完成，题目难度并不大，但是题目描述有很多不清楚的地方，所以导致有些细节理解错了，耽误很长时间，最后因为罚时太长而排在了满分的最后几名，大概排在50多名的位置。之所以锲而不舍的做完，也完全是因为只有满分才有机会拿到衣服。除了几道算法题之外，如果想去总部参加复赛，还要提交一份构建推荐系统的提案，纯属开放问题。考虑到之前面试Google时被评价为方案不够scalable，而被拒的经历，这次我的提案非常简单，尽管涉及了非常多的因素，但是用到的数学工具只有贝叶斯估计和高斯分布。花了一个小时用Latex草草完成之后就草草提交了，毕竟开始就对于去总部没有抱太大希望。</p>

<h1 id="section-2">复赛：</h1>
<p>一边做毕设，一边等T-shirt的做了大概一个月，收到HR的通知希望了解我的详细信息决定是否让我参加复赛。电话上大概给HR介绍了下我的情况，在德国和荷兰边境上学，即将毕业，在找工作，HR表示他的同事看了我的推荐系统提案表示不明觉厉，所以应该会让我去总部进行复赛。一周之后，收到正式的通知邮件。并且简单确定我的行程之后，就收到了火车票（因为距离太近，实在没有合适的飞机）和酒店的确认邮件。尽管比赛完整流程只有2天半，但是Booking可以提供5天的住宿，这让我觉得Booking应该是个土豪公司。</p>

<p>4月中旬，在小鱼放假回到德国的第二天，我就坐火车离开了德国，当晚8点左右到达阿姆斯特丹，因为毕竟不是第一次来了，所以没有安排逛红灯区等计划，早早去酒店checkin，配置我的开发环境，毕竟当时对于比赛内容还一无所知。</p>

<p><img src="http://xuanhuangyiqi.github.io/images/98ace3b6f08ed564505fe6fe9fba82ba.jpeg" /></p>

<p>第二天等到下午3点，才到只有一街之隔的 The-Bank 里面的那家Booking进行第一次kick-off meeting。本以为会是人山人海的场面，结果发现一共入选最终决赛的只有8个人，这让排名50+的我着实有些吃惊。不过后来也能理解，HR估计只是打算找一些来自不同文化背景的人来比赛，而我又是中国人中花费最小的，还不用另外办签证。其余的7个人中4个来自东欧，一个来自巴西，一个印度，还有一个德国小哥。其中只有我和德国小哥是学生，其余的都是在职程序员，我们这8个人的共同点是都有一些ACM经历，东欧的几个还参加过Final。
本以为这会是Booking一次挺大的活动，结果只是一个HR租了一件会议室，并且连同几个负责API的工程师知道我们这个活动。这次meeting开始是HR简单介绍下Booking的文化，近期目标——大力发展中国市场，然后负责API的同事进来分别介绍下自己的职位和API的基本信息，接着我们依次介绍我们想要实现的想法，我提了很多个，但是最后因为API的限制，大部分都被我否定了，然后第一天就结束了，之后就是大家一起去一家墨西哥餐厅吃饭，看了眼菜单，大概每人40+欧的menu加一些前菜和饮料，席间和那些工程师们扯淡，聊了聊中国文化、荷兰的生活和欧洲好玩的地方之类的东西，另外还和在德国哥廷根学数学的小哥聊了聊他们那边的校园生活之类的。</p>

<p>晚餐结束之后，各自回到酒店，于是开始了调试API的活动，并且基本构想了想要实施的计划。第二天一早在Booking门口集合，然后就正式开始coding了，总共的coding时间之后1.5个工作日，也就是8+4个小时，这和真正意义上的24小时Hackthon相差很远，不过据他们所说，这也是乐趣所在——用最好的精力进行这场比赛。当然，第一天下班回家之后也可自己在酒店继续写。</p>

<p>之前在北航时组队参加雅虎的Hackthon拿了奖，所以自认为自己还算比较适合参加Hackthon这种形式的比赛。然而API的限制实在太严重，再加上Booking已经把我能想到的大部分功能都已经实现，所以最终的idea是我在第一天下午3-4点的时候才临时决定的，于是时间变得异常紧迫，再加上为了尽量精美的展示我的idea，我在网上找一个很特殊的js插件用了很久，并花了很长时间来debug（毕竟自己在前端方面还是新人）。</p>

<p>好在最后一切都提前做完了，相比于其他通过算法筛选出来，没有太多前端开发经验的选手，我的想法的创意和完成度应该算很高了。然而最终展示时，我还是被一个名不见经传的东欧大叔的精美作品震惊了。果然还是自己太naive了。</p>

<p>最后没有评奖，只是叫来了一些Booking的一些中层领导来旁听提问，然后一人发了一盒巧克力作为奖励。接下来进行招聘介绍，我才知道这次比赛是Booking办的第二届，我也应该是第一个参加现场的中国人。在之后，大家就被拉到Booking的一家合作酒吧里喝酒聊天，了解了一下我感兴趣的几个工程师的奇葩经历，在欧洲工作的很多工程师其实并不比湾区的差，只是每个人有不同的生活追求而已，同时更让我对Booking这家在德国名气并不大的公司非常佩服。喝到晚上9点，各自回酒店，第二天大家各奔东西。</p>

<h1 id="section-3">后记</h1>

<p>一周之后收到HR的邮件和电话，本以为要联系进一步面试的情况，结果是直接发offer。</p>

<p>在Booking的一些印象：中午在Booking食堂吃了两顿饭，感觉比Google Zurich的要好吃一些，大概是因为Zurich的物价太高，所以待遇变差了？而且大概是因为两地物价房价差异，Booking位于Amsterdam的几件office都比Google Zurich位置更好，内部更宽敞。不过员工待遇来说还是Google更好，如果最后不能成功转成Data Scientist，我还是更希望去Google。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Theoretical view of Regularization]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2016/03/27/theoretical-view-of-regularization/"/>
    <updated>2016-03-27T16:53:56+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2016/03/27/theoretical-view-of-regularization</id>
    <content type="html"><![CDATA[<p>Around two weeks ago, I attended a job interview, which is ML-related. During the interview, I was asked to explain SVM and regulizers. When I said the classical SVM didn’t involve empirical risk term in its objective functions, the interviewer was shocked. This experience inspired me to write this blog to summarize the some of the aspects of regularization techniques.</p>

<h1 id="introduction-to-regularizaiton">Introduction to regularizaiton</h1>
<p>In many introduction courses of machine learning, it’s always referred that regularization terms are considered in order to avoid overfitting problem. The most common way is to write objective function as follows:</p>

<script type="math/tex; mode=display"> f(\theta) = R_{emp}(X, \theta) + R_{structural}(\theta) </script>

<p>Intuitively, the risk (or called “error”) of a model come from two parts, the empirical risk is a measure of how different is the behaviours of the parameters and real data set, the structural risk is caused by the model itself. The structural risk is relatively difficult to understand. It’s mainly due to the high capability of the model. The capability or so called complexity is the main topic of <em>statistical learning theory</em>. Roughly speaking, <strong>we don’t want our model to be too capable</strong>, or not fit the current data-set very well. The reason can be explained from two aspects: </p>

<ul>
  <li><em>Deviation of the training-set and real data distribution</em>. Because of the limited size of training set, we can not find any possible data point from hypothesis space in the training set. More specificly, if the estimated parameters are too complex, the unlabeled data points (maybe in the test-set) may differ much from its real output. </li>
  <li><em>Existing noise in data-set</em>. Data are not always clean, the existances noise and outliers requires the model not to fit noisy data very well.</li>
</ul>

<p>While the measures of empirical risk and structural risk are independent, we often use a trade-off parameter to combine them together as:</p>

<script type="math/tex; mode=display"> \hat{\theta} = argmin_{\theta} R_{emp}(X, \theta) + C \cdot R_{structural}(\theta) </script>

<h1 id="svm-as-regularizer">SVM as Regularizer</h1>

<p>In most practical machine learning books, SVM is described as a very common classifier. Even everyone can write down the objective function as a combination of the two risks. But only a few people know how it comes to the current form.</p>

<p>As we talked in the last paragrah, regulizer aims to reduce high complexity of the model (we will discuss the <em>complexity</em> later on). Usually the regularizer term is auxiliary in the objective function. But for SVM, regularizer is the only term that matters. The empirical risk term is dispensable to deal with noise.</p>

<p>The origion SVM didn’t take noise into account, the only goal was to maximize the distance of  two parallel hyper-planes. As we know, if they are separable, the hyperplane definitely exist, so the empirical loss is 0. The only goal is to minimize the structural risk under the restriction that each training sample is correctly classified. In terms of <strong>Structural Risk Minimization</strong>, you can refer to <a href="http://www.svms.org/srm/">SRM</a>. The original goal to maximize the distance between $y=w^Tx + b + 1$ and $y=w^Tx + b - 1$ under the restriction that every point is correctly classified is formulated as follows:</p>

<script type="math/tex; mode=display"> \min_w w^T w </script>

<script type="math/tex; mode=display">% &lt;![CDATA[
 \begin{align*}
\text{subject to:} & (w^Tx_i+b+1)y_i \ge 0 \quad & \forall i: positive\\
& (w^Tx_i+b-1)y_i \ge 0 \quad & \forall i: negative
\end{align*} %]]&gt;</script>

<p>In order to tolerate noises, we add empirical terms to allow some points to appear between hyperplanes.</p>

<h1 id="choice-of-regularizer">Choice of Regularizer</h1>

<p>In many examples explaining <em>underfit</em>, <em>fit</em>, <em>overfit</em> we have seen how regularizer helps solve a optimal regression problem. But for classification problems, there are no general principle in terms of definition of regularization terms. For the existing regularization terms, we seldom see how each theoretically minimize the structural risk. For example, the objective function of decision tree problem involves the regularizer as the number of nodes in the tree, the more are the nodes $n$, the large is the penality. Obviously there are more alternatives. We can also use $e^n$. For both cases, we can find out a suitable trade-off parameter $C$ to balance the two risks. But ideally, which means all error come from noise, the best $C$ is independent on the traning set. However, the best $C$ must be different if we change the penality from $n$ to $e^n$. </p>

<p>The next question is whether there is alternative to combine the two risks instead of the summarization. Maybe product?</p>

<p>But the most annoying question is $L_1$ or $ L_2 $. In most cases we use $L_2$ to penalize outliers, but why we use $L_1$ in SVM to tolerate noise?</p>

<p>The answer to the previous questions can be concluded that <strong>it’s not necessary to be theoretically accurate</strong>. Machine learning is an engineering anyway.</p>

<h1 id="overfitting">Overfitting?</h1>

<p>A question that I have been ever asked is like “If you see overfitting with the current parameters, how to change your trade-off parameter”. I was therefore confused: is overfitting observable? Ideally it is definitely right theoretically if the objective function with respect to complexity of the model is convex. The convexity implies, there is a optimal complexity that minimizes the global error. Practically it’s sometimes wrong. To take SVM as an example, if we see higher test error rate than training error rate, there may be due to the overfitting. Now if we reduce the value of $C$ (reduce the weight of the tolerance), the distance between the two hyperplanes would absolutely be increased. But how about the discriminate boundary? It all depends on the distribution of the noises around the hyperplanes. Therefore, whether the generality of the model increases is still unknown. </p>

<h1 id="modern-regularizer">Modern Regularizer</h1>

<h1 id="dropout">Dropout</h1>
<p>In the most of traditional statistical models, regulizer normally acts as an additional term in the objective function. But in order to tacle the side effect of the most popular and effective learning machine – Deep Neural Networks, new kinds of regularizer should be proposed. As mentioned in the paper [1], the most popular regulizer <em>Dropout</em>, is a variation of model averaging. Comparing to averaging serveral independent neural networks, Dropout is much easier to implement. Due the the popularity of Dropout, we don’t need to explain it in details.</p>

<h1 id="l2-normalization">L2 normalization</h1>
<p>One of the simplest way to apply L2 constraint is adding penalty as the L2 norm of the weight vectors at each layer, but instead, the second strategy is limiting the upper bound of the L2 norm of the weight vector of each layer. Each time when the constraint is violated, the vector is re-normalized.</p>

<p>[1] Hinton, Geoffrey E., et al. “Improving neural networks by preventing co-adaptation of feature detectors.” arXiv preprint arXiv:1207.0580 (2012).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expressiveness]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/12/05/biao-da-li-expressiveness/"/>
    <updated>2015-12-05T11:16:24+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/12/05/biao-da-li-expressiveness</id>
    <content type="html"><![CDATA[<p>The first formal contact to the concept “expressive” is in the course “Fondation of Data Science”, after that I prefer to use the words “Computational Power” to understand it, because at that time, I was more familiar with computational models, such as automata, turing machines, first-order logic etc. Also it measures the models from the perspective of computational functions. But later I found the phrase “computational power” is the word and somewhat ambiguous, so I gave up this saying. Due to the fact that, the models I was familiar with are mostly for the determination, that is, to accept one input and only returns a “yes” or “no” model, so for the “expressiveness” is more appropriate. Let’s look at a example of natural language: “I am a smart and friendly person”, this sentence can be used to judge the authenticity of the right, if the “and” and similar expressions deleted from natural language, then there is no way to express this sentence, or that the original expression expressiveness weaker than the latter, so that the theory used to explain the “expressiveness” of the concept is more reasonable.</p>

<p>As mentioned above, this translation “computing capabilities” is ambiguous because it’s similar to “computability”, but there are big differences from their definitions. “Computability” is a set of classes can be calculated using a turing machine or its equivalent models, the answer to this question is only “yes” or “no.” So computability is a judgement which takes turing machines as a tool. However expressiveness is more likely to be a contionous measurement.</p>

<!--more-->

<p>Also another thing that is worth mentioning is the decision problem. All computable functions can be converted to its corresponding indicator function. For example, $f(x) = y$ can be converted into $F(x, y) = 1$. And we say “expressiveness”, while we are really talking about the “expressiveness” of an indicator function. This is why, I compared automata, first-order logic, which can only return true and false, with turing machines, which seems more flexible. All these models are comparable if we only consider their equivalent indicator functions.</p>

<p>From this point of view, we can answer a question, why the automata are often used as the simplest model in compiler theory of syntax analysis, or for regular expressions, however they act as a research field of theoretical computer science.</p>

<h1 id="automaton-and-turing-machine">Automaton and Turing Machine</h1>

<p>I have ever answered the two questions at Zhihu: “Can an automaton have infinite states?” And “The most important component to a turing machine is the state transition table, is the unlimited tape also necessary?.”</p>

<p>First of all, in terms of the historical development, automaton was developed earlier than the turing machines, but now even nobody pays attention to the automata, which has a simpler structure than a turing machine. Instead, Turing machine was proposed as the beginning of computer science. The reason is, because turing machine is the first implementation of “mechanical movement” of first logic, and the automaton though more a simple and easier simulation, but can only represent limited ability, compared to turing machine, or to say the automaton is weaker than Turing machines.</p>

<p>In fact, the two issues mentioned above are essentially the same, we took the latter as an example to explain from the ability to express it:</p>

<p>First turing machines and automaton actually from operations is essentially the same: According to the current state and determine the transition rules, only transformed into another state. The difference is that state machine is a clear definition of the state, and the state of the turing machine is “Configuration”: (tape contents, the pointer position, status). Suppose the turing machines only have finte tapes, then the possibility of tape and the pointer positions are limited, also for the definition of turing machine, the states is limited as well, so for every turing machine’s configuration, we can construct an automaton such that, the number of states of the automaton is finte, and behaves equivalent to the turing machine, so turing machine is no more expressive than automata. Furthermore, the power of turing machines lies in the infinite taps.</p>

<p>The first question to answer is relatively not trivial, because within “infinite”, it could be divided into countable, uncountable, uncountable and can continue to continue to divide. Almost have to consider each case separately, it is not so simple.</p>

<h1 id="expressiveness-and-analyzability">Expressiveness and Analyzability</h1>

<p>No matter for mathematical logic, or computer models that are usually discussed, an important conclusion is that “the stronger is the expressiveness, the weaker is the analyzability.” For example, given the facts, all expressions of first-order logic is decidable, while second-order logic is not. On the other hand, the expressions of the second-order logic are stronger than the first-order logic. This conclusion can be extended to any logical and computational models. To take formal languages as an example, the expressiveness of star-free languages is weaker than it of regular languages, and the latter is weaker than context-free languages, but thefact is star-free languages are equivalent to groups, regular languages are equivalent to semigroups. As we know, semigroups has less constraints than groups, also they are less analyzable.</p>

<p>Of course, this conclusion is true for turing machines, on the one hand, we hope to invent a model can express a wealth of ideas, on the other hand, we have to lose the contronl of the model. we even can not propose a general method to test whether a program can stop or not in theory, let alone to judge whether a program has bugs.</p>

<p>It is also because of this trade-off, a lot of weaker expressive models survive and not been completely replaced in some areas.</p>

<h1 id="parametric-machine-learning-and-expressiveness">Parametric machine learning and expressiveness</h1>

<p>Here that we focus on the parametric machine learning models, which implies parameters can all be estimated from machine learning models. Nonparametric methods such as KNN are not discussed. from the point of view of whether the parameters of a machine learning problem can be learned, we can add a property: “learnability”. Such as state transition tables of automata and turing machines, which can be seen as different from the other parameters, the parameters of SVM is the dividing plane of $w$ and $b$, the parameters of a decision tree is the sequence of rules, and so on. Can these parameters be learnt? Obviously, if it is, we seem to see an foundamental problem of machine learning - learning a turing machine. As the theoretical model with the strongest expressiveness, if the argument turing machine can be learned by some algorithms, and intelligent thinking can be represented by an arbitrary computable function, then it seems to have arrived at the end of artificial intelligence.</p>

<p>However, even if we simplify the problem down to a certain terminating turing machine, the problem is still unsolvable. In order to explain, of course, we must first define a reasonable “learnability”, we define it with the criteria PAC-learnable. For any small error $ \delta &gt; 0 $, we can find out a training set size $ N $ such that For any sample set with greater than $ N $ samples, we can garantee that the error is less than $ \delta $ parameters. However, for one of the most classic PAC unlearnable problem: a countable set on a finite number of points can be expressed as a discriminant function of a turing machine. It implies, this class of turing machines are not learnable. As a result, It seems this idea from the most basic level is unachievable.</p>

<h1 id="the-expressiveness-of-deep-learning">The expressiveness of deep learning</h1>

<p>Previous articles have mentioned some of the theoretical reasons of success of deep learning, but did not discussed very specificly. Here again we talk slightly in terms of expressiveness of deep learning.</p>

<p>In the seminar depth learning in last semester, I chose the topic “Recurrent Neural Networks and Convolutional Neural Networks”, which is regarded as one of the largest estimate subject, because two topics can be easily extented to a series of sub-problems, and finally the result is that I didn’t catch the professor’s interest points. At the end of the slides, I was asked to present the relationship between the two neural networks from any perspectives, even though they are not really linked to each other. Finally I chose the option: the ability to express. That is, with the same structures (the same number of layers and the same number of neurons, connections are not necessarily the same), RNN, ordinary DNN and CNN, have the expresiveness RNN &gt; DNN &gt; CNN. This conclusion is very obvious, because they are decreasing with the number of parameters, and the parameters of the latter is is a subset of the former. However this conclusion obviously does not make sense, but to introduce more machine learning model as comparisons, we may have a new understanding. </p>

<p>To take SVM as an example for comparison on image recognition, before Deep Learning became ad-hoc in 2006, SVM + kernel was an efficient and sophisticated methods for a very long time in many areas. All high-dimensional classification problems can be mapped to a linear seperation problem if there is a suitable kernel method. However, this conclusion is not totally correct, in “<a href="http://www.iro.umontreal.ca/~lisa/pointeurs/PBR_chapter.pdf">On the challenge of learning complex functions</a>” of Y. Bengio, it mentions the kernel as machine learning almost the most common method has limitations in the simulation of complex structured functions. From this perspective, the non-linear kernel as the surface is mapped to an arbitrary dividing hyperplane method from the expressiveness is not perfect. From another side, it also explained the underlying reason of the success of deep learning - traditional models can not have the expressiveness.</p>

<p>Go back to the two common deep neural networks RNN and CNN, we can say they are enhancing their expressiveness from different ways. The reason differs with the specific issues, such as CNN is extracting local feature hierachically, RNN is handling time-dependence.</p>

<p>Although the enhanced expressiveness of is the trend, but not true for any cases. In machine learning, standing on its opposite position is generalization. In particular, for training of 100 samples, we use a 10-layer neural network is clearly inappropriate, the resulting parameters are very likely to be over-fitting, the reason is the inbalance between the number of samples and parameters. It also explains why in early years when number of samples is limited, neural networks didn’t perform very well.</p>

<p>After all, the big trend is the development of expressiveness. But the issues which really bothered academia and industry are often beyond the expressiveness, such as generalization problems, and lack of adequate data.</p>

<p>As neural networks ability to express the most interesting question - neural turing machines, we will discuss in detailes in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Standard form of algorithmic problems]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/10/16/wen-ti-luo-ji-suan-fa/"/>
    <updated>2015-10-16T20:02:30+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/10/16/wen-ti-luo-ji-suan-fa</id>
    <content type="html"><![CDATA[<p>Almost every student, who studies computer science at an early age are exposed to “algorithm” concept, but everyone often regards “algorithm” to solve a specific problem in a sequence of operations. The “operations” are always in the term can be described as several consecutive nested steps, and finally can be converted into a command-style code, including sequential, looping, selecting  structures.</p>

<p>After exposure to functional programming, the “algorithm” has become a broader understanding. Functional programming algorithm is more like a direct description of the problem itself, rather than one by one step. For example, in Haskell algorithm to achieve quick sort, ideas and details is much simpler than imperative programming: in each step, filter out the elements that are larger and smaller than as the standard, send them to the recursion, and concate their results. Because of this characteristic of functional programming, when many people introduce functional programming, they always referred to “learn functional programming without learning algorithm because functional programming itself is the description of the problem.” After I was taking the words as a goal, and finished learning the course Funcitonal Programming (and Logic Programming later), and found such a sentence has a strong misleading. Although functional programming is simply a description of the problem, but You must be in accordance with functional programming (and logic programming) in their own way to achieve this. There is a problem which relates to “conversion cost”, for many algorithms which are able to be achieved within 10 lines of code, may cost a very long time and code to be translated into functional programs. On the other hand the advantage of the “algorithm” is that they can optimize the performance, that is, as long as you learn algorithm very well, any programs can be completed within a short enough time to run.</p>

<!-- more -->

<p>To come back to our topic – That is how to treat almost the most important concept of computer science - algorithm. Although the way of understanding “algorithm” in the previous statement is nothing wrong, but as an important part of science, algorithm seems too fragmented, it has no structure, framework. Such as the same called algorithms, for machine learning learning algorithms (such as EM) and  shortest path algorithm in graph theory, there is no way to find a general form simultaneously to summarize them. Another more specific example is the algorithms in ACM/ICPC contest, from junior high school students to the programmers, they more or less contact with some books or online tutorials to introducing algorithm contests. The most common point is they are divided into several sections: a section roughly speaking about greedy method; a section about graph algorithms; and a section about dynamic programming, and so on. But in fact there is no link between the chapters, the only clue can link them is - they are easy to study even without any research background, and flexible computer algorithms. Or that they are simple algorithms in computer science projected to the dimension “complexity to study” , and then classified by subjects. If the student only learn algorithms from this perspective is clearly one-sided.</p>

<p>Back to the question itself, which when I was thinking in the course of optimization, is there a way to unite all the algorithms in the general sense? Of course. In machine learning the concept is called “objective function”, in continuous optimization it’s called the standard form .</p>

<p>From a more general point, for all deterministic algorithm, or algorithms, called a clear objective, can be divided into the “optimization of certain functions”, and “check the the feasible solutions”. Then they can be again united, the result is that all deterministic algorithms can be described in a standard form of optimization problems to indicate that:</p>

<script type="math/tex; mode=display"> \min_x f(x) </script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
    \text{subject to:} &g_i(x) \le 0 \\
    &h_j(x) = 0
\end{align*}
 %]]&gt;</script>

<p>More generally，we ignore $h_j(x)$, the problems of finding feasiable solutions can  be described as standard forms without $f(x)$.
While $x$ is the input，$f$ is the to be optimized function. To take backpack problem as an example:</p>

<script type="math/tex; mode=display"> \min_x \sum x_i w_i </script>

<script type="math/tex; mode=display"> \text{subject to:} \sum x_i v_i - V \le 0 </script>

<p>x_i equals 0 or 1, indicates being chosen or not. After thinking again, we find there is still another problem: the domain $x_i$ only contains 0 and 1, which is not observed in the standard form, it will lead to a more complex problems - integer programming. It’s too complicated to be explained in this ariticle. </p>

<p>Another example of optimization algorhtim to solve problems in standard form that I can think of is <a href="http://research.microsoft.com/pubs/69644/tr-98-14.pdf">SMO(Sequential Minimal Optimization)</a> algorithm, The people who have already studied SVM(Support Vector Machine), knows that, the goal of SVM is maximizing the distance between paralleled straigt lines，Then with the help of Lagrange Duality we can transfer the problem to solving the support vectors，the basic idea of SMO is iteratively select $a_i$ $a_j$ under the constraints to minimize the objective funciton，until the vector $a$ approaximates to the optimal.</p>

<script type="math/tex; mode=display"> \min_a \Phi(a) = \min_a {1 \over 2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j (x_i \cdot x_j) a_i a_j - \sum_{i=1}^N a_i </script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
  \text{subject to: } & a_i \ge 0 \\
  & \sum_{i=1}^N y_i a_i = 0
\end{align*}
 %]]&gt;</script>

<p>From the point of view of Optimization to understand algorithms is that every problem to be solved by algorithms can be described in a standard form, which in fact, in some universities, the matimatical students study algorithms from the lecture “algorithms” with computer science students, but from the course “discreate optimization”, which both covers algorithms and computability and complexity theory. </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Infinite Computation]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/05/26/wu-xian-ji-suan/"/>
    <updated>2015-05-26T12:22:03+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/05/26/wu-xian-ji-suan</id>
    <content type="html"><![CDATA[<p>This is a course from last semester, to advantage of the holidays, I’ll write a short summary. Although it doesn’t revolutionize my world view like computability theory and mathematical logic, but as a relatively basic theoretical course, it did inspire me a lot.</p>

<p>I choose the course at the beginning, partly because it is the offered by mathematical logic institut, partly because the name seems bluffing. Because any discrete problems once placed under the assumption of “infinite”, it will become much more difficult, most of the solutions for the “infinite” problem becomes less applicable. For example, determining a complementary set of a regular language is trivial. But to construct the complementary set of infite regular language becomes sophiscated.</p>

<p>The relationship between “finite” and “infinite” is not only opposite. In some scenarios, they are equivalent. completeness theorem of propositional logic, which is to build the equivalence of infinite set to an arbitrary finite set for determination, so that the unsolvable problem becomes solvable. </p>

<!--more-->
<p>However, although the contents of the course can be extended to any general computing problems, the objects discussed in the course are formal language, the most is regular languages, indeed it is very consistent with the style of mathematical logic institut: Any terms in theoretical computer science are the term “formal language”. “Language” is the fundamental targets of theoretical problems, code, data, to some degree, are all “languages”. However, the perspective of “language” is not suitable for all computer problems, the problems seen as the “language” does not help solve any algorithmic problems. If the logic, the language, the turing machines as a general approach to solve the problem, then the algorithm is the special and specfic method solve the problems. Logic, turing machine only focus on the possibility, while the algorithms (and complexity theory) is the only tools to the problem in an efficient way. Exactly because the problems in the world is not possible to be solved by a general tool, even more methods appear to lead to the scientific prosperity. “Do not use one tool to solve all the problems” not in the computer science, but are applicable also in any discipline.</p>

<p>Back to the formal methods, there is an equivalent transformation of finite regular language between DFA and NFA, but for infinite languages, DBA (Definite Buechi Automaton) and NBA (Nondefinite Buechi Automaton) is not equivalent, but they are in the hierarchical relationship; and within DBA, there are also a number of additional levels. Compared to the finite languages, the language level of an finite number is more ugly, but also to deal with a number of specific issues is more difficult, such as transformation of the NBA acceptable language into DBA.</p>

<p>The courses offered by mathematical logic institut brought me another understanding, which is the equivalence between several models, also the existence of the correspondence of each language to some kinds of automata. Some complex languages would correspond to Pushdown Automaton, even correspond to a turing machine, first-order logic, second-order logic; even some logic under another system: Linear Temporal Logic. From a more general point, “finite” or “infinite” language corresponds to the determination problem of the function, and even machine learning models: neural networks, decision trees, linear classifiers, SVM.</p>

<p>In the last lesson, one thing is mentioned is very interesting - correspondence between infinite languages and the rational numbers, which is also a connection between continuous space and discrete space - Cantor Set. its existence proved the correspondence between rational and real numbers does not exist.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[From computability to learnability]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/03/07/cong-ke-ji-suan-li-lun-dao-xue-xi-li-lun/"/>
    <updated>2015-03-07T11:03:37+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/03/07/cong-ke-ji-suan-li-lun-dao-xue-xi-li-lun</id>
    <content type="html"><![CDATA[<p>I have said many times before, computational theory is the root of computer science, it is also the reason of the difference between mathematics and computer science. On the border of computer science and mathematics, the one side of computer science is computational theory, the other side of mathematics is mathematical logic.</p>

<p>Computational theory mostly focused on very specific issues - the “generality”, without any conditionality, some questions are whether or not computable. Two of most common examples are the halting problem and equivalence problem: Is there a mechanical method can accurately determine whetehr an function can stop and it is determined whether two turing machines are equivalent (“equivalent” is, whether they are  calculating the same function). The answer and proof of the first question can be found anywhere: undecidable. The second question is more difficult to determine, strictly speaking, we should construct a reduction: $H \leq Eq$. An easy way to understand it is that to say the turing machines calculating the same function, we must enumerate all possible inputs, and determines whether the output is the same or not, however the input set is infinite, so the process does not stop. Since the determining process of the previous two questions by our brains is somehow heuristic, which implies, the strategy is not possible to apply to turing machines, and can not strictly concluded as mechanical process. </p>

<!--more-->

<p>In the folloing, we take the $Eq$ problem as an intruduction to discuss computational learning theory.</p>

<p>Computational theory discuss the prossibilities of some general solutions. Namely whether a problem is computable in most general case. While in most cases, we don’t require the solution to be so strict in all cases. It’s enough if it can approximate to the correct solution with arbitrary small error rate. Or with the growth of the “learning materials”, the results can approximate to our expected solution. Back to the $Eq$ problem, if we have already tested a very large input set, and two turing machines $M<em>1$ and $M</em>2$give the same results. Then we would like to say, the two turing machines are equivalent. </p>

<p>To formulate the problem in another way: <em>For any turing machine $M_1$, where the transition table is unknown. We construct another turing machine $M_2$, such that for any given input set they perform in the same way.</em> Then we can say we can learn $M_1$. </p>

<p>The related theoy to this issue is “computational learning theory”, they focus on whether for a unknown function such a leaning algorithm exists or not. To say a problem is learnable if with increasing number of samples, the error rate goes infinitely close to zero. A formal descriptioon is:</p>

<script type="math/tex; mode=display">P(error \ge \epsilon) \le \delta</script>

<p>Note that, $\epsilon$ is the error rate of one sample. Obviously the left part is dependent on $N$, </p>

<script type="math/tex; mode=display">P(error \ge \epsilon) = p^N \le \delta</script>

<p>There is another equivalent formulation: for any positive error rate $\epsilon$和$\delta$, whether there exists a $N$.</p>

<p>Of course, not all models are learnable. The most classical example is “learn a finite set”, and all unlearnable question can be reduced to the example. More details of the theory can be found in PAC theory. </p>

<p>Not all the learning are based on probability, some simple models can be directed learnt to the objective form. For example the previously mentioned <a href="http://blog.htedsv.com/blog/2014/08/15/anluin-learning-algorithm/">Angluin’s Learning Algorithm</a>，which is based on the consistance, of the current example and previous examples.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An extension to Spotlight]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/01/17/spotlight-extension/"/>
    <updated>2015-01-17T19:34:08+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/01/17/spotlight-extension</id>
    <content type="html"><![CDATA[<p>The current version of spotlight of MacOS is the better than any other versions in the past, however it’s not comparable to Alfred. To day I came up with an extension to Spotlight. </p>

<ul>
  <li>First write a Shell file, no matter where it is. For example in the following example, open the newest folder which contains the lastest homework.</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#! /bin/sh
</span><span class="line"># Filename: isc
</span><span class="line">find ~/Desktop/Dropbox/ISC/Exercises/ -name "SC*" -type d | tail -n 1 | xargs open</span></code></pre></td></tr></table></div></figure></notextile></div>
<ul>
  <li>change permission</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">&gt; chmod +x isc</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>
    <p>right click of the file -&gt; “Get Info” -&gt; “Open With” -&gt; “iTerm” -&gt; “Change All”</p>
  </li>
  <li>
    <p>search in spotlight and execute</p>
  </li>
</ul>

<p>It’s only a simple extension with the shell, but it’s still not so flexible which accepts arguments from input and give output in realtime. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于德国教育的私吐槽]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/12/14/guan-yu-de-guo-jiao-yu-de-si-tu-cao/"/>
    <updated>2014-12-14T13:07:06+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/12/14/guan-yu-de-guo-jiao-yu-de-si-tu-cao</id>
    <content type="html"><![CDATA[<p>列举几个关于德国教育的私吐槽。之前在知乎上评论或者发表过一些对于德国大学教育的看法，从始至终，无论对方如何恶言相对，我都保持和善和尽量谦虚的态度和他讨论，但最后发现，真正得到对方的认可，不是通过你的讨论态度、不是你的论点、不是你说话的逻辑，重要的是你有多了不起的经历。只要有这些经历，对方就会认为自己没资格继续否定你，否则，你的所有观点都是逻辑混乱。大概经历了几次，也就不再会去知乎参与德国或者德国大学相关的问题了。</p>

<p>不过最近的一些学习体验还是让我想对德国大学或者教育发表一些看法。当然我不是最有资格发表评论的，经历也肯定比不上很多人。但是因为从未在公开场合看到完全符合我的认识的想法或观点，所以我很希望能把我的认识在这个相对隐私的地方表达出来，以免招来不必要的非议。</p>

<!--more-->

<h1 id="section">教学与科研</h1>

<p>在很多人看来，科研重于教学，一个发过顶级会议论文的人一定比每天花大量时间上课学习的人要厉害，因为科研活动能培养一个人全面独立的思维，而上课只能学来一些死知识。然而有这一观点的人除了人云亦云的情况之外，基本都是受了国内学术氛围的影响。国内教授能在基础理论上有深入理解并且转化到课堂上的少之又少，基础理论在计算机学生心中的印象是“无聊”“没用”，即使学得好，领悟的透，也并不一定会用。</p>

<p>然而，更多情况下，无论是工作、科研，一定程度上都是在消耗课上学习的知识，尽管科研、工作过程中会继续补充知识，但是没因为没有系统的知识体系，学习速度、深度肯定远比不上课堂。那些没有把课上知识深入理解的人一定不会在科研道路上走得太远。科研并不比工作高尚太多，都是在有目的有针对性的解决不同层面具体问题。</p>

<p>鄙视“上课”的人不仅表现在对科研盲目崇拜上，还表现在对于自学成才的大牛的盲目崇拜以及效仿上。同样的知识，自学而成的大牛往往比上课或者某种循规蹈矩学来的人更容易得到崇拜。从而有时会形成一种风气，放弃好的学习资源去选择自学，仿佛这么做了就能让自己更牛。</p>

<p>这是搞错了因果关系，厉害的人被人关注，往往是因为他处在相对弱的环境里，比如老师水平很差，没有办法轻易获得好的学习资源等等，从而迫不得已选择自学。自学成才的人尽管自我提高了自学能力，但是往往会遇到两个问题：</p>

<ol>
  <li>
    <p>因为自己自学的小成就而对正式、规范的知识体系不屑一顾。比如我在不懂编译原理的时候写出的编译器大作业没有符号表，词法分析和语法分析被写在了一起，并且当时自认为这是化繁为简的行为，因为经过我的改进，减少了代码量、简化了步骤、提高了效率。并且对规范的流程不屑一顾。直到后来深入学习，才意识到规范化的重要性，野路子因为对问题的简化假设，往往做不大，问题稍微复杂一些就要重写很多东西。</p>
  </li>
  <li>
    <p>对问题的理解认识流于表面。无论在哪个学科，凡是相对理论一点的东西都会有不同层次的理解，比如在数值分析课上学到了几种基本的矩阵分解和特点。因为当时做了不少作业，所以并不像当年自己从网上找资料时理解的那么肤浅，但是依然没有理解到各种矩阵分解内在的关系。直到这学期学SVD时遇到一些证明题，从课上的仅有几个理论完全难以下手，学长看完就可以瞬间联想到从Schur分解的结论推过来，从而解决了Singular-Value的一些问题。我才意识到我之前自己为是的学习成果是多么不堪一击。</p>
  </li>
  <li>
    <p>浪费更多的时间。一些公认的优秀学习资源（比如某某教授上的课）不一定比自己查阅资料的方式学习效率更高，但是一定是在兼顾理解深度和广度的最高效做法。随着知识的深入，自学的人往往会遇到很多理解瓶颈，可能需要比上课更多的时间重新理解，才能突破瓶颈。</p>
  </li>
</ol>

<p>正如前面所说，科研是在消耗课堂学过的基础知识，科研虽然听起来高端，但深度一定比不上那些基础知识，新的idea是否可行，从理论模型的层面上一定能找到答案的，基础知识薄弱的人，往往会为一个idea的可行性被迫重新学习一些资料，因为被迫，这些临时学来的东西又难以转化到另一个idea的判断上。</p>

<p>说了这么多，我无非是要表达，在学术上，那些看似捷径的成功做法都很难成就长久的成功。那些片面看重科研重视程度而忽略教学的做法对于学校和希望从事科研的学生个人的长久发展都是不利的。</p>

<h1 id="section-1">大学排名</h1>

<p>除了批评德国过于重视“教学”之外，其他的批评声音很多都指向大学排名。毕竟大学排名是最客观、可量化的标准。但是既然标准化，很多特殊因素就可能很难被完全考虑进去。RWTH的Informatik专业会有10个左右的研究所，负责教学和科研活动。然而在亚琛的强项方向：形式化、逻辑的1所和7所，都只有1个教授，1-3个讲师。其余全是博士生。但是这一个教授的水平往往是领域内奠基人。相比之下，QS排名更高的KIT，每个研究所会有5-10个教授。</p>

<p>RWTH的自然语言识别处理的研究所也是只有一个教授，1个讲师，然而教授是机器翻译领域几个奠基人之一，Google的数据中，他的引用数有30000+，在一个相对不算太水的领域，这个数字已经能说明一些问题了。如果在亚琛想做自然语言相关的学习或者科研，你一定会频繁地接触到Ney教授，比如读博士，你的导师只能是Ney，当然从任何角度来说，这都比去排名更高，跟水平一般的教授要划算得多。</p>

<p>这些东西是大学排名反映不出来的。</p>

<h1 id="section-2">本科生水平与课余生活</h1>

<p>最近因为感觉自己在理论课的思维方式和助教相差很大，可能是知识结构导致的，所以最近开始狂补本科生课程。最大的感受就是德国的本科教育非常的填鸭。必修课的内容2年学完，剩下一年上一上选修课，而且大部分人还学有余力，可以提前修Master的课程。最可怕的是，他们的高中毕业水平和中国学生相差太多，基本中国一般的学生过来大一都能拿到相当好的成绩。然而2年之后，德国学生的理论水平就比我这个学了4年本科的学生高出不少了。以至于自己如果不找时间补他们的本科课，Master的选课空间就小了很多。</p>

<p>具体来说，一门叫做 Diskrete Struktur 的第二学期本科课程（直译就是“离散结构”）。可能和我们的离散数学比较像。但是书中分的三个大章节分别是组合数学、图论、代数，只不过都是比较基础的概念。回顾一下，虽然自己好像也学了一个学期的离散数学，不过也不比他们图论的部分多什么了。</p>

<p>因为他们本科生的课程紧，作业多，所以一方面他们本科期间没有时间参加类似于ACM竞赛之类的课外活动；另一方面因为长期接受这样的填鸭教育，导致在Master和PhD阶段尽管基础扎实，但是缺乏自主性，成果并不多。</p>

<h1 id="section-3">形式化</h1>

<p>形式化向来是法国的特点，然而不知道为什么，外国学生中比较喜欢理论方向的来到亚琛都会有“过度形式化”的感受。不仅仅是理论方向，比如授课时，每个新知识都是先给出定义，然后就是理论和证明，中间很少会有具体的例子，以至于对定义的理解要通过理论的证明来猜。经过一番猜测之后，对这一理论有了一些理解，发现它并不难，完全可以从一个感性的角度更轻松的理解。</p>

<p>另外一个表现就是很多偏工程课程也会大量把自己的概念去靠向形式化。一方面这种形式化本身就是不严谨的，另一方面，把问题形式化的意义是使用形式化方法解决问题，然而这些问题明明直接去理解会更简单。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A general perspective to understand code and data]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program/"/>
    <updated>2014-12-01T00:22:00+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program</id>
    <content type="html"><![CDATA[<p>In this semester, there was course called “Foundation of Data Science”, which is in terms of database but pretends to be related to “Data Science”. The reason for my difficulty understanding it is probably that some prerequisite I never learned before.</p>

<p>Nevertheless, many of the views in the course are worth learning and thinking. Looking back to the courses in my bachelors, which contains Compiler Theory, Database Theory, Computer Network, Operating Systems. As I know, the top universities in america are trying to weaken the connection between different domains. Of course it has benefits in some respects. However from the point of an abstract and theoretical view, it does not help the students understand the essential question.</p>

<!--more-->

<p>Such as database, which is a concept, appears relatively late in the history. Besides it was used to deal with concurrence and high-performance operations, it was ever regarded as a general model to solve problems, just like tapes to a turing machine. To be specific, <strong>All the data in the world can be stored in a database, all the operations to manipulate data are via SQL languages</strong>. Here the “Data” is in the database, “Code” is the SQL. Why don’t we consider other languages, like C or Java? Because we now are standing by data, however relational database is the easiest to understand as a theoretical storage, because it has a lot of algebra-related foundations and guarantees. Now we are facing a new problem: SQL is not turing-complete, which means, it can not perform any operations of turing machine. At least, it can not perform recursion. The easiest way to improve SQL is add a feature: “function”.  <a href="http://en.wikipedia.org/wiki/Datalog">Datalog</a> is such a stuff. As we see from the examples in wikipedia, it looks like rules of a context-free grammar, which contains a series of rules. Since each term can occur both in the left side and right side, it may be evaluated for infinitely long time. The key reason of its incredible power is the ability of “self-recursion”. Consider the turing-complete programming languages you know, the common features of them is “recursion”. However it’s not exactly what we need, in order to keep the “monotone” (which is defined for the programs $x$ and $y$, “$eval(x) \le eval(y) \rightarrow eval(f(x)) eval(f(y))$ for any function $f$”), we forbid “not” in the rules. which finally leads to “Inflationary Datalog”.</p>

<p>Let’s come back to the programming languages, the essential of the strong power of the programming languages, is not “if” “for” or even “function”, but “first-order”, “hiher-order”, or to say “recursive” or not instead. Even though “for” and “if” are not allowed, we can still program whatever we want. Functional programming is such a invention， which is defined on a more elementary operation. The equivalence between functional programming and the foundation of the most modern languages – turing machine has been emphasized in Turing-Church thesis.</p>

<p>Now I have to mention “Prolog”, which is the intuition of this article. When we fist got to know the basic grammar of it, we won’t realize its importance, but after we gain the basic knowledge of mathematical logic, we get to know how the language model the world with first-order logic. While formal systems are developed at the peak of the power, almost every scientist believe the logical derivation is the fundamental method to construct the world. The evaluation of an arbitrary first-order expression is referred to <a href="http://en.wikipedia.org/wiki/Ehrenfeucht%E2%80%93Fra%C3%AFss%C3%A9_game">Ehrenfeucht–Fraïssé game</a>. Under this assumption, if we construct the Axiom System, and Rules of a first-order logic, we can infer everything with a program. However the assumption is not true because of the limitation of the “finite axiom system” and “low expressiveness of first-order logic”. Besides it’s nothing more powerful than any other languages and confusing grammar, the failures of the languages in industrial application may due to the followings:</p>

<ol>
  <li>Despite the conceptual simplicity, it’s nothing powerful than other languages.</li>
  <li>“Intuitionistric” in theory is not equal to “intuitionistric” practically, for most cases in our real life, expressing them with first-order logic may be more difficult.</li>
  <li>The most important: logic is used to express any possible problems, but not sufficient to solve every problem in the most fast way.</li>
</ol>

<p>To understand these points, can really help people, who just get in touch with mathematical logic, adore AI and mathematical logic sightless. However the basic introductions to these fields sometimes ignore these basic but important facts, so that the students who don’t have an deep insight of the fields commit some naive comments.</p>

<p>For the term “generality”, we have mentioned that database is the most general theoretical model of storage. However the tape of turing machine, which came out much earlier than database, also it’s able to manipulate data with transition table and pointers. While the representations of the algorithms or the computing procedures are at very low level, simply by  0 or 1. which is not a really explicit data representation. We can express everything with tapes, but we can not analysis them.</p>

<p>Even we did talk a lot about data, it’s obviously not enough, let alone the code in the title. To understand study computer science, we are usually talking two things: code and data. However in different fields, they have different names: functions and variables, programs and files, logic expressions and facts, classifiers and datasets. They are sometimes equivalent, sometimes opposite, or even they have no differences.</p>

<p>A theoretical machine learning model is often identified by a set of parameters. The parameters is equivalent with a infinite set of labeled dataset, which it can learnt from.</p>

<p>However, without variables we can not say what is program, how a algorithm works.</p>

<p>The most philosophical significant result about code and data is <strong>code is sometimes data, data is sometimes code</strong>. We always say, memory is a storage of data, but without context, we can not identify a 01-string, whether it is a part of program or a data file. Another example is universal turing machine, turing machine can be encoded as data and input to another turing machine. The case is similar to Lisp, the ability of compiling a string to code (called meta programming) also makes its fans exciting talking about it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[确定世界与概率世界]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie/"/>
    <updated>2014-10-24T21:45:20+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie</id>
    <content type="html"><![CDATA[<p>这篇没有涉及具体的理论知识，主要是建立在部分相关经历之上的一些臆想，所以多少会有些民科思维。的确，我在这方面的经验少的可怜。但是相比于能看到我博客的人来说，信息不对称还是非常严重的。从刚入学我就多次表达了我的震惊。我发现计算机科学还有这么大一片领域我几乎一无所知，最可怕的是，我所谓的一些“基础和见识”可能也就是当地大二学生的水平。经过了一个暑假的沉淀和反思，我对于形式理论的极端态度渐渐冷却，所以，我也希望我能把它放到和其他领域平等的地位上考虑一些它们之间的关系。</p>

<!--more-->
<p>如果我们想表达两件事情A和B的必然因果关系，形式方法是A-&gt;B，而概率方法是p(B\|A)=1。这就是两个世界的建立基础。所谓“世界”并不是从学科发展的角度，即，不是按照先学p(A)，然后学p(A,B)最后学条件概率。而是按照世界运转的“元操作”。说到这，可能还是要提一下“世界”的动与静。世界可以是一个生态环境，一个程序中的类，也可以是一个人的个体。而“动”和“静”（或者叫行为与状态），可以类比成“算法”和“数据结构”，“类方法”和“类参数”，“代码”与“数据”。不过要认识、分析和解决计算机科学根本的问题还是要放在最根本的模型上。我说的当然不是图灵机，而是比图灵机更早，而且标定计算机能力疆界的——形式逻辑。</p>

<p>从计算机出现甚至更早，整个计算机科学的发展都是建立在形式理论的基础上，比如程序语言的设计、数据库的设计。到了后来，计算机的计算能力已经达到可以真正为人所用。于是就有了大量人开始用计算机来搞数值分析，并因此诞生了一些图灵奖得主。这些就扯远了。总之，到了90年代，单纯的形式理论已经没再出现真正的突破。</p>

<p>形式理论正如前面的例子中提到的，是一套完备的体系，一切都是确定的。当然，“真正的智能无法实现”也是被确定的，因为二阶逻辑的不完备性在图灵机诞生之前已经把计算能力的极限定在了那里。
而无法实现的原因可以直接参照停机问题的矛盾句式。</p>

<p>于是，当我们发现一个世界的运行规则被基本挖掘干净，并且没有找到我们真正需要的东西时，就尝试在确定性世界的框架下加入不确定性，也就是概率来完善形式理论的一些不足：</p>

<ol>
  <li>
    <p>一个世界的规则是确定的，不能随着外部输入而变化，一个世界需要更多的可能性。尽管随着概率的加入，我们可以通过修改参数的方式修改世界的一些规则（一个具体的例子就是通过样本来训练机器学习模型的参数），然而制定规则的世界依然是确定的。概率的加入只是帮助我们从应用的角度解决了更多具体的问题，对于真正的理论研究上的“智能”并没有什么帮助。</p>
  </li>
  <li>
    <p>真实世界是概率的，至少我们对于一个值得研究的子世界（所谓子世界，就是真实世界中我们需要研究的对象集合的子集），其中的个体都是概率的；或者说全世界是确定的，其原子行为构成一个闭包。但对于真实世界的任意一个有限子集，都不是封闭的，所以我们只能把其中的一部分当成随机变量来考虑和认识。通过从“元”上达到和真实世界的一致可以帮助我们更好的转移现实世界的内容到构造世界。尽管我们依然构造不出绝对的智能，但是已经可以用简单的智能结合真实生活，完善生活中的问题，这样可能更有社会意义。</p>
  </li>
  <li>
    <p>我们可以在假设存在一个真实世界的子集，并且在这个我们要研究的子集是一个闭包，即，一切都是确定的。但问题就变成，为了尽量真实的模拟现实世界，需要足够大的子集，以至于超过了计算机的运算能力，难以继续，一个实例就是神经网络算法的发展瓶颈。</p>
  </li>
  <li>
    <p>确定性世界并非一无是处，因为其关系的建立都是稳固而确定的，所以整个世界体系也是可靠的（往往是无限大），而概率世界更多的是表达隐含关系，不确定关系，所以单纯由概率搭建的世界（比如贝叶斯网络）不会很庞大，仅仅是为了解决某类特定小问题。</p>
  </li>
</ol>

<p>当然概率帮助我们简化了一些“确定性”解决起来很棘手的问题，但是根本性问题永远无法解决。也正因为这一点，形式化理论研究很早就进入了瓶颈，而且创造不了价值，所以时至今日，只有为数不多的欧洲大学把它作为重要计算机专业最重要的基础知识传授给学生。然而学习它却对于理解计算机科学从宏观上能有最重要的价值。</p>

<p>如今，“单纯的形式理论”已死，也不会有人太多人研究“真正的智能”。更多的是将其和统计学方法结合，解决生活中具体的问题。相信随着更多复杂的形式理论的引入，我们能在未来构建更加完善的“世界”。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[一年咖啡龄感受总结]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/23/yi-nian-ka-pei-ling-gan-shou-zong-jie/"/>
    <updated>2014-09-23T04:40:00+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/23/yi-nian-ka-pei-ling-gan-shou-zong-jie</id>
    <content type="html"><![CDATA[<p>尽管从大三开始就已经开始喝咖啡，不过在国内，更多的还是笼罩在速溶咖啡的阴云之下。速溶咖啡能比较受欢迎，一方面是因为比较好买到的速溶咖啡都比较甜，大众能接受；另一方面也是因为它确实有一定的提神功效。不过后来到了德国，比较甜的速溶不是那么普遍，所以也就趁超市打折买了一个滴漏咖啡壶，从此开始喝起了咖啡粉。</p>

<p>德国的咖啡粉比较好买到，而且一般一包也就4欧左右，即使每天喝也差不多能喝两个月，比较实惠。</p>

<!--more-->

<h1 id="section">适应</h1>

<p>开始的时候因为太苦，所以每次都要往咖啡里加一些白砂糖或者牛奶。大概适应了一两周，开始习惯直接喝，不添加其他食物。直接喝的原因也很简单，就是为了能够更充分的享受咖啡粉本身的香气。后来也很多次尝试添加糖，打些奶泡，不过咖啡的香气却被削弱了不少。</p>

<h1 id="section-1">健康</h1>

<p>相比于茶，咖啡可能更适合火气旺盛的西方人。在过去的一年里，我每天平均能喝一杯多的咖啡，有时候是因为兴趣，有时候是提神需要。喝完之后，一个最常见的反应就是利尿，所以经常会感觉身体缺水。如果完全空腹喝，也会身体过于亢奋而不能专注。最严重的可能也是对它的依赖——很长一段时间都会很享受刚起床喝一杯咖啡的时刻。在这一年中，我也莫名其妙的发烧了3次，最严重的一次嗓子疼到不能喝粥。用中医的说法应该就是火气太重，上火了。在最后一次发烧之后，我又尝试加入些牛奶，感觉对胃部刺激就轻了很多，所以加奶咖啡的另一个好处就是能缓解上火。</p>

<h1 id="section-2">效率</h1>

<p>尤其是到了后半年，因为学习压力太大，每天都是被作业牵着鼻子走，所以必须要用咖啡提高效率，否则在思考一些数学题时完全没有任何进展。然而，咖啡虽然短期内效果明显，可以把大脑疲惫期拖延到睡觉的时间，然而时间一长，推迟效果越来越不明显，甚至喝完之后，本来还算清醒的大脑直接就疲惫了，然而身体还处于亢奋，所以没办法通过睡觉消除疲惫，只能等咖啡的效果消失了再睡觉，反而更耽误时间。</p>

<p>所以，当这些负面效果逐渐明显时，我会稍微克制几天，通过多喝水和跑步来缓解。</p>

<h1 id="section-3">咖啡胶囊</h1>

<p>有一次超市胶囊咖啡促销，借机买了一台胶囊咖啡机，从此喝咖啡变得更简单。通过网上了解的一些信息，咖啡胶囊的品质整体会比普通的粉品质高一些。不过刚开始喝得几天反倒觉得胶囊更苦，而且也没有更香。适应了一周，慢慢觉得两种方式冲出的咖啡应该说是口味不同，各有千秋。</p>

<h1 id="section-4">口味</h1>

<p>无论是滴漏式咖啡机还是胶囊咖啡机，从口味上都难以和手冲相比，所以时间充裕的时候我也会和手冲咖啡。个人感觉手冲咖啡一方面因为水和咖啡粉能够更充分的接触，另一方面漏下的咖啡没有被持续加热，所以味道更香，酸涩感更淡。</p>

<h1 id="section-5">免疫</h1>

<p>有些人咖啡喝多了会有免疫，我觉得有些时候可能真是因为咖啡喝的不够多。我对自己冲的咖啡就从来没有免疫过，但是昨天下午在一家咖啡厅不听服务生劝阻的喝了两杯Espresso却没影响到我晚上睡觉。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automaton to Regex]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/12/automaton-to-regex/"/>
    <updated>2014-09-12T04:12:15+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/12/automaton-to-regex</id>
    <content type="html"><![CDATA[<p>前面提过了Angluin学习算法的基本原理和作用，通过不断回答程序提问的句子是否属于该指定语言来生成这门语言对应的最简DFA。然而DFA的表示并不是很直观，也不实用。所以我今天介绍一个方法，把生成的DFA转化成对应的正则表达式。
<!--more--></p>

<h1 id="section">问题</h1>

<p>通过之前在网上搜索，没有找到比较完美的解决方案，但是从正则表达式到DFA的方法却有很多。我想“DFA -&gt; Regex”的主要难点有两点：
- 没有一个好的方式化简(合并)生成的正则表达式，或者说不能把任意正则表达式形式化的转化成一个特定的范式。然而对于任意NFA都是有范式的，即“最简DFA”。所以我经常觉得正则表达式是一个不太好的设计。
- 存在多个互相可达的终止状态的情况会丢掉一些句子。</p>

<h1 id="section-1">相关工作</h1>

<p><a href="http://cs.stackexchange.com/questions/2016/how-to-convert-finite-automata-to-regular-expressions">这里</a>有一个方法实现自动机到NFA的转化，是针对更广泛的NFA，但是限制也很大，就是不能完美解决多终止状态的情况。其实现方法可以归结为：<strong>每一步删除一个中间状态k，并且对于每对状态i，j，如果存在i-&gt;k，k-&gt;j，那么删除的时候用L[i][k].star(L[k][k]).L[k][j]来完善L[i][j]，其中L[i][j]是从状态i到j的所有语言，初始情况两个状态间的语言最多只包括了一个字母。最后计算初始状态到终止状态的语言就是结论</strong> 这么做的问题就在于如果有多个终止状态，那么他们都不会被删除，但是最后结果会漏掉状态start -&gt; f1 -&gt; f2的语言，其中f1，f2是终止状态。由于在剩余的状态集合中可能存在多种路径，而且不能继续删除，比如f1 -&gt; start -&gt; f2，因此情况会变得非常复杂。</p>

<h1 id="floyd">Floyd算法计算状态间语言</h1>

<p>我的想法是不删除中间状态，而是通过Floyd算法直接计算每两个状态之间的语言，听起来似乎和删除中间状态效果一样，不过用Floyd的好处是能把终止状态作为中间状态的情况也考虑全面。这样，最终的答案就是对于每一个终结状态f，<script type="math/tex"> result = \Sigma_{f \in F} L_{start,f} </script></p>

<h1 id="section-2">计算过程使用结构元组而不是直接用字符串</h1>

<p>在更新状态间语言的过程中，<strong>L[i][j]</strong>不是直接用字符串格式的正则表达式来存储i，j之间的语言，因为用字符串尽管不会影响结果的正确性，但是会给字符串化简制造麻烦。比如消除冗余括号。所以我使用了tuple来代替字符串来描述正则表达式的”树结构”，描述只使用’+’, ‘.’, ‘*‘三种结构，分别代表“语言集合的并”，“语言的连接”，“出现任意次”。当然，这个符号定义和平时使用的正则表达式有区别，只要修改程序的输出函数即可。比如(‘+’, [‘a’, ‘b’, ‘c’])代表一个有4个节点的树，生成的语言L只接受aa,b,c三个句子。</p>

<h1 id="section-3">冗余消除策略</h1>

<p>用Floyd算法遇到的最大问题就是会制造很多语言的冗余。下面说几个主要策略，不过并不能保证完全消除，其中一些策略的正确性是建立在“输入DFA是最简DFA”的基础上的。</p>

<ul>
  <li>消除空结构，比如(‘+’, []), (‘+’, [’’])，这种结果本身不会造成错误，但是会在计算过程中造成冗余</li>
  <li>消除重复括号，当一个结构例如s=(‘+’, [‘1’,’2’])作为子结构出现在另外一个结构中时，并且主结构包含多于一个元素，这时要给子结构的字符串形式加括号，比如t=(‘.’, [s, (‘*’, ‘b’, ‘c’)])</li>
  <li>消除不必要的循环变量运算，在Floyd算法中需要枚举变量k, i, j。然而根据k和i，j的等价关系可以省略一些运算，例如如果k和i相等，那么当<script type="math/tex">L_{i,j} \neq EMPTY</script>时，可以化简运算为<script type="math/tex">L_{i,j}= star(i,i).L_{i,j}</script></li>
  <li>最简DFA，这个优化并不需要我用代码实现什么，而是作为一项限制，用来证明我很多看似不严谨的代码的正确性。例如不会有两个等价的终止状态。</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[考试总结]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/08/kao-shi-zong-jie/"/>
    <updated>2014-09-08T16:33:34+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/08/kao-shi-zong-jie</id>
    <content type="html"><![CDATA[<p>中秋佳节之际做一些小学生做的事情：考试总结。</p>

<p>这学期总体来说收获很大，但是和成果并不匹配，也就是说成绩很不理想，或者说和我之前的经验恰恰相反，唯一一门考试资格都没有获得的课是我本科算上加分之后拿到满分的“编译原理”，而且两次课程内容基本一致。唯一一门挂掉的MA课是我自认为本科期间做过很多相关工作（包括两次实验室经历和一次实习经历）的“数据挖掘”。通过的3门都是之前接触不多，反而最后能低分飘过。下面总结一下原因：
<!--more--></p>

<h1 id="section">基础知识薄弱</h1>

<p>现在回头看看，确实觉得这一学期学的很辛苦，但是实际上那些被奉为神课的课并没有想象的那么难，即使缺少一些必要的基础知识，也是可以通过以前学习的考试技巧来Hack一下。尤其是理论方向的课，现在想想，每门课研究的“实体”其实都有章可循。大概都是归为相关性质与相关实例，性质引出实例，不同的实例又有不同的新性质。另外，通过考试内容可以发现，很多性质的证明并不重要，不需要深刻的理解。</p>

<h1 id="section-1">题型不适应</h1>

<p>基本每门课都不会给太多复习资料，对于第一次来德国，完全不知道如何复习，很多国内通用的复习策略在这里行不通。比如没有办法通过作业中的题目本身来判断是否可能在考试中出现，尤其是第一门的数据挖掘，看了一遍卷子就直接傻了。</p>

<h1 id="section-2">时间安排不合理</h1>

<p>考前一方面有些轻敌，一方面回家心切，所以5门必须的考试被安排在15天里，以至于第一门发现策略失败后，后面完全没有时间调整。另外一个很主观的原因就是这学期记忆力大不如前，连写带画的看了5遍的课件，考试时还是记不住。</p>

<p>时间安排的另一个方面就是平时上课写作业，每天被作业牵着鼻子走，最后只是为了草草完成作业，难点没有时间搞懂，重点也没有时间回顾。</p>

<h1 id="section-3">考试与学习的协调</h1>

<p>这也是最让我最难以权衡的。正如和在国内一样，应对考试必须要有很多额外的时间去刷题，背一些没有启发性的知识，并不是你把课上的东西领会了或者有所推广就一定能考试。所谓启发性，也就是我最感兴趣的。而这些启发性又难以用标准化的题目去考核。所以到了最后，就不得已放弃很多思考启发性问题的时间去背一些标准化的概念。</p>

<p>上个学期总体说来时间安排还算合理，或者说幸运，每天一门作业。在接下来的学期上课撞车更严重，恐怕要更加努力了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数理逻辑如何改变我的世界]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/08/20/how-mathematische-logik-changes-my-world/"/>
    <updated>2014-08-20T18:25:31+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/08/20/how-mathematische-logik-changes-my-world</id>
    <content type="html"><![CDATA[<p>在大二上学期，我自认为对程序员的工作有了足够的了解——无非就是学语言，学库，即使是涉及到其他专业领域，也只要掌握最基本的概念，然后调用对应的库就可以了。于是“程序员”这个概念更多的是对经验的要求——对各种库的用法掌握、编码风格的统一等等。与之相对应的就是我自认为之后的所有课程就是背概念，因为计算机科学的基本轮廓都是围绕着编程的。</p>

<p>不过当我真正学习编译、操作系统以及自己后来看一些工业界用机器学习解决实际问题的文章的时候，忽然发现，我才刚刚了解计算机科学是在做什么。</p>

<p>Master的第一学期学习了数理逻辑之后，我的世界观进一步被摧毁。我在之前从没认真想过“为什么计算机科学可以看做数学的分支？”“计算机科学在过去几十年都有哪些成果？”“为什么说图灵是计算机之父？他的成就都有什么价值？”这类的问题。这些问题似乎成为了计算机科学系统的公理，不需要证明，也无法解释。虽然并不是每个问题都能直接从“数理逻辑”上找到答案，但是很多它却给了我们一个入口，让我们看到计算机科学目前很少被关注的一面。</p>

<!--more-->

<h1 id="section">计算机科学与数学</h1>

<p>如果去问刚上本科的我，为什么说计算机科学是数学的分支，我多半会说“因为算法、数据结构都是数学问题，计算机科学经常要处理一些数学问题。”这个回答内容不算错，不过答非所问，因为既然说“分支”，必然是有“根”有“叶”。计算机处理数学问题最多只能说“计算机和数学关系密切，计算机因为其性能特点，能帮助解决一些数学问题”。然而体现不出计算机理论如何建立在数学之上，或者说计算机有什么核心理论是从数学得出的。</p>

<p>如果说有的话，我觉得“数理逻辑”应该是其中最重要的一门。很多基础学科都会有一些理论能普世万物，“数理逻辑”因为其主要其应用领域算到了计算机科学下面，但很多核心概念都是能普世万物的。比如公理系统，公理系统并不只作用于数域，任何<strong>理论(Theory)</strong>都有可能会有有一个公理系统。公理系统具体来说就是描述一个理论最小的句子集合，以至于建立在该理论上面的所有句子都能通过这个集合的句子组合判断真伪。而一个理论，正是一个无限的句子集合。建立在这个基础上，我们就可以轻易判断一个理论有没有可能被证明。这仿佛给给各学科开疆拓土，只要有公理系统，那么任何问题都可以用形式化方法判断真伪，当然也不会有什么“无法证明的问题”了。然而并不是每个理论都能被公理化（具体细节专业性较强，在此不做讨论），而且对于自然学科，总会有新的发现，因此公理系统也就不断变化，即使是已经被证明的理论，也会在之后被推翻。</p>

<p>回到计算机与数学上面，正是因为数学与计算机对计算问题的研究都建立在一个较为封闭的系统，因此才会有价值。具体到编程语言上，可以把命令式编程语言中看成顺序结构、循环结构与判断结构看成它的公理系统，它与函数式编程的基础“Lambda表达式”是原子等价的，因此可以把他们具有同样的表达能力。</p>

<p>数理逻辑中另外一个非常有用的定理就是一阶逻辑的完备性定理。很多问题一旦涉及到“无限”，那么许多有限问题上的做法都不能适用了，比如我们不能对比两个无限字符串中某个字母的个数的长度奇偶性。而完备性定理正是帮我们把无限问题放在有限问题上解决。</p>

<h1 id="acm">数理逻辑与ACM</h1>

<p>在来德国之前，我从来没有怀疑过计算机领域还有什么问题是因为其模型抽象而难以理解，主要还是因为德国大学在ACM方面基本没有涉足。然而从数理逻辑课上遇到的很多问题，即使直接看答案也要思考几个小时才能彻底看懂。所以说，其实抽象的问题很多，只是通过编程方法能够表达出来的数量很有限。</p>

<p>另一方面，ACM涉及的知识散碎，各种算法间相互关联性并不大，所以每掌握一门新算法，收获是常数增长，从中得到的一些小方法或许能够很好地解决一些常见小问题，但是没有体系，无法深入。</p>

<h1 id="section-1">关于图灵机</h1>

<p>很多人都喜欢表达对图灵这一计算机科学鼻祖的憧憬，不过大多数人可能对其认识仅仅停留在知道图灵机的程度上，至于图灵机的意义和相关理论，感兴趣的人似乎不多。了解的多一些的可能还会知道它和自动机关系密切，受限图灵机是和自动机、以及Pushdown System等价的，然而根据市面上的为数不多的自动机相关资料，很少会提及它与数理逻辑之间的关系。事实上受限图灵机或者说与其对应的自动机课可以看成一个<strong>理论</strong>，也就是说，图灵机也可以被公理化的。在自动机领域有很多图灵机的变种，有些互相等价，有些不等价，甚至随便一次自动机考试的试卷都能看到一个新变种。要讨论清楚任意两种之间的关系可能方法各异，但是如果从最本源的角度——公理系统来考虑，很多问题一下子就简单多了。</p>

<h1 id="section-2">函数式编程</h1>

<p>与其要讨论“函数式编程”这种被讨论烂的话题，不如简单说一下相对不那么烂的“Lambda演算”。所谓的看似高端的函数是语言Lisp、Scheme、Haskell从抽象程度的角度来看都是Lambda演算的一个具体实现方法。那些希望通过掌握Lisp来标榜自己的人，与其掌握一个实现，不如首先想透lambda表达式相关的一些抽象问题。比如Lambda不允许自递归，而是通过不动点的方式。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Angluin-Learning-Algorithm]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/08/15/anluin-learning-algorithm/"/>
    <updated>2014-08-15T20:24:10+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/08/15/anluin-learning-algorithm</id>
    <content type="html"><![CDATA[<p>前两天在知乎回答了一个关于<a href="http://www.zhihu.com/question/24824487/answer/29196253">识别某种特定语言的方法</a>，根据这两天0赞同的情况，不禁让我对知户上计算机从业者的基础理论水平深表怀疑，不过当我看到第一名的答案时确实非常激动，因为一下子想起了2年前在知乎实习时为了识别某个语言，发现用正则表达式可能会非常麻烦，于是写了个自动机，当时刚刚从编译课上学了些自动机的原理，并且第一次正式尝试用它来解决实际遇到的问题，自然是非常激动，而第一名的方法也正是通过构造自动机来生成正则表达式。</p>

<p>不过在那之后，“自动机”与“编译”在我脑中基本成为两个非常相似的概念。我因为自动机的神奇而对编译中各种文法以及编译器的实现产生了兴趣。自认为目前做过最NB的一件事也于此相关——在看过龙书第一章，并且学习了推到规则之后，自己动手写了个简单的编译器作为大作业，不过大作业并没有因此得到满分，因为理论的欠缺，我甚至分不清lexical和syntax之间的区别，自认为高明的把词法分析、语法分析写在一起，并且跳过符号表；这些都被老师当成了扣分点。</p>

<!--more-->

<p>知道来到RWTH，一学期同时学了自动机与编译原理，才渐渐认识到两种理论其实各自自成体系，交集并没我想象的那么多。重学一边编译的过程从理论方面并没有给我更多的收获，这也让我第一次感觉本科上的课也不是都那么水，但是每周的作业中都包含编程题，也就是实现一个语言的一部分代码，当然这部分都是每周课上新学的，从词法分析，到LL(1)、LR(0)、SLR(1)、LR(1)一直到最后的语义分析和生成代码部分。相比于自己实现，这种方法显然更加“德国”，减少了像我一样“误入歧途”的风险。因为从这个过程中确实学到了很多编译器更规范的做法。</p>

<p>不过相比于编译原理，我从应用自动机课上收获的东西更多，以至于很多东西对我的世界观产生了很大影响，具体的很多想法以后在慢慢谈。今天就来说个具体的，也就是我认为这门课最正统，也是最NB的算法——Angluin’s Learning Algorithm。在回答问题之前我也把<a href="https://github.com/xuanhuangyiqi/AnluinLearning">项目</a>地址贴了上去。除了一些简单的介绍外，我今天想着重介绍下Angluin’s Learning Algorithm的核心：封闭性与一致性。不过在此之前，我还是要说，这个算法不是机器学习算法，不涉及概率，或者说每一步都是为了达到一个严谨的稳定状态。在混沌的世界，机器学习可以解决“一切”问题。这种说法是要表达：对于现实世界，机器学习因为对问题本身要求更低，所以解决问题的范围更广（因此看似神奇），然而也正因为其规律并不能适用所有，所以解决的效果无法达到完美。然而在理想化的形式世界，所有的问题都被抽象，形式化，虽然和现实脱离，但是却能在形式化的世界得到完美解决。下面就要正式介绍一下两个性质：</p>

<p>根据我在回答中的一些简单描述，大概可以把该算法解释为：<strong>通过反复询问新的词是否属于该语言来维护一个观测表的封闭性与一致性。</strong></p>

<h2 id="section">封闭性</h2>

<p>对于有两个词w,v 属于语言L，如果它们后面无论接什么词，都是要么同时属于L，要么同时不属于L，那么可以说这两个词的等价的，如果把他们放到自动机上面从初始状态开始跑一下，它们会停在同一个状态，那么这两个词就是等价的，比如对于正则语言(12)*，12和1212还有121212是互相等价的，1和121也是等价的。然而为了说明两个词的等价，我们并不一定需要找到所有词接在他们后面来验证他们是否属于这一语言，只需要几个有代表性的就够了，而要接的词也就是S集合（具体原因稍后再说），<strong>而R集合中的每一个词都可以根据后接S中的每一个词是否属于语言L来代表一个DFA上的状态</strong>，如果R中两个词w, v它们后接S中的每一个词都是要么同时属于L，要么同时不属于L，那么显然他们是等价的。如果我观测到有一个词R中的词w后接某个字母a之后不在R中，并且和R集合中的任意一个词都不等价，那么目前的DFA是不封闭的，说明wa代表了一个自动机上的新状态，那么通过R中加入wa来维护这个自动机的封闭性。</p>

<h2 id="section-1">一致性</h2>

<p>前面提到了两个词w, v等价的条件，并且我们认为S集合可以代表全集，来验证任意两个词的等价性，然而S集合虽然表面上R中的每个词是否等价区分开了，但其实可能是因为S集合不够大，并没有真正把两个不等价的词区分开，比如从现有S集合来判断，w~v（表示两者等价）。然而如果发现对于某个字母a，wa与va通过S来区分时并不等价，即对于S中的某个词s，was与vas并不是同时属于L，那么显然w和v也并不是真的等价，而区分它们的词正是as，所以通过给S添加元素as来维持自动机的一致性。</p>

<p>好了，虽然算法还有很多小问题值得思考与证明，比如知乎回答下vczh提的问题，但是最核心的东西应该是说清楚了，细节可以以后慢慢讨论。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[反面看极简生活]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/07/30/fan-mian-kan-ji-jian-sheng-huo/"/>
    <updated>2014-07-30T08:54:47+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/07/30/fan-mian-kan-ji-jian-sheng-huo</id>
    <content type="html"><![CDATA[<p>这篇来说一下自己对于极简生活的2年多的实践和成果。最重要的不是倡导人去追求极简，而是如何让它改善我们现有的生活。当然我的大部分观点和主流极简主义倡导者是有出入的。</p>

<p>我从两年前的豆瓣上看到“极简”这个词，当时还不太清楚极简代表着什么，指示看到一些优秀的极简风格设计，或者是品牌，比如MUJI，甚是喜欢。于是我决定尝试改善我的生活，让我能在学习工作中更专注于任务本身。最开始的几个主要方面就是删除手机上的多余app，减少生活用品，卖掉不必要的书，减少自己的信息源（比如当时的Google Reader），处理掉不重要不紧急的任务，改掉信息焦虑症。</p>

<!--more-->
<p>但在之后的一段时间，我发现这样的实践和我很多固有的观念产生了冲突，让我找不到好的妥协方案。比如对待无用或者劣质的生活用品，我从很早就一直在贯彻父母教导的节俭思想，只要还凑活能用的东西就继续用，不要轻易扔东西。然而按照极简生活的要求，我应该果断把它们舍弃，换成一些品质更高、数量更精简的替代品。由于那段时间我要花很多的钱学习德语，准备考试，同时也没有挣钱的途径，自然也不愿意再找父母要钱花在这种小事上，当时觉得极简就是有钱没处花的大人为了挣更多的钱，更舒服的生活才要考虑的。另外一方面极简要求摒弃无用的东西，但事实上，每个物品对于你都有两个维度：使用频率与重要性，另外还有一个获取难度。有太多东西是生活中使用频率并不高，但是每次需要都非常迫切，而且难以及时获得，比如指甲刀、体重秤、插线板。道理同样适用于一些手机app。另外一个问题是处在公共生活空间很难把自己的生活观念让周围的人认同支持，这里“公共生活空间”指的是宿舍或者二人世界，因为不是每个人都愿意按着你的标准来生活，同样每个人对不同物品的几个维度值也各不相同。</p>

<p>总结问题所在：极简并不能做到“极”，因为一个维度上的“极”会引发其他维度造成的问题。</p>

<p>不过幸运的是有一项实践是成功的，因为它实践成本很低。那就是极简我的硬盘。把文档、作品放到云端，常用的文件放到网盘同步，脚本与项目放到Github托管。至于图片没有找到完美的解决办法，目前采用的是放到iPhoto里面，并且把文件夹放在网盘同步目录来实现备份。</p>

<p>不过来到德国之后，发现极简生活的实践变得更加简单，因为德国人对于多样性并不那么在乎，或者说好的品牌很有限，也不会有淘宝用价格诱惑你买便宜而劣质的物品。经常的搬家也不会让你有机会考虑什么是更重要的。</p>

<p>最后一个成功的实践还是手机app。在德国除了查车意外，基本没有什么app是必须的。这也让我的app数量相比在国内大大减少。另外也开始把大量信息源转移到苹果原生的应用上，比如任务、日历、笔记。相比之下，苹果原生的应用做的更简洁，方便，省去了大部分传统优秀GTD应用的无用功能。最后，应用数量成功控制在一个屏幕之内。</p>

<p>当然以上只是一个细碎事务少的穷学生的个人极简生活实践。相信对于不同的人，都会有不同的最佳“极简”状态。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[lambda]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/07/16/lambda/"/>
    <updated>2014-07-16T13:39:35+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/07/16/lambda</id>
    <content type="html"><![CDATA[<p>复习无聊，写点东西，想起来昨天“数理逻辑”课上最后一道题有些意思，拿出来分享一下：</p>

<p>我们定义一个运算“◦”，它是一个二元函数。下面我们定3个结构（包括操作域与运算符）：
<script type="math/tex"> \mathfrak{A}_1 = (\{0, 1\}, ◦), a ◦ b = min(a, b) </script></p>

<p>\[  \mathfrak{A}_2 = (\{0, 1\}, ◦), a ◦ b = max(a, b) \]</p>

<p>\[  \mathfrak{A}_3 = (\{0, 1\}, ◦), a ◦ b = (a+b) mod 2 \]</p>

<p>对于$ i, j \in {1, 2, 3}, i&lt;j $ 给出一个FO范式$\phi$来使得对于任意$i, j$都有$ \mathfrak{A}_i \models \phi $ 但是 $\mathfrak{A}_j \models \urcorner\phi$</p>

<!--more-->

<p><a href="https://logic.rwth-aachen.de/files/MaLo-SS14/home12.pdf">原题目</a>中要求对于每个<script type="math/tex">i, j</script>各写一个<script type="math/tex">\phi_{i,j}</script>，本题提高了难度，用一个范式写出来，如果这个题用编程的方法实现，大概是这样的，我们假设了：</p>

<pre><code>for all a in {0, 1}:

if i == 1, j == 2 then
	return (a ◦ !a == 0)
elif i == 1, j == 3 then
	return (a ◦ !a == 0)
elif i == 2, j == 3 then
	return ((a ◦ a) != (!a ◦ !a))
</code></pre>

<p>显然，我们对于特定的<script type="math/tex">i, j</script>我们选择了尽量简单的区分两种运算符的范式，如果你能看懂上面的思路，那么下面就是要把这些东西依次转化成FO范式：</p>

<script type="math/tex; mode=display"> \forall a(((i=1,j=2)\land(a◦!a == 0))\vee((i=1,j=3)\land(a ◦!a == 0))\vee((i=2,j=3)\land((a ◦ a) != (!a ◦ !a)))) </script>

<p>不过在FO范式中，我们只定义了◦操作，所以要把取反“!”和常数“0”替换掉，例如我们定义取反<script type="math/tex">$\exists y(y \neq a \land \varphi)</script>$，替换常数0的过程省略。</p>

<p>下一步要替换掉每个if条件，我们只要判断这个范式不符合第(6-i-j)个结构的性质就可以了，比如对于i=1, j=2,我们可以判断$ ((a ◦ a) != (!a ◦ !a)) $。</p>

<p>这道题的改进版思路有两个重要的解题关键：替换常数和if。是不是感觉很熟悉？对！纯lambda算子，理解它的关键就是在于所有现代编程的3大控制结构：循环、选择、顺序是如何替换成lambda表达式的，以及在纯lambda中不出现常数，或者说是常量，有的只有函数。下面给一个简单的关于纯lambda思考题：</p>

<p>我们用$\lambda xy.x$代表常量True，$\lambda xy.y$代表常量False。写出下列函数的lambda形式：</p>

<ul>
  <li>and</li>
  <li>or</li>
  <li>not</li>
  <li>xor</li>
  <li>implies</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从一个梦中得到的关于生活中的加密方法]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/06/24/encrypt-from-dream/"/>
    <updated>2014-06-24T10:30:05+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/06/24/encrypt-from-dream</id>
    <content type="html"><![CDATA[<p>前天做了一个梦：在火车上，列车员来查票，给她看了我的车票之后，他让我在上面写一个密码，来证明这张票是我的。</p>

<p>在德国，买火车票的时候，有些车票并不是实名，所以要在车票上签名才能使用。因为德国上火车不需要检票，只需要上车之后列车员查票，并且一般不要求同时出示对应证件。而签名，似乎只是为了表示确认车票的使用者，而不能随便更改。这种措施显然没有什么意义，任何倒票、冒名用票的手段都不能被防止。</p>

<p>显然倒票只能从购票的时候控制，所以不做讨论。我们下面来说下__如何防止偷票坐车__。</p>

<ul>
  <li>按照现有签名的办法，那么在检票员查票时应该同时查车票和有效证件，然后保证车票签名和证件姓名一致，证件照片和持票人本人一致，从而证明票人一致。但是因为增加了“证件”从而可能会引发一些问题，比如恰好偷票人和购票人姓名一样（当然可以通过增加生日来区分），比如伪造有效证件，或者忘记携带身份证，身份证被偷……</li>
</ul>

<p>下面提一下我在梦中的做法，也就是计算机加密算法的基础————大数分解：</p>

<ul>
  <li>
    <p>首先想两个大质数，然后它们的乘积写到车票上，显然这个大数与两个质数唯一对应。查票人只要检查持票人能否写出两个质数，乘积为车票上的数即可。对于非法持有人，理论上是不能通过车票上的大数直接得到两个质因数的。</p>
  </li>
  <li>
    <p>显然上面的方法要求持票人记住两个大质数，难以实现。所以完全可以换成两个普通的对自己有意义的大数，检票时，只要向检票员证明分解方法和两个数分别对自己的意义。</p>
  </li>
</ul>

<p>嗯，这就是我前天做梦的内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[最近这三年]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/04/17/zui-jin-zhe-san-nian/"/>
    <updated>2014-04-17T21:00:34+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/04/17/zui-jin-zhe-san-nian</id>
    <content type="html"><![CDATA[<h1 id="section">前言</h1>
<p>两年前想着，考过德福之后写一篇最近这一年，来总结一下德语学习对于一个IT男的影响。当时万万没想到，真正拿到语言证明竟然是在那两年之后。今天收到了第二份足以让我入学的语言证书，才想起当初给自己的约定，于是写下此文缅怀本应该最珍贵那三年。</p>

<!--more-->

<h1 id="section-1">起因</h1>
<p>对于一心想毕业直接工作的我，从来没把学习与成绩看得很重要。直到大三即将开始的暑假，受到女友的怂恿，决定一起尝试一个最不靠谱的选择————去德国留学。我只记得当时虽然意识到这个选择一定是错的，但是我决定和她一起沿着错误的路走到正确。</p>

<h1 id="section-2">经过</h1>
<p>在做出这个决定之后，我们争取一切时间一起报班学德语。同时也决心拒绝一切可能会影响留德之路的一切机会。在学了一年德语之后，开始尝试参加德语考试，一年3次德福考试被我们考了个遍，最后还是成绩很低。最后拿着语言条件限制的录取通知书来德国读语言班，并和大多数来德国读语言班的同学一样，半年后通过，入学。</p>

<h1 id="section-3">最重要的两年</h1>
<p>对于我而言，最重要的两年就是大三、大四。这两年内的决定会直接决定我未来的人生轨迹。这两年，可以潜下心认真继续搞ACM，或许能拿到更好成绩，从而有更多机会去美国或者国内顶级IT公司；我也可以认真找个实习，或许可以去一家比知乎或者雅虎更好的公司；或者早一点去唐老师的实验室，做一些有意思的研究，也可能已经去了美国一个不错的学校。再不济，放弃一些德福备考的时间，多做些外包，也能让当时的生活有很大的改善。但是当时决定了一条路，就要一直走下去，如果中途放弃，拐了弯，不管新的路是否更加宽阔，“德语学习的经历”对我来说才是真正的走了“弯路”。所以，我能做的就是放弃一切可能影响我的诱惑。</p>

<h1 id="section-4">收获</h1>
<p>大一大二我过的顺风顺水，究其原因是和很多大牛们学到要学会“走捷径”，这样能节省不少时间做更多的事。对于不管是计算机、软件的学生，还是IT从业人员，这一思想根深蒂固。我大一可以用OpenCV实现看似很牛逼的识别功能，大二用脚本语言框架来做各种Web项目，但其实做到这些都不需要你懂得太多。这一观念，是直到我德语数次失败之后才意识到的。大三大四我尝试学习思考一些复杂的过程，比如理解函数式编程，比如用机器学习方法获得一些有趣的实验，还有理解，甚至背写ACM的一些复杂算法。但是浮躁的内心已经让我无法专注于这些真正值得在那个年纪做的东西。</p>

<p>学了德语，在德国上了课，才让我认识到，真正厉害的人都是能耐心的做枯燥的事的。那些看似很好掌握，有威力强大的东西，对别人同样很简单。这些想法，恐怕我如果不学德语，一辈子都无法意识到的。</p>

<h1 id="section-5">目标与规划</h1>
<p>刚刚开学，身边的人都在讨论着毕业之后的路，大多是选择读博，确实毕业也不过是2年之后的事了。而毕业对于我似乎还算遥远，我并没有明确的想做什么。只是觉得，来到德国或者说亚琛最难得的就是能与世隔绝的学些大部头的东西，这些东西是我在国内恐怕难以做到的。所以就趁这段时间，学一些以后恐怕很难再直接用到，却又非常重要的理论课。至于我以后要做什么，确实和我现在在学什么关系不大，在一些理论课上，我已经清晰地感受到自己的基本功有多麽不扎实了。通过一些感兴趣的课拿到足够学分之后，我再开始考虑以后的问题，相信到时候选择会更多。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[德语助手Workflow]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/02/03/godic/"/>
    <updated>2014-02-03T22:04:57+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/02/03/godic</id>
    <content type="html"><![CDATA[<p>前两天mac下的德语助手不知怎么变聪明了，发现我没有购买，于是彻底不能用了，断网重装都不能解决，于是果断放弃，当晚写了一个alfred的workflow来代替。缺点就是在线查询确实慢。。。</p>

<p><img src="http://xuanhuangyiqi.github.io/images/ScreenShot2014-02-03at9.58.55PM.png" /></p>

<p>废话不多说，上项目链接：</p>

<p><a href="https://github.com/xuanhuangyiqi/godic-alfred">项目地址</a></p>
]]></content>
  </entry>
  
</feed>
