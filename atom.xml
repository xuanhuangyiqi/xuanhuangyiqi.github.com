<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Htedsv Backyard]]></title>
  <link href="http://xuanhuangyiqi.github.io/atom.xml" rel="self"/>
  <link href="http://xuanhuangyiqi.github.io/"/>
  <updated>2016-01-10T17:05:31+01:00</updated>
  <id>http://xuanhuangyiqi.github.io/</id>
  <author>
    <name><![CDATA[htedsv]]></name>
    <email><![CDATA[xuanhuangyiqi@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Expressiveness]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/12/05/biao-da-li-expressiveness/"/>
    <updated>2015-12-05T11:16:24+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/12/05/biao-da-li-expressiveness</id>
    <content type="html"><![CDATA[<p>The first formal contact to the concept “expressive” is in the course “Fondation of Data Science”, after that I prefer to use the words “Computational Power” to understand it, because at that time, I was more familiar with computational models, such as automata, turing machines, first-order logic etc. Also it measures the models from the perspective of computational functions. But later I found the phrase “computational power” is the word and somewhat ambiguous, so I gave up this saying. Due to the fact that, the models I was familiar with are mostly for the determination, that is, to accept one input and only returns a “yes” or “no” model, so for the “expressiveness” is more appropriate. Let’s look at a example of natural language: “I am a smart and friendly person”, this sentence can be used to judge the authenticity of the right, if the “and” and similar expressions deleted from natural language, then there is no way to express this sentence, or that the original expression expressiveness weaker than the latter, so that the theory used to explain the “expressiveness” of the concept is more reasonable.</p>

<p>As mentioned above, this translation “computing capabilities” is ambiguous because it’s similar to “computability”, but there are big differences from their definitions. “Computability” is a set of classes can be calculated using a turing machine or its equivalent models, the answer to this question is only “yes” or “no.” So computability is a judgement which takes turing machines as a tool. However expressiveness is more likely to be a contionous measurement.</p>

<!--more-->

<p>Also another thing that is worth mentioning is the decision problem. All computable functions can be converted to its corresponding indicator function. For example, $f(x) = y$ can be converted into $F(x, y) = 1$. And we say “expressiveness”, while we are really talking about the “expressiveness” of an indicator function. This is why, I compared automata, first-order logic, which can only return true and false, with turing machines, which seems more flexible. All these models are comparable if we only consider their equivalent indicator functions.</p>

<p>From this point of view, we can answer a question, why the automata are often used as the simplest model in compiler theory of syntax analysis, or for regular expressions, however they act as a research field of theoretical computer science.</p>

<h1 id="automaton-and-turing-machine">Automaton and Turing Machine</h1>

<p>I have ever answered the two questions at Zhihu: “Can an automaton have infinite states?” And “The most important component to a turing machine is the state transition table, is the unlimited tape also necessary?.”</p>

<p>First of all, in terms of the historical development, automaton was developed earlier than the turing machines, but now even nobody pays attention to the automata, which has a simpler structure than a turing machine. Instead, Turing machine was proposed as the beginning of computer science. The reason is, because turing machine is the first implementation of “mechanical movement” of first logic, and the automaton though more a simple and easier simulation, but can only represent limited ability, compared to turing machine, or to say the automaton is weaker than Turing machines.</p>

<p>In fact, the two issues mentioned above are essentially the same, we took the latter as an example to explain from the ability to express it:</p>

<p>First turing machines and automaton actually from operations is essentially the same: According to the current state and determine the transition rules, only transformed into another state. The difference is that state machine is a clear definition of the state, and the state of the turing machine is “Configuration”: (tape contents, the pointer position, status). Suppose the turing machines only have finte tapes, then the possibility of tape and the pointer positions are limited, also for the definition of turing machine, the states is limited as well, so for every turing machine’s configuration, we can construct an automaton such that, the number of states of the automaton is finte, and behaves equivalent to the turing machine, so turing machine is no more expressive than automata. Furthermore, the power of turing machines lies in the infinite taps.</p>

<p>The first question to answer is relatively not trivial, because within “infinite”, it could be divided into countable, uncountable, uncountable and can continue to continue to divide. Almost have to consider each case separately, it is not so simple.</p>

<h1 id="expressiveness-and-analyzability">Expressiveness and Analyzability</h1>

<p>No matter for mathematical logic, or computer models that are usually discussed, an important conclusion is that “the stronger is the expressiveness, the weaker is the analyzability.” For example, given the facts, all expressions of first-order logic is decidable, while second-order logic is not. On the other hand, the expressions of the second-order logic are stronger than the first-order logic. This conclusion can be extended to any logical and computational models. To take formal languages as an example, the expressiveness of star-free languages is weaker than it of regular languages, and the latter is weaker than context-free languages, but thefact is star-free languages are equivalent to groups, regular languages are equivalent to semigroups. As we know, semigroups has less constraints than groups, also they are less analyzable.</p>

<p>Of course, this conclusion is true for turing machines, on the one hand, we hope to invent a model can express a wealth of ideas, on the other hand, we have to lose the contronl of the model. we even can not propose a general method to test whether a program can stop or not in theory, let alone to judge whether a program has bugs.</p>

<p>It is also because of this trade-off, a lot of weaker expressive models survive and not been completely replaced in some areas.</p>

<h1 id="parametric-machine-learning-and-expressiveness">Parametric machine learning and expressiveness</h1>

<p>Here that we focus on the parametric machine learning models, which implies parameters can all be estimated from machine learning models. Nonparametric methods such as KNN are not discussed. from the point of view of whether the parameters of a machine learning problem can be learned, we can add a property: “learnability”. Such as state transition tables of automata and turing machines, which can be seen as different from the other parameters, the parameters of SVM is the dividing plane of $w$ and $b$, the parameters of a decision tree is the sequence of rules, and so on. Can these parameters be learnt? Obviously, if it is, we seem to see an foundamental problem of machine learning - learning a turing machine. As the theoretical model with the strongest expressiveness, if the argument turing machine can be learned by some algorithms, and intelligent thinking can be represented by an arbitrary computable function, then it seems to have arrived at the end of artificial intelligence.</p>

<p>However, even if we simplify the problem down to a certain terminating turing machine, the problem is still unsolvable. In order to explain, of course, we must first define a reasonable “learnability”, we define it with the criteria PAC-learnable. For any small error $ \delta &gt; 0 $, we can find out a training set size $ N $ such that For any sample set with greater than $ N $ samples, we can garantee that the error is less than $ \delta $ parameters. However, for one of the most classic PAC unlearnable problem: a countable set on a finite number of points can be expressed as a discriminant function of a turing machine. It implies, this class of turing machines are not learnable. As a result, It seems this idea from the most basic level is unachievable.</p>

<h1 id="the-expressiveness-of-deep-learning">The expressiveness of deep learning</h1>

<p>Previous articles have mentioned some of the theoretical reasons of success of deep learning, but did not discussed very specificly. Here again we talk slightly in terms of expressiveness of deep learning.</p>

<p>In the seminar depth learning in last semester, I chose the topic “Recurrent Neural Networks and Convolutional Neural Networks”, which is regarded as one of the largest estimate subject, because two topics can be easily extented to a series of sub-problems, and finally the result is that I didn’t catch the professor’s interest points. At the end of the slides, I was asked to present the relationship between the two neural networks from any perspectives, even though they are not really linked to each other. Finally I chose the option: the ability to express. That is, with the same structures (the same number of layers and the same number of neurons, connections are not necessarily the same), RNN, ordinary DNN and CNN, have the expresiveness RNN &gt; DNN &gt; CNN. This conclusion is very obvious, because they are decreasing with the number of parameters, and the parameters of the latter is is a subset of the former. However this conclusion obviously does not make sense, but to introduce more machine learning model as comparisons, we may have a new understanding. </p>

<p>To take SVM as an example for comparison on image recognition, before Deep Learning became ad-hoc in 2006, SVM + kernel was an efficient and sophisticated methods for a very long time in many areas. All high-dimensional classification problems can be mapped to a linear seperation problem if there is a suitable kernel method. However, this conclusion is not totally correct, in “<a href="http://www.iro.umontreal.ca/~lisa/pointeurs/PBR_chapter.pdf">On the challenge of learning complex functions</a>” of Y. Bengio, it mentions the kernel as machine learning almost the most common method has limitations in the simulation of complex structured functions. From this perspective, the non-linear kernel as the surface is mapped to an arbitrary dividing hyperplane method from the expressiveness is not perfect. From another side, it also explained the underlying reason of the success of deep learning - traditional models can not have the expressiveness.</p>

<p>Go back to the two common deep neural networks RNN and CNN, we can say they are enhancing their expressiveness from different ways. The reason differs with the specific issues, such as CNN is extracting local feature hierachically, RNN is handling time-dependence.</p>

<p>Although the enhanced expressiveness of is the trend, but not true for any cases. In machine learning, standing on its opposite position is generalization. In particular, for training of 100 samples, we use a 10-layer neural network is clearly inappropriate, the resulting parameters are very likely to be over-fitting, the reason is the inbalance between the number of samples and parameters. It also explains why in early years when number of samples is limited, neural networks didn’t perform very well.</p>

<p>After all, the big trend is the development of expressiveness. But the issues which really bothered academia and industry are often beyond the expressiveness, such as generalization problems, and lack of adequate data.</p>

<p>As neural networks ability to express the most interesting question - neural turing machines, we will discuss in detailes in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Standard form of algorithmic problems]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/10/16/wen-ti-luo-ji-suan-fa/"/>
    <updated>2015-10-16T20:02:30+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/10/16/wen-ti-luo-ji-suan-fa</id>
    <content type="html"><![CDATA[<p>Almost every student, who studies computer science at an early age are exposed to “algorithm” concept, but everyone often regards “algorithm” to solve a specific problem in a sequence of operations. The “operations” are always in the term can be described as several consecutive nested steps, and finally can be converted into a command-style code, including sequential, looping, selecting  structures.</p>

<p>After exposure to functional programming, the “algorithm” has become a broader understanding. Functional programming algorithm is more like a direct description of the problem itself, rather than one by one step. For example, in Haskell algorithm to achieve quick sort, ideas and details is much simpler than imperative programming: in each step, filter out the elements that are larger and smaller than as the standard, send them to the recursion, and concate their results. Because of this characteristic of functional programming, when many people introduce functional programming, they always referred to “learn functional programming without learning algorithm because functional programming itself is the description of the problem.” After I was taking the words as a goal, and finished learning the course Funcitonal Programming (and Logic Programming later), and found such a sentence has a strong misleading. Although functional programming is simply a description of the problem, but You must be in accordance with functional programming (and logic programming) in their own way to achieve this. There is a problem which relates to “conversion cost”, for many algorithms which are able to be achieved within 10 lines of code, may cost a very long time and code to be translated into functional programs. On the other hand the advantage of the “algorithm” is that they can optimize the performance, that is, as long as you learn algorithm very well, any programs can be completed within a short enough time to run.</p>

<!-- more -->

<p>To come back to our topic – That is how to treat almost the most important concept of computer science - algorithm. Although the way of understanding “algorithm” in the previous statement is nothing wrong, but as an important part of science, algorithm seems too fragmented, it has no structure, framework. Such as the same called algorithms, for machine learning learning algorithms (such as EM) and  shortest path algorithm in graph theory, there is no way to find a general form simultaneously to summarize them. Another more specific example is the algorithms in ACM/ICPC contest, from junior high school students to the programmers, they more or less contact with some books or online tutorials to introducing algorithm contests. The most common point is they are divided into several sections: a section roughly speaking about greedy method; a section about graph algorithms; and a section about dynamic programming, and so on. But in fact there is no link between the chapters, the only clue can link them is - they are easy to study even without any research background, and flexible computer algorithms. Or that they are simple algorithms in computer science projected to the dimension “complexity to study” , and then classified by subjects. If the student only learn algorithms from this perspective is clearly one-sided.</p>

<p>Back to the question itself, which when I was thinking in the course of optimization, is there a way to unite all the algorithms in the general sense? Of course. In machine learning the concept is called “objective function”, in continuous optimization it’s called the standard form .</p>

<p>From a more general point, for all deterministic algorithm, or algorithms, called a clear objective, can be divided into the “optimization of certain functions”, and “check the the feasible solutions”. Then they can be again united, the result is that all deterministic algorithms can be described in a standard form of optimization problems to indicate that:</p>

<script type="math/tex; mode=display"> \min_x f(x) </script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
    \text{subject to:} &g_i(x) \le 0 \\
    &h_j(x) = 0
\end{align*}
 %]]&gt;</script>

<p>More generally，we ignore $h_j(x)$, the problems of finding feasiable solutions can  be described as standard forms without $f(x)$.
While $x$ is the input，$f$ is the to be optimized function. To take backpack problem as an example:</p>

<script type="math/tex; mode=display"> \min_x \sum x_i w_i </script>

<script type="math/tex; mode=display"> \text{subject to:} \sum x_i v_i - V \le 0 </script>

<p>x_i equals 0 or 1, indicates being chosen or not. After thinking again, we find there is still another problem: the domain $x_i$ only contains 0 and 1, which is not observed in the standard form, it will lead to a more complex problems - integer programming. It’s too complicated to be explained in this ariticle. </p>

<p>Another example of optimization algorhtim to solve problems in standard form that I can think of is <a href="http://research.microsoft.com/pubs/69644/tr-98-14.pdf">SMO(Sequential Minimal Optimization)</a> algorithm, The people who have already studied SVM(Support Vector Machine), knows that, the goal of SVM is maximizing the distance between paralleled straigt lines，Then with the help of Lagrange Duality we can transfer the problem to solving the support vectors，the basic idea of SMO is iteratively select $a_i$ $a_j$ under the constraints to minimize the objective funciton，until the vector $a$ approaximates to the optimal.</p>

<script type="math/tex; mode=display"> \min_a \Phi(a) = \min_a {1 \over 2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j (x_i \cdot x_j) a_i a_j - \sum_{i=1}^N a_i </script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
  \text{subject to: } & a_i \ge 0 \\
  & \sum_{i=1}^N y_i a_i = 0
\end{align*}
 %]]&gt;</script>

<p>From the point of view of Optimization to understand algorithms is that every problem to be solved by algorithms can be described in a standard form, which in fact, in some universities, the matimatical students study algorithms from the lecture “algorithms” with computer science students, but from the course “discreate optimization”, which both covers algorithms and computability and complexity theory. </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Infinite Computation]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/05/26/wu-xian-ji-suan/"/>
    <updated>2015-05-26T12:22:03+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/05/26/wu-xian-ji-suan</id>
    <content type="html"><![CDATA[<p>This is a course from last semester, to advantage of the holidays, I’ll write a short summary. Although it doesn’t revolutionize my world view like computability theory and mathematical logic, but as a relatively basic theoretical course, it did inspire me a lot.</p>

<p>I choose the course at the beginning, partly because it is the offered by mathematical logic institut, partly because the name seems bluffing. Because any discrete problems once placed under the assumption of “infinite”, it will become much more difficult, most of the solutions for the “infinite” problem becomes less applicable. For example, determining a complementary set of a regular language is trivial. But to construct the complementary set of infite regular language becomes sophiscated.</p>

<p>The relationship between “finite” and “infinite” is not only opposite. In some scenarios, they are equivalent. completeness theorem of propositional logic, which is to build the equivalence of infinite set to an arbitrary finite set for determination, so that the unsolvable problem becomes solvable. </p>

<!--more-->
<p>However, although the contents of the course can be extended to any general computing problems, the objects discussed in the course are formal language, the most is regular languages, indeed it is very consistent with the style of mathematical logic institut: Any terms in theoretical computer science are the term “formal language”. “Language” is the fundamental targets of theoretical problems, code, data, to some degree, are all “languages”. However, the perspective of “language” is not suitable for all computer problems, the problems seen as the “language” does not help solve any algorithmic problems. If the logic, the language, the turing machines as a general approach to solve the problem, then the algorithm is the special and specfic method solve the problems. Logic, turing machine only focus on the possibility, while the algorithms (and complexity theory) is the only tools to the problem in an efficient way. Exactly because the problems in the world is not possible to be solved by a general tool, even more methods appear to lead to the scientific prosperity. “Do not use one tool to solve all the problems” not in the computer science, but are applicable also in any discipline.</p>

<p>Back to the formal methods, there is an equivalent transformation of finite regular language between DFA and NFA, but for infinite languages, DBA (Definite Buechi Automaton) and NBA (Nondefinite Buechi Automaton) is not equivalent, but they are in the hierarchical relationship; and within DBA, there are also a number of additional levels. Compared to the finite languages, the language level of an finite number is more ugly, but also to deal with a number of specific issues is more difficult, such as transformation of the NBA acceptable language into DBA.</p>

<p>The courses offered by mathematical logic institut brought me another understanding, which is the equivalence between several models, also the existence of the correspondence of each language to some kinds of automata. Some complex languages would correspond to Pushdown Automaton, even correspond to a turing machine, first-order logic, second-order logic; even some logic under another system: Linear Temporal Logic. From a more general point, “finite” or “infinite” language corresponds to the determination problem of the function, and even machine learning models: neural networks, decision trees, linear classifiers, SVM.</p>

<p>In the last lesson, one thing is mentioned is very interesting - correspondence between infinite languages and the rational numbers, which is also a connection between continuous space and discrete space - Cantor Set. its existence proved the correspondence between rational and real numbers does not exist.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[From computability to learnability]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/03/07/cong-ke-ji-suan-li-lun-dao-xue-xi-li-lun/"/>
    <updated>2015-03-07T11:03:37+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/03/07/cong-ke-ji-suan-li-lun-dao-xue-xi-li-lun</id>
    <content type="html"><![CDATA[<p>I have said many times before, computational theory is the root of computer science, it is also the reason of the difference between mathematics and computer science. On the border of computer science and mathematics, the one side of computer science is computational theory, the other side of mathematics is mathematical logic.</p>

<p>Computational theory mostly focused on very specific issues - the “generality”, without any conditionality, some questions are whether or not computable. Two of most common examples are the halting problem and equivalence problem: Is there a mechanical method can accurately determine whetehr an function can stop and it is determined whether two turing machines are equivalent (“equivalent” is, whether they are  calculating the same function). The answer and proof of the first question can be found anywhere: undecidable. The second question is more difficult to determine, strictly speaking, we should construct a reduction: $H \leq Eq$. An easy way to understand it is that to say the turing machines calculating the same function, we must enumerate all possible inputs, and determines whether the output is the same or not, however the input set is infinite, so the process does not stop. Since the determining process of the previous two questions by our brains is somehow heuristic, which implies, the strategy is not possible to apply to turing machines, and can not strictly concluded as mechanical process. </p>

<!--more-->

<p>In the folloing, we take the $Eq$ problem as an intruduction to discuss computational learning theory.</p>

<p>Computational theory discuss the prossibilities of some general solutions. Namely whether a problem is computable in most general case. While in most cases, we don’t require the solution to be so strict in all cases. It’s enough if it can approximate to the correct solution with arbitrary small error rate. Or with the growth of the “learning materials”, the results can approximate to our expected solution. Back to the $Eq$ problem, if we have already tested a very large input set, and two turing machines $M<em>1$ and $M</em>2$give the same results. Then we would like to say, the two turing machines are equivalent. </p>

<p>To formulate the problem in another way: <em>For any turing machine $M_1$, where the transition table is unknown. We construct another turing machine $M_2$, such that for any given input set they perform in the same way.</em> Then we can say we can learn $M_1$. </p>

<p>The related theoy to this issue is “computational learning theory”, they focus on whether for a unknown function such a leaning algorithm exists or not. To say a problem is learnable if with increasing number of samples, the error rate goes infinitely close to zero. A formal descriptioon is:</p>

<script type="math/tex; mode=display">P(error \ge \epsilon) \le \delta</script>

<p>Note that, $\epsilon$ is the error rate of one sample. Obviously the left part is dependent on $N$, </p>

<script type="math/tex; mode=display">P(error \ge \epsilon) = p^N \le \delta</script>

<p>There is another equivalent formulation: for any positive error rate $\epsilon$和$\delta$, whether there exists a $N$.</p>

<p>Of course, not all models are learnable. The most classical example is “learn a finite set”, and all unlearnable question can be reduced to the example. More details of the theory can be found in PAC theory. </p>

<p>Not all the learning are based on probability, some simple models can be directed learnt to the objective form. For example the previously mentioned <a href="http://blog.htedsv.com/blog/2014/08/15/anluin-learning-algorithm/">Angluin’s Learning Algorithm</a>，which is based on the consistance, of the current example and previous examples.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An extension to Spotlight]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2015/01/17/spotlight-extension/"/>
    <updated>2015-01-17T19:34:08+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2015/01/17/spotlight-extension</id>
    <content type="html"><![CDATA[<p>The current version of spotlight of MacOS is the better than any other versions in the past, however it’s not comparable to Alfred. To day I came up with an extension to Spotlight. </p>

<ul>
  <li>First write a Shell file, no matter where it is. For example in the following example, open the newest folder which contains the lastest homework.</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#! /bin/sh
</span><span class="line"># Filename: isc
</span><span class="line">find ~/Desktop/Dropbox/ISC/Exercises/ -name "SC*" -type d | tail -n 1 | xargs open</span></code></pre></td></tr></table></div></figure></notextile></div>
<ul>
  <li>change permission</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">&gt; chmod +x isc</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>
    <p>right click of the file -&gt; “Get Info” -&gt; “Open With” -&gt; “iTerm” -&gt; “Change All”</p>
  </li>
  <li>
    <p>search in spotlight and execute</p>
  </li>
</ul>

<p>It’s only a simple extension with the shell, but it’s still not so flexible which accepts arguments from input and give output in realtime. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于德国教育的私吐槽]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/12/14/guan-yu-de-guo-jiao-yu-de-si-tu-cao/"/>
    <updated>2014-12-14T13:07:06+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/12/14/guan-yu-de-guo-jiao-yu-de-si-tu-cao</id>
    <content type="html"><![CDATA[<p>列举几个关于德国教育的私吐槽。之前在知乎上评论或者发表过一些对于德国大学教育的看法，从始至终，无论对方如何恶言相对，我都保持和善和尽量谦虚的态度和他讨论，但最后发现，真正得到对方的认可，不是通过你的讨论态度、不是你的论点、不是你说话的逻辑，重要的是你有多了不起的经历。只要有这些经历，对方就会认为自己没资格继续否定你，否则，你的所有观点都是逻辑混乱。大概经历了几次，也就不再会去知乎参与德国或者德国大学相关的问题了。</p>

<p>不过最近的一些学习体验还是让我想对德国大学或者教育发表一些看法。当然我不是最有资格发表评论的，经历也肯定比不上很多人。但是因为从未在公开场合看到完全符合我的认识的想法或观点，所以我很希望能把我的认识在这个相对隐私的地方表达出来，以免招来不必要的非议。</p>

<!--more-->

<h1 id="section">教学与科研</h1>

<p>在很多人看来，科研重于教学，一个发过顶级会议论文的人一定比每天花大量时间上课学习的人要厉害，因为科研活动能培养一个人全面独立的思维，而上课只能学来一些死知识。然而有这一观点的人除了人云亦云的情况之外，基本都是受了国内学术氛围的影响。国内教授能在基础理论上有深入理解并且转化到课堂上的少之又少，基础理论在计算机学生心中的印象是“无聊”“没用”，即使学得好，领悟的透，也并不一定会用。</p>

<p>然而，更多情况下，无论是工作、科研，一定程度上都是在消耗课上学习的知识，尽管科研、工作过程中会继续补充知识，但是没因为没有系统的知识体系，学习速度、深度肯定远比不上课堂。那些没有把课上知识深入理解的人一定不会在科研道路上走得太远。科研并不比工作高尚太多，都是在有目的有针对性的解决不同层面具体问题。</p>

<p>鄙视“上课”的人不仅表现在对科研盲目崇拜上，还表现在对于自学成才的大牛的盲目崇拜以及效仿上。同样的知识，自学而成的大牛往往比上课或者某种循规蹈矩学来的人更容易得到崇拜。从而有时会形成一种风气，放弃好的学习资源去选择自学，仿佛这么做了就能让自己更牛。</p>

<p>这是搞错了因果关系，厉害的人被人关注，往往是因为他处在相对弱的环境里，比如老师水平很差，没有办法轻易获得好的学习资源等等，从而迫不得已选择自学。自学成才的人尽管自我提高了自学能力，但是往往会遇到两个问题：</p>

<ol>
  <li>
    <p>因为自己自学的小成就而对正式、规范的知识体系不屑一顾。比如我在不懂编译原理的时候写出的编译器大作业没有符号表，词法分析和语法分析被写在了一起，并且当时自认为这是化繁为简的行为，因为经过我的改进，减少了代码量、简化了步骤、提高了效率。并且对规范的流程不屑一顾。直到后来深入学习，才意识到规范化的重要性，野路子因为对问题的简化假设，往往做不大，问题稍微复杂一些就要重写很多东西。</p>
  </li>
  <li>
    <p>对问题的理解认识流于表面。无论在哪个学科，凡是相对理论一点的东西都会有不同层次的理解，比如在数值分析课上学到了几种基本的矩阵分解和特点。因为当时做了不少作业，所以并不像当年自己从网上找资料时理解的那么肤浅，但是依然没有理解到各种矩阵分解内在的关系。直到这学期学SVD时遇到一些证明题，从课上的仅有几个理论完全难以下手，学长看完就可以瞬间联想到从Schur分解的结论推过来，从而解决了Singular-Value的一些问题。我才意识到我之前自己为是的学习成果是多么不堪一击。</p>
  </li>
  <li>
    <p>浪费更多的时间。一些公认的优秀学习资源（比如某某教授上的课）不一定比自己查阅资料的方式学习效率更高，但是一定是在兼顾理解深度和广度的最高效做法。随着知识的深入，自学的人往往会遇到很多理解瓶颈，可能需要比上课更多的时间重新理解，才能突破瓶颈。</p>
  </li>
</ol>

<p>正如前面所说，科研是在消耗课堂学过的基础知识，科研虽然听起来高端，但深度一定比不上那些基础知识，新的idea是否可行，从理论模型的层面上一定能找到答案的，基础知识薄弱的人，往往会为一个idea的可行性被迫重新学习一些资料，因为被迫，这些临时学来的东西又难以转化到另一个idea的判断上。</p>

<p>说了这么多，我无非是要表达，在学术上，那些看似捷径的成功做法都很难成就长久的成功。那些片面看重科研重视程度而忽略教学的做法对于学校和希望从事科研的学生个人的长久发展都是不利的。</p>

<h1 id="section-1">大学排名</h1>

<p>除了批评德国过于重视“教学”之外，其他的批评声音很多都指向大学排名。毕竟大学排名是最客观、可量化的标准。但是既然标准化，很多特殊因素就可能很难被完全考虑进去。RWTH的Informatik专业会有10个左右的研究所，负责教学和科研活动。然而在亚琛的强项方向：形式化、逻辑的1所和7所，都只有1个教授，1-3个讲师。其余全是博士生。但是这一个教授的水平往往是领域内奠基人。相比之下，QS排名更高的KIT，每个研究所会有5-10个教授。</p>

<p>RWTH的自然语言识别处理的研究所也是只有一个教授，1个讲师，然而教授是机器翻译领域几个奠基人之一，Google的数据中，他的引用数有30000+，在一个相对不算太水的领域，这个数字已经能说明一些问题了。如果在亚琛想做自然语言相关的学习或者科研，你一定会频繁地接触到Ney教授，比如读博士，你的导师只能是Ney，当然从任何角度来说，这都比去排名更高，跟水平一般的教授要划算得多。</p>

<p>这些东西是大学排名反映不出来的。</p>

<h1 id="section-2">本科生水平与课余生活</h1>

<p>最近因为感觉自己在理论课的思维方式和助教相差很大，可能是知识结构导致的，所以最近开始狂补本科生课程。最大的感受就是德国的本科教育非常的填鸭。必修课的内容2年学完，剩下一年上一上选修课，而且大部分人还学有余力，可以提前修Master的课程。最可怕的是，他们的高中毕业水平和中国学生相差太多，基本中国一般的学生过来大一都能拿到相当好的成绩。然而2年之后，德国学生的理论水平就比我这个学了4年本科的学生高出不少了。以至于自己如果不找时间补他们的本科课，Master的选课空间就小了很多。</p>

<p>具体来说，一门叫做 Diskrete Struktur 的第二学期本科课程（直译就是“离散结构”）。可能和我们的离散数学比较像。但是书中分的三个大章节分别是组合数学、图论、代数，只不过都是比较基础的概念。回顾一下，虽然自己好像也学了一个学期的离散数学，不过也不比他们图论的部分多什么了。</p>

<p>因为他们本科生的课程紧，作业多，所以一方面他们本科期间没有时间参加类似于ACM竞赛之类的课外活动；另一方面因为长期接受这样的填鸭教育，导致在Master和PhD阶段尽管基础扎实，但是缺乏自主性，成果并不多。</p>

<h1 id="section-3">形式化</h1>

<p>形式化向来是法国的特点，然而不知道为什么，外国学生中比较喜欢理论方向的来到亚琛都会有“过度形式化”的感受。不仅仅是理论方向，比如授课时，每个新知识都是先给出定义，然后就是理论和证明，中间很少会有具体的例子，以至于对定义的理解要通过理论的证明来猜。经过一番猜测之后，对这一理论有了一些理解，发现它并不难，完全可以从一个感性的角度更轻松的理解。</p>

<p>另外一个表现就是很多偏工程课程也会大量把自己的概念去靠向形式化。一方面这种形式化本身就是不严谨的，另一方面，把问题形式化的意义是使用形式化方法解决问题，然而这些问题明明直接去理解会更简单。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[更通用的角度理解程序与数据]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program/"/>
    <updated>2014-12-01T00:22:00+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/12/01/general-way-to-understand-program</id>
    <content type="html"><![CDATA[<p>本学期有一门叫做“Fondation of Data Science”的课，打着Data Science的幌子来讲数据库，让我完全对7所的理论课兴趣大减。究其原因，大概是它的某些先修课程我之前没学过，所以学起来吃力。</p>

<p>尽管如此，其中的很多观点还是值得借鉴和思考的。现在回想本科比较重要的课，大概有这么几门：编译原理、数据库、计算机网络、操作系统。我认为包括美国在内的计算机专业比较强的学校都在教育过程中弱化各个领域的关系。这当然从某些角度来说是有好处的。但是从更抽象、更理论的角度来看，不利于学生对于本质问题的理解。
<!--more--></p>

<p>比如数据库，这个比较晚才出现的概念，在正式用来解决并发管理、高效操作之前，它也被看做解决问题的通用模型的一部分。具体来说，就是<strong>我们认为世界上所有的数据都存在关系数据库里面，所有的程序都是SQL写的</strong>，因为我们假设这就是最基础的通用模型。我们为什么不假设别的语言呢，比如C或者Java？因为现在我们是要站在数据的角度来思考，而关系数据库是相对好理解的模型中最基本的，其他的NoSQL数据库模型算不上理论模型，因为它们更自由，缺少约束。既然已经确定了关系数据库，那么SQL显然是最合适的程序语言。但是基本的SQL并不能表达其他语言的所有逻辑，最基本的，就是递归。因为SQL和代数表达式，一阶逻辑等价（证明方法很多，这里不赘述），所以我们也可以换个角度理解一个概念，一阶逻辑（First-Order Logik）像是一维空间，没有层次。我们需要“函数”来完善SQL来实现递归逻辑。<a href="http://en.wikipedia.org/wiki/Datalog">Datalog</a>就是这么一个东西，既包含了一般的以细节的规则，每条规则是一些关系表达式的Intersection，这是不是和编译原理中的文法从逻辑上很类似？因为我们写的文法是针对Context-Free Language的，所以是不是现在又打通了语言和程序的关系？回到Datalog上，因为左边的变量可以出现在右边，所以我们可以实现递归了。但这并不完全是我们要的，因为要保证其单调性，所以Datalog不能包含“非”运算，所以要使用它的升级版Inflationary Datalog。相比之下，传统的SQL从理论层面上来看简直弱爆了。</p>

<p>现在再回到程序语言上来看，编程语言最本质的性质不是循环Loop、选择if，而是一阶和高阶，也就是是否包含递归（而不仅仅是函数，函数可以不实现递归）。至于“循环”和“选择”只是为了更适应人类和业务思维而产生的，即使没有他们，也可以用其他的方式用递归来代替。这就是函数式编程所谓“最高级”的地方，他们采用的都是更“元”的操作。</p>

<p>说到这，就不得不提Prolog——我打算写这篇文章的初衷。在基本了解这门语言的语法之后不会意识到这门语言的强大，但当我们具备了一些数理逻辑的知识之后，发现这门语言就是在用一阶逻辑编程。在以形式理论发展如日中天的时候，几乎所有科学家都认为逻辑推导就是构造世界的基本方法。对于形式化推导求值方法，可以了解<a href="http://en.wikipedia.org/wiki/Ehrenfeucht%E2%80%93Fra%C3%AFss%C3%A9_game">Ehrenfeucht–Fraïssé game</a>。同样的，对于任意一个问题，只要我们构造出这个问题的基本事实和推到规则，用术语来说就是公理系统(Axiom System)和赋值(Interpretation)，我们就可以判断这个子世界的任何命题的真假，前提是你需要了解理论(Theory)是由公理系统分形出来的无限谓词句子的集合。回到Prolog，我们只要向它输入所有世界的规则，就可以知道任何可以推断出的问题。然而这种最直观的构造世界的方式并没有被工业界认可。其原因在我看来有几点：</p>

<ol>
  <li>尽管它更加直观，但是从表达能力上来看，并不比其他语言更强大。我们完全可以用其他语言代替，即任何常见的编程语言都能实现与之等价的功能。</li>
  <li>理论上更直观并不代表应用上更直观，很多简单的问题并不需要抽象出它的实施和推理规则就能解决，或者说，大部分我们能接触到的问题并没有太多的逻辑规则。</li>
  <li>每种问题会有特殊的解法，我们的目标是解决问题，而不是把所有问题标准化成一种问题，标准化的过程会把问题复杂化。</li>
</ol>

<p>真正理解这几点之后，才不会让一些略懂人工智能的人们对人工智能盲目崇拜，对形式逻辑盲目崇拜。而我们之前接受的教育似乎完完全全的把这些东西避开了，以至于让一些求知欲更强的学生在课外学习中以为自己了解了更高深的东西而发出更幼稚的言论。</p>

<p>说到通用，前面已经提到数据库是表述数据相对直观的最通用的模型。然而在更早的时候就出现了一个更加通用的模型用来表述数据的输入、输出、中间过程——那就是用01纸带做记录的图灵机。图灵机最大，或者说唯一的意义在于分析程序的复杂度，或者叫实现难度。这是一项相当底层的任务，所以最好也要用相当底层的数据表示来完成更为恰当。具体原因可以这么解释：所有较难的复杂度分析几乎都是基于reduction去和已知复杂度的模型建立联系，也就是说，越是底层的表示，reduction的自由度也就越大，具体方法可以参照<a href="http://en.wikipedia.org/wiki/Reduction_%28complexity%29">Reduction</a>，而P != NP并不是一个已经被证实的问题，尽管我们感性上能理解，但不存在这样的方法来证明，我们找不到NP到P的reduction并不能代表不存在。</p>

<p>提到复杂度，可能也要顺便提一下，时间复杂度NP和P并不是衡量复杂度的唯一标准，在其之上还有PSPACE和NPSPACE，只不过是把占用的空间作为评价标准，刚了解时有点毁三观。之后有时间我会单独写一篇关于复杂度的东西。</p>

<p>关于通用，还有一个我一直记错的概念，就是“通用图灵机”，他只是一个可以模拟图灵机的图灵机，即把需要模拟的图灵机的状态转移表用二进制表示作为字符串输入给通用图灵机，让其模拟。它就像一个编译器，读一段代码，然后执行这段代码，高明的地方就是两种代码用同一种语言写的，也就是说，我们可以再用一另一个通用图灵机来模拟这个通用图灵机——即用编译器来编译执行编译器。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[确定世界与概率世界]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie/"/>
    <updated>2014-10-24T21:45:20+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/10/24/que-ding-shi-jie-yu-gai-lu-shi-jie</id>
    <content type="html"><![CDATA[<p>这篇没有涉及具体的理论知识，主要是建立在部分相关经历之上的一些臆想，所以多少会有些民科思维。的确，我在这方面的经验少的可怜。但是相比于能看到我博客的人来说，信息不对称还是非常严重的。从刚入学我就多次表达了我的震惊。我发现计算机科学还有这么大一片领域我几乎一无所知，最可怕的是，我所谓的一些“基础和见识”可能也就是当地大二学生的水平。经过了一个暑假的沉淀和反思，我对于形式理论的极端态度渐渐冷却，所以，我也希望我能把它放到和其他领域平等的地位上考虑一些它们之间的关系。</p>

<!--more-->
<p>如果我们想表达两件事情A和B的必然因果关系，形式方法是A-&gt;B，而概率方法是p(B\|A)=1。这就是两个世界的建立基础。所谓“世界”并不是从学科发展的角度，即，不是按照先学p(A)，然后学p(A,B)最后学条件概率。而是按照世界运转的“元操作”。说到这，可能还是要提一下“世界”的动与静。世界可以是一个生态环境，一个程序中的类，也可以是一个人的个体。而“动”和“静”（或者叫行为与状态），可以类比成“算法”和“数据结构”，“类方法”和“类参数”，“代码”与“数据”。不过要认识、分析和解决计算机科学根本的问题还是要放在最根本的模型上。我说的当然不是图灵机，而是比图灵机更早，而且标定计算机能力疆界的——形式逻辑。</p>

<p>从计算机出现甚至更早，整个计算机科学的发展都是建立在形式理论的基础上，比如程序语言的设计、数据库的设计。到了后来，计算机的计算能力已经达到可以真正为人所用。于是就有了大量人开始用计算机来搞数值分析，并因此诞生了一些图灵奖得主。这些就扯远了。总之，到了90年代，单纯的形式理论已经没再出现真正的突破。</p>

<p>形式理论正如前面的例子中提到的，是一套完备的体系，一切都是确定的。当然，“真正的智能无法实现”也是被确定的，因为二阶逻辑的不完备性在图灵机诞生之前已经把计算能力的极限定在了那里。
而无法实现的原因可以直接参照停机问题的矛盾句式。</p>

<p>于是，当我们发现一个世界的运行规则被基本挖掘干净，并且没有找到我们真正需要的东西时，就尝试在确定性世界的框架下加入不确定性，也就是概率来完善形式理论的一些不足：</p>

<ol>
  <li>
    <p>一个世界的规则是确定的，不能随着外部输入而变化，一个世界需要更多的可能性。尽管随着概率的加入，我们可以通过修改参数的方式修改世界的一些规则（一个具体的例子就是通过样本来训练机器学习模型的参数），然而制定规则的世界依然是确定的。概率的加入只是帮助我们从应用的角度解决了更多具体的问题，对于真正的理论研究上的“智能”并没有什么帮助。</p>
  </li>
  <li>
    <p>真实世界是概率的，至少我们对于一个值得研究的子世界（所谓子世界，就是真实世界中我们需要研究的对象集合的子集），其中的个体都是概率的；或者说全世界是确定的，其原子行为构成一个闭包。但对于真实世界的任意一个有限子集，都不是封闭的，所以我们只能把其中的一部分当成随机变量来考虑和认识。通过从“元”上达到和真实世界的一致可以帮助我们更好的转移现实世界的内容到构造世界。尽管我们依然构造不出绝对的智能，但是已经可以用简单的智能结合真实生活，完善生活中的问题，这样可能更有社会意义。</p>
  </li>
  <li>
    <p>我们可以在假设存在一个真实世界的子集，并且在这个我们要研究的子集是一个闭包，即，一切都是确定的。但问题就变成，为了尽量真实的模拟现实世界，需要足够大的子集，以至于超过了计算机的运算能力，难以继续，一个实例就是神经网络算法的发展瓶颈。</p>
  </li>
  <li>
    <p>确定性世界并非一无是处，因为其关系的建立都是稳固而确定的，所以整个世界体系也是可靠的（往往是无限大），而概率世界更多的是表达隐含关系，不确定关系，所以单纯由概率搭建的世界（比如贝叶斯网络）不会很庞大，仅仅是为了解决某类特定小问题。</p>
  </li>
</ol>

<p>当然概率帮助我们简化了一些“确定性”解决起来很棘手的问题，但是根本性问题永远无法解决。也正因为这一点，形式化理论研究很早就进入了瓶颈，而且创造不了价值，所以时至今日，只有为数不多的欧洲大学把它作为重要计算机专业最重要的基础知识传授给学生。然而学习它却对于理解计算机科学从宏观上能有最重要的价值。</p>

<p>如今，“单纯的形式理论”已死，也不会有人太多人研究“真正的智能”。更多的是将其和统计学方法结合，解决生活中具体的问题。相信随着更多复杂的形式理论的引入，我们能在未来构建更加完善的“世界”。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[一年咖啡龄感受总结]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/23/yi-nian-ka-pei-ling-gan-shou-zong-jie/"/>
    <updated>2014-09-23T04:40:00+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/23/yi-nian-ka-pei-ling-gan-shou-zong-jie</id>
    <content type="html"><![CDATA[<p>尽管从大三开始就已经开始喝咖啡，不过在国内，更多的还是笼罩在速溶咖啡的阴云之下。速溶咖啡能比较受欢迎，一方面是因为比较好买到的速溶咖啡都比较甜，大众能接受；另一方面也是因为它确实有一定的提神功效。不过后来到了德国，比较甜的速溶不是那么普遍，所以也就趁超市打折买了一个滴漏咖啡壶，从此开始喝起了咖啡粉。</p>

<p>德国的咖啡粉比较好买到，而且一般一包也就4欧左右，即使每天喝也差不多能喝两个月，比较实惠。</p>

<!--more-->

<h1 id="section">适应</h1>

<p>开始的时候因为太苦，所以每次都要往咖啡里加一些白砂糖或者牛奶。大概适应了一两周，开始习惯直接喝，不添加其他食物。直接喝的原因也很简单，就是为了能够更充分的享受咖啡粉本身的香气。后来也很多次尝试添加糖，打些奶泡，不过咖啡的香气却被削弱了不少。</p>

<h1 id="section-1">健康</h1>

<p>相比于茶，咖啡可能更适合火气旺盛的西方人。在过去的一年里，我每天平均能喝一杯多的咖啡，有时候是因为兴趣，有时候是提神需要。喝完之后，一个最常见的反应就是利尿，所以经常会感觉身体缺水。如果完全空腹喝，也会身体过于亢奋而不能专注。最严重的可能也是对它的依赖——很长一段时间都会很享受刚起床喝一杯咖啡的时刻。在这一年中，我也莫名其妙的发烧了3次，最严重的一次嗓子疼到不能喝粥。用中医的说法应该就是火气太重，上火了。在最后一次发烧之后，我又尝试加入些牛奶，感觉对胃部刺激就轻了很多，所以加奶咖啡的另一个好处就是能缓解上火。</p>

<h1 id="section-2">效率</h1>

<p>尤其是到了后半年，因为学习压力太大，每天都是被作业牵着鼻子走，所以必须要用咖啡提高效率，否则在思考一些数学题时完全没有任何进展。然而，咖啡虽然短期内效果明显，可以把大脑疲惫期拖延到睡觉的时间，然而时间一长，推迟效果越来越不明显，甚至喝完之后，本来还算清醒的大脑直接就疲惫了，然而身体还处于亢奋，所以没办法通过睡觉消除疲惫，只能等咖啡的效果消失了再睡觉，反而更耽误时间。</p>

<p>所以，当这些负面效果逐渐明显时，我会稍微克制几天，通过多喝水和跑步来缓解。</p>

<h1 id="section-3">咖啡胶囊</h1>

<p>有一次超市胶囊咖啡促销，借机买了一台胶囊咖啡机，从此喝咖啡变得更简单。通过网上了解的一些信息，咖啡胶囊的品质整体会比普通的粉品质高一些。不过刚开始喝得几天反倒觉得胶囊更苦，而且也没有更香。适应了一周，慢慢觉得两种方式冲出的咖啡应该说是口味不同，各有千秋。</p>

<h1 id="section-4">口味</h1>

<p>无论是滴漏式咖啡机还是胶囊咖啡机，从口味上都难以和手冲相比，所以时间充裕的时候我也会和手冲咖啡。个人感觉手冲咖啡一方面因为水和咖啡粉能够更充分的接触，另一方面漏下的咖啡没有被持续加热，所以味道更香，酸涩感更淡。</p>

<h1 id="section-5">免疫</h1>

<p>有些人咖啡喝多了会有免疫，我觉得有些时候可能真是因为咖啡喝的不够多。我对自己冲的咖啡就从来没有免疫过，但是昨天下午在一家咖啡厅不听服务生劝阻的喝了两杯Espresso却没影响到我晚上睡觉。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automaton to Regex]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/12/automaton-to-regex/"/>
    <updated>2014-09-12T04:12:15+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/12/automaton-to-regex</id>
    <content type="html"><![CDATA[<p>前面提过了Angluin学习算法的基本原理和作用，通过不断回答程序提问的句子是否属于该指定语言来生成这门语言对应的最简DFA。然而DFA的表示并不是很直观，也不实用。所以我今天介绍一个方法，把生成的DFA转化成对应的正则表达式。
<!--more--></p>

<h1 id="section">问题</h1>

<p>通过之前在网上搜索，没有找到比较完美的解决方案，但是从正则表达式到DFA的方法却有很多。我想“DFA -&gt; Regex”的主要难点有两点：
- 没有一个好的方式化简(合并)生成的正则表达式，或者说不能把任意正则表达式形式化的转化成一个特定的范式。然而对于任意NFA都是有范式的，即“最简DFA”。所以我经常觉得正则表达式是一个不太好的设计。
- 存在多个互相可达的终止状态的情况会丢掉一些句子。</p>

<h1 id="section-1">相关工作</h1>

<p><a href="http://cs.stackexchange.com/questions/2016/how-to-convert-finite-automata-to-regular-expressions">这里</a>有一个方法实现自动机到NFA的转化，是针对更广泛的NFA，但是限制也很大，就是不能完美解决多终止状态的情况。其实现方法可以归结为：<strong>每一步删除一个中间状态k，并且对于每对状态i，j，如果存在i-&gt;k，k-&gt;j，那么删除的时候用L[i][k].star(L[k][k]).L[k][j]来完善L[i][j]，其中L[i][j]是从状态i到j的所有语言，初始情况两个状态间的语言最多只包括了一个字母。最后计算初始状态到终止状态的语言就是结论</strong> 这么做的问题就在于如果有多个终止状态，那么他们都不会被删除，但是最后结果会漏掉状态start -&gt; f1 -&gt; f2的语言，其中f1，f2是终止状态。由于在剩余的状态集合中可能存在多种路径，而且不能继续删除，比如f1 -&gt; start -&gt; f2，因此情况会变得非常复杂。</p>

<h1 id="floyd">Floyd算法计算状态间语言</h1>

<p>我的想法是不删除中间状态，而是通过Floyd算法直接计算每两个状态之间的语言，听起来似乎和删除中间状态效果一样，不过用Floyd的好处是能把终止状态作为中间状态的情况也考虑全面。这样，最终的答案就是对于每一个终结状态f，<script type="math/tex"> result = \Sigma_{f \in F} L_{start,f} </script></p>

<h1 id="section-2">计算过程使用结构元组而不是直接用字符串</h1>

<p>在更新状态间语言的过程中，<strong>L[i][j]</strong>不是直接用字符串格式的正则表达式来存储i，j之间的语言，因为用字符串尽管不会影响结果的正确性，但是会给字符串化简制造麻烦。比如消除冗余括号。所以我使用了tuple来代替字符串来描述正则表达式的”树结构”，描述只使用’+’, ‘.’, ‘*‘三种结构，分别代表“语言集合的并”，“语言的连接”，“出现任意次”。当然，这个符号定义和平时使用的正则表达式有区别，只要修改程序的输出函数即可。比如(‘+’, [‘a’, ‘b’, ‘c’])代表一个有4个节点的树，生成的语言L只接受aa,b,c三个句子。</p>

<h1 id="section-3">冗余消除策略</h1>

<p>用Floyd算法遇到的最大问题就是会制造很多语言的冗余。下面说几个主要策略，不过并不能保证完全消除，其中一些策略的正确性是建立在“输入DFA是最简DFA”的基础上的。</p>

<ul>
  <li>消除空结构，比如(‘+’, []), (‘+’, [’’])，这种结果本身不会造成错误，但是会在计算过程中造成冗余</li>
  <li>消除重复括号，当一个结构例如s=(‘+’, [‘1’,’2’])作为子结构出现在另外一个结构中时，并且主结构包含多于一个元素，这时要给子结构的字符串形式加括号，比如t=(‘.’, [s, (‘*’, ‘b’, ‘c’)])</li>
  <li>消除不必要的循环变量运算，在Floyd算法中需要枚举变量k, i, j。然而根据k和i，j的等价关系可以省略一些运算，例如如果k和i相等，那么当<script type="math/tex">L_{i,j} \neq EMPTY</script>时，可以化简运算为<script type="math/tex">L_{i,j}= star(i,i).L_{i,j}</script></li>
  <li>最简DFA，这个优化并不需要我用代码实现什么，而是作为一项限制，用来证明我很多看似不严谨的代码的正确性。例如不会有两个等价的终止状态。</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[考试总结]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/09/08/kao-shi-zong-jie/"/>
    <updated>2014-09-08T16:33:34+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/09/08/kao-shi-zong-jie</id>
    <content type="html"><![CDATA[<p>中秋佳节之际做一些小学生做的事情：考试总结。</p>

<p>这学期总体来说收获很大，但是和成果并不匹配，也就是说成绩很不理想，或者说和我之前的经验恰恰相反，唯一一门考试资格都没有获得的课是我本科算上加分之后拿到满分的“编译原理”，而且两次课程内容基本一致。唯一一门挂掉的MA课是我自认为本科期间做过很多相关工作（包括两次实验室经历和一次实习经历）的“数据挖掘”。通过的3门都是之前接触不多，反而最后能低分飘过。下面总结一下原因：
<!--more--></p>

<h1 id="section">基础知识薄弱</h1>

<p>现在回头看看，确实觉得这一学期学的很辛苦，但是实际上那些被奉为神课的课并没有想象的那么难，即使缺少一些必要的基础知识，也是可以通过以前学习的考试技巧来Hack一下。尤其是理论方向的课，现在想想，每门课研究的“实体”其实都有章可循。大概都是归为相关性质与相关实例，性质引出实例，不同的实例又有不同的新性质。另外，通过考试内容可以发现，很多性质的证明并不重要，不需要深刻的理解。</p>

<h1 id="section-1">题型不适应</h1>

<p>基本每门课都不会给太多复习资料，对于第一次来德国，完全不知道如何复习，很多国内通用的复习策略在这里行不通。比如没有办法通过作业中的题目本身来判断是否可能在考试中出现，尤其是第一门的数据挖掘，看了一遍卷子就直接傻了。</p>

<h1 id="section-2">时间安排不合理</h1>

<p>考前一方面有些轻敌，一方面回家心切，所以5门必须的考试被安排在15天里，以至于第一门发现策略失败后，后面完全没有时间调整。另外一个很主观的原因就是这学期记忆力大不如前，连写带画的看了5遍的课件，考试时还是记不住。</p>

<p>时间安排的另一个方面就是平时上课写作业，每天被作业牵着鼻子走，最后只是为了草草完成作业，难点没有时间搞懂，重点也没有时间回顾。</p>

<h1 id="section-3">考试与学习的协调</h1>

<p>这也是最让我最难以权衡的。正如和在国内一样，应对考试必须要有很多额外的时间去刷题，背一些没有启发性的知识，并不是你把课上的东西领会了或者有所推广就一定能考试。所谓启发性，也就是我最感兴趣的。而这些启发性又难以用标准化的题目去考核。所以到了最后，就不得已放弃很多思考启发性问题的时间去背一些标准化的概念。</p>

<p>上个学期总体说来时间安排还算合理，或者说幸运，每天一门作业。在接下来的学期上课撞车更严重，恐怕要更加努力了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数理逻辑如何改变我的世界]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/08/20/how-mathematische-logik-changes-my-world/"/>
    <updated>2014-08-20T18:25:31+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/08/20/how-mathematische-logik-changes-my-world</id>
    <content type="html"><![CDATA[<p>在大二上学期，我自认为对程序员的工作有了足够的了解——无非就是学语言，学库，即使是涉及到其他专业领域，也只要掌握最基本的概念，然后调用对应的库就可以了。于是“程序员”这个概念更多的是对经验的要求——对各种库的用法掌握、编码风格的统一等等。与之相对应的就是我自认为之后的所有课程就是背概念，因为计算机科学的基本轮廓都是围绕着编程的。</p>

<p>不过当我真正学习编译、操作系统以及自己后来看一些工业界用机器学习解决实际问题的文章的时候，忽然发现，我才刚刚了解计算机科学是在做什么。</p>

<p>Master的第一学期学习了数理逻辑之后，我的世界观进一步被摧毁。我在之前从没认真想过“为什么计算机科学可以看做数学的分支？”“计算机科学在过去几十年都有哪些成果？”“为什么说图灵是计算机之父？他的成就都有什么价值？”这类的问题。这些问题似乎成为了计算机科学系统的公理，不需要证明，也无法解释。虽然并不是每个问题都能直接从“数理逻辑”上找到答案，但是很多它却给了我们一个入口，让我们看到计算机科学目前很少被关注的一面。</p>

<!--more-->

<h1 id="section">计算机科学与数学</h1>

<p>如果去问刚上本科的我，为什么说计算机科学是数学的分支，我多半会说“因为算法、数据结构都是数学问题，计算机科学经常要处理一些数学问题。”这个回答内容不算错，不过答非所问，因为既然说“分支”，必然是有“根”有“叶”。计算机处理数学问题最多只能说“计算机和数学关系密切，计算机因为其性能特点，能帮助解决一些数学问题”。然而体现不出计算机理论如何建立在数学之上，或者说计算机有什么核心理论是从数学得出的。</p>

<p>如果说有的话，我觉得“数理逻辑”应该是其中最重要的一门。很多基础学科都会有一些理论能普世万物，“数理逻辑”因为其主要其应用领域算到了计算机科学下面，但很多核心概念都是能普世万物的。比如公理系统，公理系统并不只作用于数域，任何<strong>理论(Theory)</strong>都有可能会有有一个公理系统。公理系统具体来说就是描述一个理论最小的句子集合，以至于建立在该理论上面的所有句子都能通过这个集合的句子组合判断真伪。而一个理论，正是一个无限的句子集合。建立在这个基础上，我们就可以轻易判断一个理论有没有可能被证明。这仿佛给给各学科开疆拓土，只要有公理系统，那么任何问题都可以用形式化方法判断真伪，当然也不会有什么“无法证明的问题”了。然而并不是每个理论都能被公理化（具体细节专业性较强，在此不做讨论），而且对于自然学科，总会有新的发现，因此公理系统也就不断变化，即使是已经被证明的理论，也会在之后被推翻。</p>

<p>回到计算机与数学上面，正是因为数学与计算机对计算问题的研究都建立在一个较为封闭的系统，因此才会有价值。具体到编程语言上，可以把命令式编程语言中看成顺序结构、循环结构与判断结构看成它的公理系统，它与函数式编程的基础“Lambda表达式”是原子等价的，因此可以把他们具有同样的表达能力。</p>

<p>数理逻辑中另外一个非常有用的定理就是一阶逻辑的完备性定理。很多问题一旦涉及到“无限”，那么许多有限问题上的做法都不能适用了，比如我们不能对比两个无限字符串中某个字母的个数的长度奇偶性。而完备性定理正是帮我们把无限问题放在有限问题上解决。</p>

<h1 id="acm">数理逻辑与ACM</h1>

<p>在来德国之前，我从来没有怀疑过计算机领域还有什么问题是因为其模型抽象而难以理解，主要还是因为德国大学在ACM方面基本没有涉足。然而从数理逻辑课上遇到的很多问题，即使直接看答案也要思考几个小时才能彻底看懂。所以说，其实抽象的问题很多，只是通过编程方法能够表达出来的数量很有限。</p>

<p>另一方面，ACM涉及的知识散碎，各种算法间相互关联性并不大，所以每掌握一门新算法，收获是常数增长，从中得到的一些小方法或许能够很好地解决一些常见小问题，但是没有体系，无法深入。</p>

<h1 id="section-1">关于图灵机</h1>

<p>很多人都喜欢表达对图灵这一计算机科学鼻祖的憧憬，不过大多数人可能对其认识仅仅停留在知道图灵机的程度上，至于图灵机的意义和相关理论，感兴趣的人似乎不多。了解的多一些的可能还会知道它和自动机关系密切，受限图灵机是和自动机、以及Pushdown System等价的，然而根据市面上的为数不多的自动机相关资料，很少会提及它与数理逻辑之间的关系。事实上受限图灵机或者说与其对应的自动机课可以看成一个<strong>理论</strong>，也就是说，图灵机也可以被公理化的。在自动机领域有很多图灵机的变种，有些互相等价，有些不等价，甚至随便一次自动机考试的试卷都能看到一个新变种。要讨论清楚任意两种之间的关系可能方法各异，但是如果从最本源的角度——公理系统来考虑，很多问题一下子就简单多了。</p>

<h1 id="section-2">函数式编程</h1>

<p>与其要讨论“函数式编程”这种被讨论烂的话题，不如简单说一下相对不那么烂的“Lambda演算”。所谓的看似高端的函数是语言Lisp、Scheme、Haskell从抽象程度的角度来看都是Lambda演算的一个具体实现方法。那些希望通过掌握Lisp来标榜自己的人，与其掌握一个实现，不如首先想透lambda表达式相关的一些抽象问题。比如Lambda不允许自递归，而是通过不动点的方式。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Angluin-Learning-Algorithm]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/08/15/anluin-learning-algorithm/"/>
    <updated>2014-08-15T20:24:10+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/08/15/anluin-learning-algorithm</id>
    <content type="html"><![CDATA[<p>前两天在知乎回答了一个关于<a href="http://www.zhihu.com/question/24824487/answer/29196253">识别某种特定语言的方法</a>，根据这两天0赞同的情况，不禁让我对知户上计算机从业者的基础理论水平深表怀疑，不过当我看到第一名的答案时确实非常激动，因为一下子想起了2年前在知乎实习时为了识别某个语言，发现用正则表达式可能会非常麻烦，于是写了个自动机，当时刚刚从编译课上学了些自动机的原理，并且第一次正式尝试用它来解决实际遇到的问题，自然是非常激动，而第一名的方法也正是通过构造自动机来生成正则表达式。</p>

<p>不过在那之后，“自动机”与“编译”在我脑中基本成为两个非常相似的概念。我因为自动机的神奇而对编译中各种文法以及编译器的实现产生了兴趣。自认为目前做过最NB的一件事也于此相关——在看过龙书第一章，并且学习了推到规则之后，自己动手写了个简单的编译器作为大作业，不过大作业并没有因此得到满分，因为理论的欠缺，我甚至分不清lexical和syntax之间的区别，自认为高明的把词法分析、语法分析写在一起，并且跳过符号表；这些都被老师当成了扣分点。</p>

<!--more-->

<p>知道来到RWTH，一学期同时学了自动机与编译原理，才渐渐认识到两种理论其实各自自成体系，交集并没我想象的那么多。重学一边编译的过程从理论方面并没有给我更多的收获，这也让我第一次感觉本科上的课也不是都那么水，但是每周的作业中都包含编程题，也就是实现一个语言的一部分代码，当然这部分都是每周课上新学的，从词法分析，到LL(1)、LR(0)、SLR(1)、LR(1)一直到最后的语义分析和生成代码部分。相比于自己实现，这种方法显然更加“德国”，减少了像我一样“误入歧途”的风险。因为从这个过程中确实学到了很多编译器更规范的做法。</p>

<p>不过相比于编译原理，我从应用自动机课上收获的东西更多，以至于很多东西对我的世界观产生了很大影响，具体的很多想法以后在慢慢谈。今天就来说个具体的，也就是我认为这门课最正统，也是最NB的算法——Angluin’s Learning Algorithm。在回答问题之前我也把<a href="https://github.com/xuanhuangyiqi/AnluinLearning">项目</a>地址贴了上去。除了一些简单的介绍外，我今天想着重介绍下Angluin’s Learning Algorithm的核心：封闭性与一致性。不过在此之前，我还是要说，这个算法不是机器学习算法，不涉及概率，或者说每一步都是为了达到一个严谨的稳定状态。在混沌的世界，机器学习可以解决“一切”问题。这种说法是要表达：对于现实世界，机器学习因为对问题本身要求更低，所以解决问题的范围更广（因此看似神奇），然而也正因为其规律并不能适用所有，所以解决的效果无法达到完美。然而在理想化的形式世界，所有的问题都被抽象，形式化，虽然和现实脱离，但是却能在形式化的世界得到完美解决。下面就要正式介绍一下两个性质：</p>

<p>根据我在回答中的一些简单描述，大概可以把该算法解释为：<strong>通过反复询问新的词是否属于该语言来维护一个观测表的封闭性与一致性。</strong></p>

<h2 id="section">封闭性</h2>

<p>对于有两个词w,v 属于语言L，如果它们后面无论接什么词，都是要么同时属于L，要么同时不属于L，那么可以说这两个词的等价的，如果把他们放到自动机上面从初始状态开始跑一下，它们会停在同一个状态，那么这两个词就是等价的，比如对于正则语言(12)*，12和1212还有121212是互相等价的，1和121也是等价的。然而为了说明两个词的等价，我们并不一定需要找到所有词接在他们后面来验证他们是否属于这一语言，只需要几个有代表性的就够了，而要接的词也就是S集合（具体原因稍后再说），<strong>而R集合中的每一个词都可以根据后接S中的每一个词是否属于语言L来代表一个DFA上的状态</strong>，如果R中两个词w, v它们后接S中的每一个词都是要么同时属于L，要么同时不属于L，那么显然他们是等价的。如果我观测到有一个词R中的词w后接某个字母a之后不在R中，并且和R集合中的任意一个词都不等价，那么目前的DFA是不封闭的，说明wa代表了一个自动机上的新状态，那么通过R中加入wa来维护这个自动机的封闭性。</p>

<h2 id="section-1">一致性</h2>

<p>前面提到了两个词w, v等价的条件，并且我们认为S集合可以代表全集，来验证任意两个词的等价性，然而S集合虽然表面上R中的每个词是否等价区分开了，但其实可能是因为S集合不够大，并没有真正把两个不等价的词区分开，比如从现有S集合来判断，w~v（表示两者等价）。然而如果发现对于某个字母a，wa与va通过S来区分时并不等价，即对于S中的某个词s，was与vas并不是同时属于L，那么显然w和v也并不是真的等价，而区分它们的词正是as，所以通过给S添加元素as来维持自动机的一致性。</p>

<p>好了，虽然算法还有很多小问题值得思考与证明，比如知乎回答下vczh提的问题，但是最核心的东西应该是说清楚了，细节可以以后慢慢讨论。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[反面看极简生活]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/07/30/fan-mian-kan-ji-jian-sheng-huo/"/>
    <updated>2014-07-30T08:54:47+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/07/30/fan-mian-kan-ji-jian-sheng-huo</id>
    <content type="html"><![CDATA[<p>这篇来说一下自己对于极简生活的2年多的实践和成果。最重要的不是倡导人去追求极简，而是如何让它改善我们现有的生活。当然我的大部分观点和主流极简主义倡导者是有出入的。</p>

<p>我从两年前的豆瓣上看到“极简”这个词，当时还不太清楚极简代表着什么，指示看到一些优秀的极简风格设计，或者是品牌，比如MUJI，甚是喜欢。于是我决定尝试改善我的生活，让我能在学习工作中更专注于任务本身。最开始的几个主要方面就是删除手机上的多余app，减少生活用品，卖掉不必要的书，减少自己的信息源（比如当时的Google Reader），处理掉不重要不紧急的任务，改掉信息焦虑症。</p>

<!--more-->
<p>但在之后的一段时间，我发现这样的实践和我很多固有的观念产生了冲突，让我找不到好的妥协方案。比如对待无用或者劣质的生活用品，我从很早就一直在贯彻父母教导的节俭思想，只要还凑活能用的东西就继续用，不要轻易扔东西。然而按照极简生活的要求，我应该果断把它们舍弃，换成一些品质更高、数量更精简的替代品。由于那段时间我要花很多的钱学习德语，准备考试，同时也没有挣钱的途径，自然也不愿意再找父母要钱花在这种小事上，当时觉得极简就是有钱没处花的大人为了挣更多的钱，更舒服的生活才要考虑的。另外一方面极简要求摒弃无用的东西，但事实上，每个物品对于你都有两个维度：使用频率与重要性，另外还有一个获取难度。有太多东西是生活中使用频率并不高，但是每次需要都非常迫切，而且难以及时获得，比如指甲刀、体重秤、插线板。道理同样适用于一些手机app。另外一个问题是处在公共生活空间很难把自己的生活观念让周围的人认同支持，这里“公共生活空间”指的是宿舍或者二人世界，因为不是每个人都愿意按着你的标准来生活，同样每个人对不同物品的几个维度值也各不相同。</p>

<p>总结问题所在：极简并不能做到“极”，因为一个维度上的“极”会引发其他维度造成的问题。</p>

<p>不过幸运的是有一项实践是成功的，因为它实践成本很低。那就是极简我的硬盘。把文档、作品放到云端，常用的文件放到网盘同步，脚本与项目放到Github托管。至于图片没有找到完美的解决办法，目前采用的是放到iPhoto里面，并且把文件夹放在网盘同步目录来实现备份。</p>

<p>不过来到德国之后，发现极简生活的实践变得更加简单，因为德国人对于多样性并不那么在乎，或者说好的品牌很有限，也不会有淘宝用价格诱惑你买便宜而劣质的物品。经常的搬家也不会让你有机会考虑什么是更重要的。</p>

<p>最后一个成功的实践还是手机app。在德国除了查车意外，基本没有什么app是必须的。这也让我的app数量相比在国内大大减少。另外也开始把大量信息源转移到苹果原生的应用上，比如任务、日历、笔记。相比之下，苹果原生的应用做的更简洁，方便，省去了大部分传统优秀GTD应用的无用功能。最后，应用数量成功控制在一个屏幕之内。</p>

<p>当然以上只是一个细碎事务少的穷学生的个人极简生活实践。相信对于不同的人，都会有不同的最佳“极简”状态。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[lambda]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/07/16/lambda/"/>
    <updated>2014-07-16T13:39:35+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/07/16/lambda</id>
    <content type="html"><![CDATA[<p>复习无聊，写点东西，想起来昨天“数理逻辑”课上最后一道题有些意思，拿出来分享一下：</p>

<p>我们定义一个运算“◦”，它是一个二元函数。下面我们定3个结构（包括操作域与运算符）：
<script type="math/tex"> \mathfrak{A}_1 = (\{0, 1\}, ◦), a ◦ b = min(a, b) </script></p>

<p>\[  \mathfrak{A}_2 = (\{0, 1\}, ◦), a ◦ b = max(a, b) \]</p>

<p>\[  \mathfrak{A}_3 = (\{0, 1\}, ◦), a ◦ b = (a+b) mod 2 \]</p>

<p>对于$ i, j \in {1, 2, 3}, i&lt;j $ 给出一个FO范式$\phi$来使得对于任意$i, j$都有$ \mathfrak{A}_i \models \phi $ 但是 $\mathfrak{A}_j \models \urcorner\phi$</p>

<!--more-->

<p><a href="https://logic.rwth-aachen.de/files/MaLo-SS14/home12.pdf">原题目</a>中要求对于每个<script type="math/tex">i, j</script>各写一个<script type="math/tex">\phi_{i,j}</script>，本题提高了难度，用一个范式写出来，如果这个题用编程的方法实现，大概是这样的，我们假设了：</p>

<pre><code>for all a in {0, 1}:

if i == 1, j == 2 then
	return (a ◦ !a == 0)
elif i == 1, j == 3 then
	return (a ◦ !a == 0)
elif i == 2, j == 3 then
	return ((a ◦ a) != (!a ◦ !a))
</code></pre>

<p>显然，我们对于特定的<script type="math/tex">i, j</script>我们选择了尽量简单的区分两种运算符的范式，如果你能看懂上面的思路，那么下面就是要把这些东西依次转化成FO范式：</p>

<script type="math/tex; mode=display"> \forall a(((i=1,j=2)\land(a◦!a == 0))\vee((i=1,j=3)\land(a ◦!a == 0))\vee((i=2,j=3)\land((a ◦ a) != (!a ◦ !a)))) </script>

<p>不过在FO范式中，我们只定义了◦操作，所以要把取反“!”和常数“0”替换掉，例如我们定义取反<script type="math/tex">$\exists y(y \neq a \land \varphi)</script>$，替换常数0的过程省略。</p>

<p>下一步要替换掉每个if条件，我们只要判断这个范式不符合第(6-i-j)个结构的性质就可以了，比如对于i=1, j=2,我们可以判断$ ((a ◦ a) != (!a ◦ !a)) $。</p>

<p>这道题的改进版思路有两个重要的解题关键：替换常数和if。是不是感觉很熟悉？对！纯lambda算子，理解它的关键就是在于所有现代编程的3大控制结构：循环、选择、顺序是如何替换成lambda表达式的，以及在纯lambda中不出现常数，或者说是常量，有的只有函数。下面给一个简单的关于纯lambda思考题：</p>

<p>我们用$\lambda xy.x$代表常量True，$\lambda xy.y$代表常量False。写出下列函数的lambda形式：</p>

<ul>
  <li>and</li>
  <li>or</li>
  <li>not</li>
  <li>xor</li>
  <li>implies</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从一个梦中得到的关于生活中的加密方法]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/06/24/encrypt-from-dream/"/>
    <updated>2014-06-24T10:30:05+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/06/24/encrypt-from-dream</id>
    <content type="html"><![CDATA[<p>前天做了一个梦：在火车上，列车员来查票，给她看了我的车票之后，他让我在上面写一个密码，来证明这张票是我的。</p>

<p>在德国，买火车票的时候，有些车票并不是实名，所以要在车票上签名才能使用。因为德国上火车不需要检票，只需要上车之后列车员查票，并且一般不要求同时出示对应证件。而签名，似乎只是为了表示确认车票的使用者，而不能随便更改。这种措施显然没有什么意义，任何倒票、冒名用票的手段都不能被防止。</p>

<p>显然倒票只能从购票的时候控制，所以不做讨论。我们下面来说下__如何防止偷票坐车__。</p>

<ul>
  <li>按照现有签名的办法，那么在检票员查票时应该同时查车票和有效证件，然后保证车票签名和证件姓名一致，证件照片和持票人本人一致，从而证明票人一致。但是因为增加了“证件”从而可能会引发一些问题，比如恰好偷票人和购票人姓名一样（当然可以通过增加生日来区分），比如伪造有效证件，或者忘记携带身份证，身份证被偷……</li>
</ul>

<p>下面提一下我在梦中的做法，也就是计算机加密算法的基础————大数分解：</p>

<ul>
  <li>
    <p>首先想两个大质数，然后它们的乘积写到车票上，显然这个大数与两个质数唯一对应。查票人只要检查持票人能否写出两个质数，乘积为车票上的数即可。对于非法持有人，理论上是不能通过车票上的大数直接得到两个质因数的。</p>
  </li>
  <li>
    <p>显然上面的方法要求持票人记住两个大质数，难以实现。所以完全可以换成两个普通的对自己有意义的大数，检票时，只要向检票员证明分解方法和两个数分别对自己的意义。</p>
  </li>
</ul>

<p>嗯，这就是我前天做梦的内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[最近这三年]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/04/17/zui-jin-zhe-san-nian/"/>
    <updated>2014-04-17T21:00:34+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/04/17/zui-jin-zhe-san-nian</id>
    <content type="html"><![CDATA[<h1 id="section">前言</h1>
<p>两年前想着，考过德福之后写一篇最近这一年，来总结一下德语学习对于一个IT男的影响。当时万万没想到，真正拿到语言证明竟然是在那两年之后。今天收到了第二份足以让我入学的语言证书，才想起当初给自己的约定，于是写下此文缅怀本应该最珍贵那三年。</p>

<!--more-->

<h1 id="section-1">起因</h1>
<p>对于一心想毕业直接工作的我，从来没把学习与成绩看得很重要。直到大三即将开始的暑假，受到女友的怂恿，决定一起尝试一个最不靠谱的选择————去德国留学。我只记得当时虽然意识到这个选择一定是错的，但是我决定和她一起沿着错误的路走到正确。</p>

<h1 id="section-2">经过</h1>
<p>在做出这个决定之后，我们争取一切时间一起报班学德语。同时也决心拒绝一切可能会影响留德之路的一切机会。在学了一年德语之后，开始尝试参加德语考试，一年3次德福考试被我们考了个遍，最后还是成绩很低。最后拿着语言条件限制的录取通知书来德国读语言班，并和大多数来德国读语言班的同学一样，半年后通过，入学。</p>

<h1 id="section-3">最重要的两年</h1>
<p>对于我而言，最重要的两年就是大三、大四。这两年内的决定会直接决定我未来的人生轨迹。这两年，可以潜下心认真继续搞ACM，或许能拿到更好成绩，从而有更多机会去美国或者国内顶级IT公司；我也可以认真找个实习，或许可以去一家比知乎或者雅虎更好的公司；或者早一点去唐老师的实验室，做一些有意思的研究，也可能已经去了美国一个不错的学校。再不济，放弃一些德福备考的时间，多做些外包，也能让当时的生活有很大的改善。但是当时决定了一条路，就要一直走下去，如果中途放弃，拐了弯，不管新的路是否更加宽阔，“德语学习的经历”对我来说才是真正的走了“弯路”。所以，我能做的就是放弃一切可能影响我的诱惑。</p>

<h1 id="section-4">收获</h1>
<p>大一大二我过的顺风顺水，究其原因是和很多大牛们学到要学会“走捷径”，这样能节省不少时间做更多的事。对于不管是计算机、软件的学生，还是IT从业人员，这一思想根深蒂固。我大一可以用OpenCV实现看似很牛逼的识别功能，大二用脚本语言框架来做各种Web项目，但其实做到这些都不需要你懂得太多。这一观念，是直到我德语数次失败之后才意识到的。大三大四我尝试学习思考一些复杂的过程，比如理解函数式编程，比如用机器学习方法获得一些有趣的实验，还有理解，甚至背写ACM的一些复杂算法。但是浮躁的内心已经让我无法专注于这些真正值得在那个年纪做的东西。</p>

<p>学了德语，在德国上了课，才让我认识到，真正厉害的人都是能耐心的做枯燥的事的。那些看似很好掌握，有威力强大的东西，对别人同样很简单。这些想法，恐怕我如果不学德语，一辈子都无法意识到的。</p>

<h1 id="section-5">目标与规划</h1>
<p>刚刚开学，身边的人都在讨论着毕业之后的路，大多是选择读博，确实毕业也不过是2年之后的事了。而毕业对于我似乎还算遥远，我并没有明确的想做什么。只是觉得，来到德国或者说亚琛最难得的就是能与世隔绝的学些大部头的东西，这些东西是我在国内恐怕难以做到的。所以就趁这段时间，学一些以后恐怕很难再直接用到，却又非常重要的理论课。至于我以后要做什么，确实和我现在在学什么关系不大，在一些理论课上，我已经清晰地感受到自己的基本功有多麽不扎实了。通过一些感兴趣的课拿到足够学分之后，我再开始考虑以后的问题，相信到时候选择会更多。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[德语助手Workflow]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2014/02/03/godic/"/>
    <updated>2014-02-03T22:04:57+01:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2014/02/03/godic</id>
    <content type="html"><![CDATA[<p>前两天mac下的德语助手不知怎么变聪明了，发现我没有购买，于是彻底不能用了，断网重装都不能解决，于是果断放弃，当晚写了一个alfred的workflow来代替。缺点就是在线查询确实慢。。。</p>

<p><img src="http://xuanhuangyiqi.github.io/images/ScreenShot2014-02-03at9.58.55PM.png" /></p>

<p>废话不多说，上项目链接：</p>

<p><a href="https://github.com/xuanhuangyiqi/godic-alfred">项目地址</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[翻译：如何读论文]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2013/09/28/ru-he-du-lun-wen/"/>
    <updated>2013-09-28T14:02:01+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2013/09/28/ru-he-du-lun-wen</id>
    <content type="html"><![CDATA[<p>本文只翻译了论文第二三部分</p>

<h1 id="section">读论文的三个阶段</h1>

<h2 id="section-1">第一个阶段</h2>

<p>第一个阶段就是快速浏览文章，对文章有概括的认识。你可以决定是否进行下面的阶段，这个阶段大约需要5-10分钟，并且包含一下几个步骤：</p>

<ol>
  <li>仔细阅读标题、摘要和介绍</li>
  <li>阅读段落和子段落标题，但是忽略其他的一切。</li>
  <li>粗看数学的内容（如果有的话）来确定基本的数学基础</li>
  <li>读结论</li>
  <li>粗看索引，剔出一些你已经看过的。</li>
</ol>

<!--more-->
<p>在第一阶段结束的时候，你应该能够回答5个问题：</p>

<ol>
  <li>类别：这个论文属于什么类？测量论文？一个已有系统的分析？还是一个研究标准的描述？</li>
  <li>上下文：和哪些其他论文相关？有哪些理论基础被用于分析这个问题？</li>
  <li>正确性：这个假设正确吗？</li>
  <li>贡献：这个论文的主要贡献</li>
  <li>清晰：这个论文值得写吗？</li>
</ol>

<p>通过这些信息，你可能选择不继续读（并且不用打印出来，节省树木）。这可能因为你对这篇论文没兴趣，或者你对相关的领域了解的不充分，或者这些作者设定了错误的假设。第一个阶段对于不是你领域的论文已经足够了，但是某天会被证明很重要。</p>

<p>顺便的，当你写一篇论文时，你可以期望大多数审阅人可以只进行第一个阶段。小心选择清晰的段落和子段落标题，并且写简明综合的概述，如果一个审阅人在一个阶段之后不能理解主旨，这个论文就可能被拒绝；如果一个读者不能在5分钟内不能理解论文的关键点，这个文章就可能不再被读。因为这些理由，一个能通过选好的图总结一篇论文的“绘声绘色的概述”是一个接触的想法，并且可以越来越多的在科学周刊中看到。</p>

<p><a href="http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf">原文地址</a></p>

<h2 id="section-2">第二个阶段</h2>

<p>在第二个阶段，越多要有更多的关注，但是忽略证明的细节。这可以帮助你记住关键点，或者按照你读到的在旁边记下注释。 Augsburg大学的Dominik Grusemann表示，你“记下不懂得部分或者想问作者的问题”。如果你是一个论文评审人，这些注释会帮助你当你在写回顾的时候，和在一个项目会议中回忆你写的的回顾的时候。</p>

<ol>
  <li>仔细看论文中图、图表和其他图解。特别关注图形。它的坐标轴有标注吗？结果展示有错误条吗，他们有很重要的统计学意义。这些常见的错误会把匆忙、错误的从真正优秀的论文中分辨出来。</li>
  <li>记得为了以后的阅读，标记重要的未读会议。</li>
</ol>

<p>第二个部分对于有经验的读者应该占据一个小时。度过这个阶段，你应该有能力抓住文章的内容。你应该有能力根据支持的证据给别人总结论文的主要事实。这个详细的级别对于一篇你感兴趣的论文是合适的，但是在你的研究特长上面还不够。</p>

<p>有时候你即使在第二阶段结束的时候也不能理解一篇文章。这可能是因为这个主题的术语或者所写对你是新的。或者作者想用你不理解的证明方法或者实验技巧，所以这个论文的含量是不能预测的。这个论文可能是有没有证实的断言和大量的会议挤出来的。或者有可能多的时候太晚了，你很累了。你现在可以选择（a）把这个论文放一边，希望你的生涯中不需要明白这些材料（b）读完背景材料之后，等会再回到这篇论文中（c）保留现在的状态，进入第三阶段。</p>

<h2 id="section-3">第三个阶段</h2>

<p>为了完整理解这篇论文，尤其你是一个审稿人，需要第三个阶段。第三阶段最关键的是尝试“实际重现”这篇论文：就是说，和作者做相同的假设，重新做一边下面的工作。通过对比这个重新构造的过程和论文，你就可以不仅仅看出论文体现的创造力，而且还有没有写出来的失败和假设。</p>

<p>这个过程需要把大量精力放在细节。你可以了解并且挑战陈述中的每个假设。另外，你可以考虑你如何表述一个特定的想法。这个和虚拟与实际情况的对比能引导你对证明过程和展示技巧更加敏锐的目光，而且你非常可能把它加到你的工具指令中。在这个过程中，你可以略记下一些想法为了未来的工作。
这个过程对于初学者可能持续很多小时，对于有经验的读者也要多于一个小时，甚至更多。在这个阶段的最后，你应该有能力从记忆中把把想法重新构造出论文全部的结构，而且有能力识别重要和不重要的观点，尤其是，你应该有能力指出重要工作的隐含的假设，和缺失的引用，以及试验和分析技巧潜在问题。</p>

<h1 id="section-4">做文献调研</h1>

<p>论文阅读技巧在文献调研的过程中得到测试。这将需要你读10篇论文，可能在一个不熟悉的领域。你需要读什么样的论文？这里有你应该怎么用三阶段法实施。
首先，使用学术搜索引擎，比如Google Scholar或者CiteSeer，还有选择好的关键词去搜索3-5篇这个领域中最近高引用的论文。每篇文章进行一个阶段，然后读他们的相关工作的段落。你会发现一个近几年相关工作的简略总结。如果你能发现这样一个调研，你就完成了。读这个调研，祝你好运。</p>

<p>否则，在第二步，在参考书目中发现共同的引用和重复的作者姓名。这些是这个领域的关键论文和研究员。下载这些关键论文并且把它们放一边。然后去这些关键研究员的网站去看看他们最近在哪里发表。这会帮你识别这个领域的顶级会议，因为最好的研究员经常在顶级会议上发表。</p>

<p>第三步就是去这些顶级会议的网站，看看最近的进程。一个快速的浏览将帮助你经常了解最近高质量的相关工作。这些论文，还有你刚才放一边的东西，组成了你调研的第一个版本，完成这些论文的第二个阶段，如果他们都引用了一篇你之前没发现的论文，把它添加进来并且读完它，增量是必须的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS Crash Logs 基本调研]]></title>
    <link href="http://xuanhuangyiqi.github.io/blog/2013/09/02/ios-crack/"/>
    <updated>2013-09-02T13:30:03+02:00</updated>
    <id>http://xuanhuangyiqi.github.io/blog/2013/09/02/ios-crack</id>
    <content type="html"><![CDATA[<p>1个月的实习马上就要结束，本文主要介绍我在实习期间主要调研内容——iOS app崩溃时的Crash Log分析与总结。</p>

<h1 id="crash-log-">Crash Log 是什么？</h1>

<p>首先需要介绍下Crash Log的基本信息，在大家在使用iOS app时有时候会因为各种原因而自动退出，比如系统内存不足，比如程序本身的bug，比如越狱造成的系统不稳定。总之，凡是app的自动退出都会对应的生成一个log文件，记录系统崩溃时的系统信息和崩溃栈。如下是一个crash log的样子: <a href="http://xuanhuangyiqi.github.io/assets/Bildschirmfoto2013-09-02um2.25.41PM.png">Crash Log example</a>。</p>

<p>从图中可一看到，每一次崩溃发生时，会有一个Crash Log存储在iOS设备中，并且在连接Xcode时转移到mac上。</p>

<p>另外，看到下面各个Thread的函数堆栈中，能看到每行最右面有错误发生的<em>函数名</em>和对应的<em>行数</em>，如果你自己尝试做这个试验就会发现开始<em>函数</em>的位置是一个16位内存地址，过了一会才会自动被Xcode转换成函数名。</p>

<!--more-->

<h1 id="symbolicate">Symbolicate</h1>

<p>Symbolicate过程就是把对应的函数栈转化成函数名的过程，我们为了方便，在后面把这个过程叫做<em>解析</em>。这个过程是怎么实现的呢？</p>

<p>首先，无论是在模拟器，还是真机debug一个app的过程中，除了XXX.app之外，还会生成一个XXX.app.dSYM文件（准确地说，这两个都是文件夹），如果你想知道生成的这两个文件路径，可以在terminal下使用mac的内嵌find工具mdfind：</p>
<div>
  <pre><code class="bash">$ mdfind -name &quot;XXX.app&quot;</code></pre>
</div>

<p>它是Spotlight功能的核心，在你的系统有文件更新时能及时更新索引，因此速度还是很快的，当然它还可以根据其他所引进行搜索。</p>

<p>dSYM文件保存了这个app的符号表，即指令内存地址和源代码的函数名和行数关系。这个文件显然保存了app源码的大量信息，因此是不能随意公布的。当然，只有你本机上包含有app的dSYM文件，其中的指令地址才能被解析成函数名（这句话是不严谨的，我们后面在解释）。</p>

<h1 id="xcode-">Xcode 解析工具</h1>

<p>解析的工作分为很多步，我们来介绍几个Xcode自带的相关小工具：</p>

<ul>
  <li>atos，用以获取某内存地址所对应的函数名。用法：</li>
</ul>
<div>
  <pre><code class="bash">$ atos -o /path/to/XXX.app/XXX.app.dSYM/Contents/Resources/DWARF/XXX -arch i386 0x973000</code></pre>
</div>

<p>这个函数是指定了arch的，即i386，这是模拟器（或者说mac）的处理器arch，对于iOS设备就是armv6／7。如果你指定的arch和app以及dsym文件的arch不对，是会报错的。那么如何知道一个app的arch呢？前面我已经说了最常见的两个，如果你不确定是哪个，就可以用</p>

<ul>
  <li>dwarfdump</li>
</ul>
<div>
  <pre><code class="bash">$ dwarfdump -u ~/Library/Developer/Xcode/iOS\ DeviceSupport/5.0.1\ \(9A405\)/Symbols/System/Library/Frameworks/UIKit.framework/UIKit</code></pre>
</div>

<ul>
  <li>symbolicatecrash 这是symbolicate过程的核心脚本</li>
</ul>
<div>
  <pre><code class="bash">$ /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/Library/PrivateFrameworks/DTDeviceKit.framework/Versions/A/Resources/symbolicatecrash</code></pre>
</div>

<h1 id="section">动态库与静态库</h1>

<p>在学习C语言的时候，我们就应该接触过这两种库，静态库是在编译时把其中涉及到的指令一起放入最终生成的app，动态库则是在运行时动态查找库的。在iOS开发中，动态库的常见形式是dylib或者Framework。</p>

<p>无论是app还是Framework，在生成的时候都是有特定的arch的，这是可以用dwarfdump命令可以查到。但是app会把符号表放到单独的dSYM文件中，库文件的符号表会在自己内部。这就是我现在不能理解的问题，只要有足够多种类的语句，是可以通过外部app的调用把库的每条语句解析出来的。这是不是一种不安全的设计？</p>

<h1 id="crash-log">现有Crash Log服务</h1>
<ul>
  <li>QuincyApp 主要用于app发行前的crash管理</li>
  <li>Crashlytics 界面友好</li>
  <li>Crittercism 统计内容详细</li>
</ul>

<h1 id="section-1">解析过程</h1>
<p>在执行解析的过程中，首先从dSYM文件中找到app编译时所调用的动态库的名称、UUID、arch。然后利用mac自带的mdfind功能在全局搜索是否存在对应的库，只有完全匹配才可以解析。目前遇到的问题是如果使用模拟器制造crash，那么crash文件底层库的内容不会被解析出来，已经判断出是因为Xcode.app文件下的arch为i386的Framework的UUID与app的dSYM的不符。具体原因需要进一步了解。而真机测试时调用的库文件在另外的文件夹，似乎不会出现类似的问题。</p>

]]></content>
  </entry>
  
</feed>
